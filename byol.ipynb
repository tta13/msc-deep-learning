{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a115a2-48e0-4eb7-b55b-f5bd08c804d8",
   "metadata": {},
   "source": [
    "# BYOL pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ec9d08-6b6f-41cb-9248-92b921d7b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def reproducibility(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "\n",
    "def define_param_groups(model, weight_decay, optimizer_name):\n",
    "    def exclude_from_wd_and_adaptation(name):\n",
    "        if 'bn' in name:\n",
    "            return True\n",
    "        if optimizer_name == 'lars' and 'bias' in name:\n",
    "            return True\n",
    "\n",
    "    param_groups = [\n",
    "        {\n",
    "            'params': [p for name, p in model.named_parameters() if not exclude_from_wd_and_adaptation(name)],\n",
    "            'weight_decay': weight_decay,\n",
    "            'layer_adaptation': True,\n",
    "        },\n",
    "        {\n",
    "            'params': [p for name, p in model.named_parameters() if exclude_from_wd_and_adaptation(name)],\n",
    "            'weight_decay': 0.,\n",
    "            'layer_adaptation': False,\n",
    "        },\n",
    "    ]\n",
    "    return param_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568aa243-1b87-4f7c-904a-d41c7af0f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "\n",
    "class Augment:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module\n",
    "    Transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, s=1):\n",
    "        color_jitter = T.ColorJitter(\n",
    "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
    "        )\n",
    "        blur = T.GaussianBlur((3, 3), (0.1, 2.0))\n",
    "\n",
    "        self.train_transform = T.transforms.Compose([\n",
    "            T.RandomResizedCrop(img_size, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "            T.RandomApply(\n",
    "                    [T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                    p=0.8\n",
    "                ),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "            T.Resize((256, 256)),\n",
    "            T.CenterCrop(img_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x), \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff10a1a-766e-4b90-a303-7155cc4c5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.multiprocessing import cpu_count\n",
    "\n",
    "def get_data_loader(dataset_path, batch_size, transform=None, shuffle=False):\n",
    "    dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=cpu_count()//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dbf4ce9-8746-4b98-a2c3-5ae4b52dc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embedding_size=256, hidden_size=2048, batch_norm_mlp=False):\n",
    "        super().__init__()\n",
    "        norm = nn.BatchNorm1d(hidden_size) if batch_norm_mlp else nn.Identity()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            norm,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AddProjHead(nn.Module):\n",
    "    def __init__(self, model, in_features, hidden_size=4096,\n",
    "                 embedding_size=256, batch_norm_mlp=True):\n",
    "        super(AddProjHead, self).__init__()\n",
    "        self.backbone = model\n",
    "        # remove last layer\n",
    "        self.backbone.classifier[1] = nn.Identity()\n",
    "        self.backbone.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.backbone.maxpool = torch.nn.Identity()\n",
    "        # add mlp projection head\n",
    "        self.projection = MLP(in_features, embedding_size, hidden_size=hidden_size, batch_norm_mlp=batch_norm_mlp)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        embedding = self.backbone(x)\n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        return self.projection(embedding)\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    # L2 normalization\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.alpha + (1 - self.alpha) * new\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            net,\n",
    "            batch_norm_mlp=True,\n",
    "            in_features=512,\n",
    "            projection_size=256,\n",
    "            projection_hidden_size=2048,\n",
    "            moving_average_decay=0.99,\n",
    "            use_momentum=True,\n",
    "            device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            net: model to be trained\n",
    "            batch_norm_mlp: whether to use batchnorm1d in the mlp predictor and projector\n",
    "            in_features: the number features that are produced by the backbone net i.e. resnet\n",
    "            projection_size: the size of the output vector of the two identical MLPs\n",
    "            projection_hidden_size: the size of the hidden vector of the two identical MLPs\n",
    "            augment_fn2: apply different augmentation the second view\n",
    "            moving_average_decay: t hyperparameter to control the influence in the target network weight update\n",
    "            use_momentum: whether to update the target network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.student_model = AddProjHead(model=net, in_features=in_features,\n",
    "                                         embedding_size=projection_size,\n",
    "                                         hidden_size=projection_hidden_size,\n",
    "                                         batch_norm_mlp=batch_norm_mlp)\n",
    "        self.use_momentum = use_momentum\n",
    "        self.teacher_model = self._get_teacher()\n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "        self.student_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
    "        self.device = device\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_teacher(self):\n",
    "        return copy.deepcopy(self.student_model)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_moving_average(self):\n",
    "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum ' \\\n",
    "                                  'for the target encoder '\n",
    "        assert self.teacher_model is not None, 'target encoder has not been created yet'\n",
    "\n",
    "        for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):\n",
    "          old_weight, up_weight = teacher_params.data, student_params.data\n",
    "          teacher_params.data = self.target_ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            image_one, image_two=None,\n",
    "            return_embedding=False):\n",
    "        if return_embedding or (image_two is None):\n",
    "            return self.student_model(image_one, return_embedding=True)\n",
    "\n",
    "        image_one, image_two = image_one.to(self.device), image_two.to(self.device)\n",
    "\n",
    "        # student projections: backbone + MLP projection\n",
    "        student_proj_one = self.student_model(image_one)\n",
    "        student_proj_two = self.student_model(image_two)\n",
    "\n",
    "        # additional student's MLP head called predictor\n",
    "        student_pred_one = self.student_predictor(student_proj_one)\n",
    "        student_pred_two = self.student_predictor(student_proj_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # teacher processes the images and makes projections: backbone + MLP\n",
    "            teacher_proj_one = self.teacher_model(image_one).detach_()\n",
    "            teacher_proj_two = self.teacher_model(image_two).detach_()\n",
    "            \n",
    "        loss_one = loss_fn(student_pred_one, teacher_proj_one)\n",
    "        loss_two = loss_fn(student_pred_two, teacher_proj_two)\n",
    "\n",
    "        # Free tensors\n",
    "        del image_one, image_two\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return (loss_one + loss_two).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212a0978-95bb-438c-8489-c7798b7fa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/noahgolmant/pytorch-lars\n",
    "\"\"\" Layer-wise adaptive rate scaling for SGD in PyTorch! \"\"\"\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    r\"\"\"Implements layer-wise adaptive rate scaling for SGD.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): base learning rate (\\gamma_0)\n",
    "        momentum (float, optional): momentum factor (default: 0) (\"m\")\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "            (\"\\beta\")\n",
    "        eta (float, optional): LARS coefficient\n",
    "        max_epoch: maximum training epoch to determine polynomial LR decay.\n",
    "\n",
    "    Based on Algorithm 1 of the following paper by You, Gitman, and Ginsburg.\n",
    "    Large Batch Training of Convolutional Networks:\n",
    "        https://arxiv.org/abs/1708.03888\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = LARS(model.parameters(), lr=0.1, eta=1e-3)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, momentum=.9,\n",
    "                 weight_decay=.0005, eta=0.001, max_epoch=200):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\"\n",
    "                             .format(weight_decay))\n",
    "        if eta < 0.0:\n",
    "            raise ValueError(\"Invalid LARS coefficient value: {}\".format(eta))\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(lr=lr, momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        eta=eta, max_epoch=max_epoch)\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "            epoch: current epoch to calculate polynomial LR decay schedule.\n",
    "                   if None, uses self.epoch and increments it.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            eta = group['eta']\n",
    "            lr = group['lr']\n",
    "            max_epoch = group['max_epoch']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                weight_norm = torch.norm(p.data)\n",
    "                grad_norm = torch.norm(d_p)\n",
    "\n",
    "                # Global LR computed on polynomial decay schedule\n",
    "                decay = (1 - float(epoch) / max_epoch) ** 2\n",
    "                global_lr = lr * decay\n",
    "\n",
    "                # Compute local learning rate for this layer\n",
    "                local_lr = eta * weight_norm / \\\n",
    "                    (grad_norm + weight_decay * weight_norm)\n",
    "\n",
    "                # Update the momentum term\n",
    "                actual_lr = local_lr * global_lr\n",
    "\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = \\\n",
    "                            torch.zeros_like(p.data)\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
    "                p.data.add_(-buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b852d-1e2d-4872-bc85-caae99eda023",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4578ce04-012e-4549-8015-c6c262ac0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, data):\n",
    "    (view1, view2), _ = data\n",
    "    loss = model(view1, view2)\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(epoch, model, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    batch_loader = tqdm(train_dataloader)\n",
    "    for data in batch_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = training_step(model, data)\n",
    "        loss.backward()\n",
    "        optimizer.step(epoch)\n",
    "        # EMA update\n",
    "        model.update_moving_average()\n",
    "        total_loss += loss.item()        \n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb8f5e-b8b5-4ccd-964e-093cac3a38f1",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b977c879-bd56-40d9-bfad-56816a74f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536dce19-d503-4790-bc50-c88ad828a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set train params\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "base_lr = 0.1\n",
    "weight_decay = 1e-5\n",
    "load = False\n",
    "img_size = 224\n",
    "random_state = 9999\n",
    "epochs = 100\n",
    "dataset_path = './datasets/chestx-ray14-v3'\n",
    "save_path = './output/BYOL'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ade349-9c77-4c4c-8f7f-4e231dd0a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_gpus: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/378 [00:00<?, ?it/s]/tmp/user/1781838319/ipykernel_2014197/2481369168.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:53<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]  loss: 315.6053772866726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/100]  loss: 62.53307390958071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/100]  loss: 36.58280463516712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/100]  loss: 27.361718595027924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/100]  loss: 22.650810554623604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:45<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/100]  loss: 20.013647075742483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/100]  loss: 18.263133134692907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/100]  loss: 17.004760928452015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/100]  loss: 15.896410040557384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/100]  loss: 14.991358235478401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/100]  loss: 14.140239026397467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/100]  loss: 13.427570935338736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/100]  loss: 12.731849577277899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/100]  loss: 12.11959007754922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/100]  loss: 11.592642612755299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/100]  loss: 11.059506207704544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17/100]  loss: 10.583209976553917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18/100]  loss: 10.188971795141697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19/100]  loss: 9.77972012013197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/100]  loss: 9.427536331117153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21/100]  loss: 9.080560892820358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22/100]  loss: 8.720346633344889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23/100]  loss: 8.423694729804993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24/100]  loss: 8.152537446469069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25/100]  loss: 7.867703318595886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26/100]  loss: 7.619006108492613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27/100]  loss: 7.38710667565465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28/100]  loss: 7.149893205612898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29/100]  loss: 6.9444165006279945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30/100]  loss: 6.73700961843133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31/100]  loss: 6.522922437638044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32/100]  loss: 6.352552682161331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33/100]  loss: 6.189767774194479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34/100]  loss: 6.028669778257608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35/100]  loss: 5.902120176702738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36/100]  loss: 5.761668220162392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37/100]  loss: 5.633467152714729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38/100]  loss: 5.503438711166382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39/100]  loss: 5.401739835739136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40/100]  loss: 5.304343577474356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:36<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41/100]  loss: 5.2170905359089375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [42/100]  loss: 5.131762959063053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43/100]  loss: 5.04686925932765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [44/100]  loss: 4.95233827829361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [45/100]  loss: 4.862424656748772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46/100]  loss: 4.812692441046238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:35<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47/100]  loss: 4.737831100821495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [48/100]  loss: 4.6709949262440205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [49/100]  loss: 4.612625319510698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50/100]  loss: 4.552193950861692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51/100]  loss: 4.496733970940113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [52/100]  loss: 4.460517294704914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [53/100]  loss: 4.4136460572481155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [54/100]  loss: 4.368228986859322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [55/100]  loss: 4.332343067973852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [56/100]  loss: 4.282889768481255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [57/100]  loss: 4.275463692843914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [58/100]  loss: 4.24171432480216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [59/100]  loss: 4.1843603029847145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [60/100]  loss: 4.155985813587904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:36<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [61/100]  loss: 4.108313467353582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:45<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [62/100]  loss: 4.067725837230682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [63/100]  loss: 4.021059852093458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [64/100]  loss: 3.9765564426779747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [65/100]  loss: 3.9306016974151134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [66/100]  loss: 3.9045748375356197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [67/100]  loss: 3.8469014950096607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [68/100]  loss: 3.8071306757628918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69/100]  loss: 3.759790364652872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [70/100]  loss: 3.7135000452399254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71/100]  loss: 3.6813159435987473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [72/100]  loss: 3.640078842639923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:46<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [73/100]  loss: 3.586175139993429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [74/100]  loss: 3.5393689833581448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [75/100]  loss: 3.5197134763002396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [76/100]  loss: 3.472804918885231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [77/100]  loss: 3.428943704813719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [78/100]  loss: 3.4113054238259792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [79/100]  loss: 3.376995787024498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [80/100]  loss: 3.3483759500086308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [81/100]  loss: 3.325476884841919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [82/100]  loss: 3.288610562682152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83/100]  loss: 3.2645843364298344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [84/100]  loss: 3.242599930614233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [85/100]  loss: 3.227864984422922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [86/100]  loss: 3.201568204909563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [87/100]  loss: 3.1776170879602432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:44<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [88/100]  loss: 3.1584740802645683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [89/100]  loss: 3.133956853300333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [90/100]  loss: 3.1236198507249355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [91/100]  loss: 3.1073203459382057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [92/100]  loss: 3.083360068500042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [93/100]  loss: 3.06902864202857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:45<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [94/100]  loss: 3.055242210626602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [95/100]  loss: 3.0363097116351128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [96/100]  loss: 3.0282959043979645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [97/100]  loss: 2.998874917626381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [98/100]  loss: 2.985373832285404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [99/100]  loss: 2.9658458940684795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:44<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]  loss: 2.9585346840322018\n",
      "Finished in 22088.171337604523s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.efficientnet_b0(pretrained=True)\n",
    "model = BYOL(model, in_features=model.classifier[1].in_features, batch_norm_mlp=True, device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = LARS(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "\n",
    "# data\n",
    "transform = Augment(img_size)\n",
    "\n",
    "loader_train = get_data_loader(f'{dataset_path}/train', batch_size, transform=transform)\n",
    "\n",
    "# general info\n",
    "available_gpus = len([torch.cuda.device(i) for i in range(torch.cuda.device_count())])\n",
    "print('available_gpus:', available_gpus)\n",
    "\n",
    "reproducibility(random_state)\n",
    "\n",
    "if load:\n",
    "  model.load_state_dict(torch.load(\"....ckpt\"))\n",
    "\n",
    "\n",
    "mean_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch(epoch, model, loader_train, optimizer)\n",
    "    print(f'Epoch: [{epoch+1}/{epochs}]  loss: {np.mean(train_loss)}')\n",
    "    mean_losses.append(train_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Finished in {end_time - start_time}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3f39c3-33ee-497a-aa58-5b31805c1d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "indices = range(0,100,1)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(indices, mean_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c107fb98-ca70-46ae-8252-b22bfcfda773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): ConvNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): ConvNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): ConvNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Identity()\n",
      "  )\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (maxpool): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "model_backbone_weights = model.student_model.backbone\n",
    "print(model_backbone_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e562c1-858d-439c-b062-a9dc6668b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ! mkdir ./output/BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "293d0c53-0882-4589-bf3d-a791c07e925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({ 'model_state_dict': model_backbone_weights.state_dict() }, f'{save_path}/efficientnet_b0_backbone_weights_v1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2eea27-1701-4da6-a1ad-aee8ec8f7efe",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e80c3f-6e86-4992-a38e-f123e00cbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = './datasets/COVIDGR_1.0'\n",
    "positive_path = os.path.join(dataset_path, 'P')\n",
    "negative_path = os.path.join(dataset_path, 'N')\n",
    "\n",
    "# Data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomApply(\n",
    "            [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "            p=0.8\n",
    "        ),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854e6e57-6e0a-479b-9e5d-c8cfdcdc2ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 852\n",
       "    Root location: ./datasets/COVIDGR_1.0\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(224, 224), scale=(0.3, 0.9), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
       "               RandomApply(\n",
       "               p=0.8\n",
       "               ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.8, 1.2], hue=[-0.1, 0.1])\n",
       "           )\n",
       "               RandomGrayscale(p=0.2)\n",
       "               RandomApply(\n",
       "               p=0.5\n",
       "               GaussianBlur(kernel_size=(25, 25), sigma=(0.1, 2.0))\n",
       "           )\n",
       "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "           )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Map labels\n",
    "dataset.class_to_idx = {'N': 0, 'P': 1}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13de976-7e60-41d8-9329-971b06ada1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766 256\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "batch_size = 256\n",
    "print(train_size, batch_size)\n",
    "\n",
    "# Split off the test set\n",
    "train_val_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for the test set (held out)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50283805-104d-4380-81ca-7d09bf80b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./output/BYOL/efficientnet_b0_backbone_weights_v1.ckpt\"\n",
    "best_params = { \"learning_rate\": 0.01, \"weight_decay\": 0.001 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c2c1184-d12a-4d1b-8b17-b12562c5a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_model():\n",
    "    # Load the EfficientNet model\n",
    "    model = efficientnet_b0()\n",
    "    \n",
    "    # Modify the final classification head for your dataset\n",
    "    embed_dim = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Identity()\n",
    "    \n",
    "    # Load the pre-trained weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    # Remove unnecessary keys (e.g., DINO projection head weights)\n",
    "    state_dict = checkpoint[\"model_state_dict\"]  # Adjust the key if needed\n",
    "    # remove `module.` prefix\n",
    "    # state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    # remove `backbone.` prefix induced by multicrop wrapper\n",
    "    # state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "    \n",
    "    # Load the weights into the model\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Pretrained weights found at {} and loaded with msg: {}'.format(checkpoint_path, msg))\n",
    "\n",
    "    # Freeze model params\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13411c3d-29f8-43d0-8008-69df2c3cff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print (np.__version__)\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def train_model(model, classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "\n",
    "        # Validation phase\n",
    "        classifier.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                outputs = classifier(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "\n",
    "        print(f\"[{device}] Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss / len(val_loader):.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return best_val_f1\n",
    "\n",
    "# Evaluate model on the test set\n",
    "def evaluate_model(model, classifier, test_loader):\n",
    "    classifier.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def grid_search(train_loader, val_loader, learning_rates, weight_decays, num_epochs):\n",
    "    best_model = None\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decays:\n",
    "            model = get_model()\n",
    "            optimizer = optim.SGD(model.classifier[1].parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
    "            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            print(f\"\\nTraining with lr={lr}, weight_decay={wd}\")\n",
    "            f1_score = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "            if f1_score > best_f1:\n",
    "                best_f1 = f1_score\n",
    "                best_model = model\n",
    "                best_params = {\"learning_rate\": lr, \"weight_decay\": wd}\n",
    "    print(f\"\\nBest Model F1: {best_f1} with params {best_params}\")\n",
    "    return best_model, best_params\n",
    "\n",
    "# Step 9: 5-Fold Cross-Validation\n",
    "def cross_validation(best_params, dataset, test_loader, num_epochs=50, folds=5):\n",
    "    fold_metrics = []\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=100)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\nStarting fold {fold + 1}/{folds}\")\n",
    "\n",
    "        # Split dataset indices for training and validation\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Create DataLoaders for this fold\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model, embed_dim = get_model()\n",
    "        linear_classifier = nn.Linear(embed_dim, 2) # 2 is the number of features\n",
    "        optimizer = optim.SGD(linear_classifier.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'], momentum=0.9)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = train_model(model, linear_classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        accuracy, precision, recall, f1 = evaluate_model(model, linear_classifier, test_loader)\n",
    "        fold_metrics.append((accuracy, precision, recall, f1, end_time - start_time))\n",
    "\n",
    "    return np.array(fold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c9d149-1296-4a37-bdd7-5e09c1b21ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7013, Val Loss: 0.6968, F1 Score: 0.5311\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6994, Val Loss: 0.7001, F1 Score: 0.5625\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6962, Val Loss: 0.6938, F1 Score: 0.5955\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6952, Val Loss: 0.6947, F1 Score: 0.5635\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6922, Val Loss: 0.6890, F1 Score: 0.5434\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6863, Val Loss: 0.6848, F1 Score: 0.6228\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6851, Val Loss: 0.6821, F1 Score: 0.6258\n",
      "[cuda] Epoch 8/50, Train Loss: 0.6738, Val Loss: 0.6709, F1 Score: 0.6788\n",
      "[cuda] Epoch 9/50, Train Loss: 0.6742, Val Loss: 0.6612, F1 Score: 0.6282\n",
      "[cuda] Epoch 10/50, Train Loss: 0.6642, Val Loss: 0.6634, F1 Score: 0.6242\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6552, Val Loss: 0.6466, F1 Score: 0.7125\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6572, Val Loss: 0.6574, F1 Score: 0.6405\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6418, Val Loss: 0.6503, F1 Score: 0.6500\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6364, Val Loss: 0.6352, F1 Score: 0.6581\n",
      "[cuda] Epoch 15/50, Train Loss: 0.6362, Val Loss: 0.6471, F1 Score: 0.6380\n",
      "[cuda] Epoch 16/50, Train Loss: 0.6430, Val Loss: 0.6212, F1 Score: 0.7407\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6306, Val Loss: 0.6222, F1 Score: 0.6460\n",
      "[cuda] Epoch 18/50, Train Loss: 0.6359, Val Loss: 0.6226, F1 Score: 0.7097\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6282, Val Loss: 0.6350, F1 Score: 0.6710\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6278, Val Loss: 0.6290, F1 Score: 0.6375\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6186, Val Loss: 0.6109, F1 Score: 0.7195\n",
      "[cuda] Epoch 22/50, Train Loss: 0.6218, Val Loss: 0.6287, F1 Score: 0.6627\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6217, Val Loss: 0.6226, F1 Score: 0.6065\n",
      "[cuda] Epoch 24/50, Train Loss: 0.6304, Val Loss: 0.6219, F1 Score: 0.6259\n",
      "[cuda] Epoch 25/50, Train Loss: 0.6167, Val Loss: 0.6115, F1 Score: 0.6503\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6165, Val Loss: 0.6147, F1 Score: 0.7006\n",
      "[cuda] Epoch 27/50, Train Loss: 0.6135, Val Loss: 0.6203, F1 Score: 0.6541\n",
      "[cuda] Epoch 28/50, Train Loss: 0.6193, Val Loss: 0.6264, F1 Score: 0.6623\n",
      "[cuda] Epoch 29/50, Train Loss: 0.6257, Val Loss: 0.6082, F1 Score: 0.6797\n",
      "[cuda] Epoch 30/50, Train Loss: 0.6259, Val Loss: 0.6417, F1 Score: 0.6203\n",
      "[cuda] Epoch 31/50, Train Loss: 0.6189, Val Loss: 0.6008, F1 Score: 0.6753\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6122, Val Loss: 0.6048, F1 Score: 0.6667\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6175, Val Loss: 0.6140, F1 Score: 0.6358\n",
      "[cuda] Epoch 34/50, Train Loss: 0.6262, Val Loss: 0.6410, F1 Score: 0.6104\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6019, Val Loss: 0.6176, F1 Score: 0.6536\n",
      "[cuda] Epoch 36/50, Train Loss: 0.6155, Val Loss: 0.6154, F1 Score: 0.6708\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6312, Val Loss: 0.5977, F1 Score: 0.6795\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6221, Val Loss: 0.6246, F1 Score: 0.6581\n",
      "[cuda] Epoch 39/50, Train Loss: 0.6085, Val Loss: 0.6127, F1 Score: 0.6490\n",
      "[cuda] Epoch 40/50, Train Loss: 0.6107, Val Loss: 0.5870, F1 Score: 0.7059\n",
      "[cuda] Epoch 41/50, Train Loss: 0.6046, Val Loss: 0.6332, F1 Score: 0.6323\n",
      "[cuda] Epoch 42/50, Train Loss: 0.6125, Val Loss: 0.6230, F1 Score: 0.6329\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6243, Val Loss: 0.5937, F1 Score: 0.6623\n",
      "[cuda] Epoch 44/50, Train Loss: 0.5969, Val Loss: 0.6187, F1 Score: 0.6667\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6174, Val Loss: 0.6255, F1 Score: 0.6710\n",
      "[cuda] Epoch 46/50, Train Loss: 0.5961, Val Loss: 0.6086, F1 Score: 0.6533\n",
      "[cuda] Epoch 47/50, Train Loss: 0.6194, Val Loss: 0.6154, F1 Score: 0.6750\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6074, Val Loss: 0.6209, F1 Score: 0.5974\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6136, Val Loss: 0.6017, F1 Score: 0.6093\n",
      "[cuda] Epoch 50/50, Train Loss: 0.5870, Val Loss: 0.5939, F1 Score: 0.7190\n",
      "\n",
      "Starting fold 2/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6958, Val Loss: 0.6974, F1 Score: 0.5031\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6964, Val Loss: 0.6981, F1 Score: 0.4540\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6934, Val Loss: 0.6943, F1 Score: 0.5190\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6945, Val Loss: 0.6994, F1 Score: 0.4663\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6911, Val Loss: 0.6889, F1 Score: 0.6065\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6882, Val Loss: 0.6850, F1 Score: 0.5696\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6817, Val Loss: 0.6785, F1 Score: 0.6013\n",
      "[cuda] Epoch 8/50, Train Loss: 0.6741, Val Loss: 0.6729, F1 Score: 0.6267\n",
      "[cuda] Epoch 9/50, Train Loss: 0.6655, Val Loss: 0.6750, F1 Score: 0.6154\n",
      "[cuda] Epoch 10/50, Train Loss: 0.6626, Val Loss: 0.6553, F1 Score: 0.6792\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6634, Val Loss: 0.6601, F1 Score: 0.5931\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6521, Val Loss: 0.6554, F1 Score: 0.6486\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6365, Val Loss: 0.6462, F1 Score: 0.6267\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6430, Val Loss: 0.6549, F1 Score: 0.6500\n",
      "[cuda] Epoch 15/50, Train Loss: 0.6429, Val Loss: 0.6380, F1 Score: 0.6486\n",
      "[cuda] Epoch 16/50, Train Loss: 0.6349, Val Loss: 0.6288, F1 Score: 0.6536\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6291, Val Loss: 0.6369, F1 Score: 0.6358\n",
      "[cuda] Epoch 18/50, Train Loss: 0.6294, Val Loss: 0.6289, F1 Score: 0.7105\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6242, Val Loss: 0.6420, F1 Score: 0.6623\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6203, Val Loss: 0.6235, F1 Score: 0.6846\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6165, Val Loss: 0.6234, F1 Score: 0.6577\n",
      "[cuda] Epoch 22/50, Train Loss: 0.6351, Val Loss: 0.6421, F1 Score: 0.6174\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6277, Val Loss: 0.6224, F1 Score: 0.6887\n",
      "[cuda] Epoch 24/50, Train Loss: 0.6092, Val Loss: 0.6213, F1 Score: 0.6358\n",
      "[cuda] Epoch 25/50, Train Loss: 0.5948, Val Loss: 0.6253, F1 Score: 0.6623\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6161, Val Loss: 0.6150, F1 Score: 0.6533\n",
      "[cuda] Epoch 27/50, Train Loss: 0.6239, Val Loss: 0.6342, F1 Score: 0.6400\n",
      "[cuda] Epoch 28/50, Train Loss: 0.6085, Val Loss: 0.6368, F1 Score: 0.6582\n",
      "[cuda] Epoch 29/50, Train Loss: 0.6105, Val Loss: 0.6303, F1 Score: 0.6405\n",
      "[cuda] Epoch 30/50, Train Loss: 0.6068, Val Loss: 0.6235, F1 Score: 0.6667\n",
      "[cuda] Epoch 31/50, Train Loss: 0.6152, Val Loss: 0.5939, F1 Score: 0.7105\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6254, Val Loss: 0.5962, F1 Score: 0.7190\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6060, Val Loss: 0.6299, F1 Score: 0.6486\n",
      "[cuda] Epoch 34/50, Train Loss: 0.6135, Val Loss: 0.6295, F1 Score: 0.6358\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6157, Val Loss: 0.6012, F1 Score: 0.6573\n",
      "[cuda] Epoch 36/50, Train Loss: 0.6112, Val Loss: 0.6315, F1 Score: 0.6486\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6278, Val Loss: 0.6197, F1 Score: 0.6800\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6073, Val Loss: 0.6216, F1 Score: 0.6122\n",
      "[cuda] Epoch 39/50, Train Loss: 0.6202, Val Loss: 0.6060, F1 Score: 0.6797\n",
      "[cuda] Epoch 40/50, Train Loss: 0.5962, Val Loss: 0.6189, F1 Score: 0.6667\n",
      "[cuda] Epoch 41/50, Train Loss: 0.5985, Val Loss: 0.5942, F1 Score: 0.6800\n",
      "[cuda] Epoch 42/50, Train Loss: 0.6206, Val Loss: 0.6130, F1 Score: 0.6711\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6085, Val Loss: 0.6380, F1 Score: 0.6133\n",
      "[cuda] Epoch 44/50, Train Loss: 0.6103, Val Loss: 0.6393, F1 Score: 0.6267\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6134, Val Loss: 0.5999, F1 Score: 0.6842\n",
      "[cuda] Epoch 46/50, Train Loss: 0.6031, Val Loss: 0.6026, F1 Score: 0.6759\n",
      "[cuda] Epoch 47/50, Train Loss: 0.5882, Val Loss: 0.6365, F1 Score: 0.6358\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6134, Val Loss: 0.6277, F1 Score: 0.6757\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6161, Val Loss: 0.6153, F1 Score: 0.6400\n",
      "[cuda] Epoch 50/50, Train Loss: 0.6043, Val Loss: 0.6161, F1 Score: 0.7226\n",
      "\n",
      "Starting fold 3/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6954, Val Loss: 0.6951, F1 Score: 0.4744\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6932, Val Loss: 0.6963, F1 Score: 0.5000\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6941, Val Loss: 0.6900, F1 Score: 0.5714\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6879, Val Loss: 0.6888, F1 Score: 0.6087\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6882, Val Loss: 0.6898, F1 Score: 0.5455\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6808, Val Loss: 0.6796, F1 Score: 0.6013\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6743, Val Loss: 0.6783, F1 Score: 0.6351\n",
      "[cuda] Epoch 8/50, Train Loss: 0.6698, Val Loss: 0.6786, F1 Score: 0.6184\n",
      "[cuda] Epoch 9/50, Train Loss: 0.6658, Val Loss: 0.6657, F1 Score: 0.6624\n",
      "[cuda] Epoch 10/50, Train Loss: 0.6572, Val Loss: 0.6612, F1 Score: 0.6538\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6528, Val Loss: 0.6684, F1 Score: 0.6093\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6462, Val Loss: 0.6514, F1 Score: 0.6364\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6466, Val Loss: 0.6437, F1 Score: 0.6194\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6366, Val Loss: 0.6371, F1 Score: 0.6713\n",
      "[cuda] Epoch 15/50, Train Loss: 0.6380, Val Loss: 0.6533, F1 Score: 0.6400\n",
      "[cuda] Epoch 16/50, Train Loss: 0.6247, Val Loss: 0.6390, F1 Score: 0.6443\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6229, Val Loss: 0.6496, F1 Score: 0.5921\n",
      "[cuda] Epoch 18/50, Train Loss: 0.6257, Val Loss: 0.6627, F1 Score: 0.6081\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6167, Val Loss: 0.6268, F1 Score: 0.6483\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6287, Val Loss: 0.6341, F1 Score: 0.6027\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6198, Val Loss: 0.6417, F1 Score: 0.6301\n",
      "[cuda] Epoch 22/50, Train Loss: 0.6169, Val Loss: 0.6379, F1 Score: 0.6174\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6227, Val Loss: 0.6262, F1 Score: 0.6216\n",
      "[cuda] Epoch 24/50, Train Loss: 0.6218, Val Loss: 0.6218, F1 Score: 0.6443\n",
      "[cuda] Epoch 25/50, Train Loss: 0.6162, Val Loss: 0.6282, F1 Score: 0.6122\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6093, Val Loss: 0.6546, F1 Score: 0.6111\n",
      "[cuda] Epoch 27/50, Train Loss: 0.6186, Val Loss: 0.6439, F1 Score: 0.6122\n",
      "[cuda] Epoch 28/50, Train Loss: 0.6138, Val Loss: 0.6324, F1 Score: 0.6081\n",
      "[cuda] Epoch 29/50, Train Loss: 0.5983, Val Loss: 0.6582, F1 Score: 0.5695\n",
      "[cuda] Epoch 30/50, Train Loss: 0.6223, Val Loss: 0.6064, F1 Score: 0.6622\n",
      "[cuda] Epoch 31/50, Train Loss: 0.6065, Val Loss: 0.6123, F1 Score: 0.6797\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6029, Val Loss: 0.6204, F1 Score: 0.6443\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6133, Val Loss: 0.6430, F1 Score: 0.6216\n",
      "[cuda] Epoch 34/50, Train Loss: 0.6169, Val Loss: 0.6455, F1 Score: 0.5986\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6161, Val Loss: 0.6336, F1 Score: 0.5957\n",
      "[cuda] Epoch 36/50, Train Loss: 0.6114, Val Loss: 0.6170, F1 Score: 0.6622\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6088, Val Loss: 0.6399, F1 Score: 0.6338\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6149, Val Loss: 0.6428, F1 Score: 0.6225\n",
      "[cuda] Epoch 39/50, Train Loss: 0.5974, Val Loss: 0.6275, F1 Score: 0.6345\n",
      "[cuda] Epoch 40/50, Train Loss: 0.6151, Val Loss: 0.6265, F1 Score: 0.6197\n",
      "[cuda] Epoch 41/50, Train Loss: 0.6099, Val Loss: 0.6438, F1 Score: 0.6164\n",
      "[cuda] Epoch 42/50, Train Loss: 0.5990, Val Loss: 0.6247, F1 Score: 0.6197\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6174, Val Loss: 0.6002, F1 Score: 0.6713\n",
      "[cuda] Epoch 44/50, Train Loss: 0.6234, Val Loss: 0.6514, F1 Score: 0.5946\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6089, Val Loss: 0.6519, F1 Score: 0.6351\n",
      "[cuda] Epoch 46/50, Train Loss: 0.5996, Val Loss: 0.6080, F1 Score: 0.6712\n",
      "[cuda] Epoch 47/50, Train Loss: 0.5979, Val Loss: 0.6186, F1 Score: 0.6338\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6057, Val Loss: 0.6245, F1 Score: 0.5986\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6159, Val Loss: 0.6205, F1 Score: 0.6486\n",
      "[cuda] Epoch 50/50, Train Loss: 0.6075, Val Loss: 0.6089, F1 Score: 0.6667\n",
      "\n",
      "Starting fold 4/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6944, Val Loss: 0.6985, F1 Score: 0.5198\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6966, Val Loss: 0.6931, F1 Score: 0.5486\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6941, Val Loss: 0.6974, F1 Score: 0.5363\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6918, Val Loss: 0.6945, F1 Score: 0.5967\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6865, Val Loss: 0.6910, F1 Score: 0.5614\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6864, Val Loss: 0.6823, F1 Score: 0.6514\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6782, Val Loss: 0.6789, F1 Score: 0.6228\n",
      "[cuda] Epoch 8/50, Train Loss: 0.6763, Val Loss: 0.6696, F1 Score: 0.6626\n",
      "[cuda] Epoch 9/50, Train Loss: 0.6695, Val Loss: 0.6569, F1 Score: 0.6867\n",
      "[cuda] Epoch 10/50, Train Loss: 0.6634, Val Loss: 0.6540, F1 Score: 0.6909\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6537, Val Loss: 0.6547, F1 Score: 0.6282\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6541, Val Loss: 0.6436, F1 Score: 0.6456\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6422, Val Loss: 0.6338, F1 Score: 0.6797\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6468, Val Loss: 0.6542, F1 Score: 0.6093\n",
      "[cuda] Epoch 15/50, Train Loss: 0.6420, Val Loss: 0.6434, F1 Score: 0.6581\n",
      "[cuda] Epoch 16/50, Train Loss: 0.6349, Val Loss: 0.6438, F1 Score: 0.6184\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6378, Val Loss: 0.6376, F1 Score: 0.6538\n",
      "[cuda] Epoch 18/50, Train Loss: 0.6158, Val Loss: 0.6191, F1 Score: 0.6800\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6181, Val Loss: 0.6266, F1 Score: 0.6933\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6322, Val Loss: 0.6437, F1 Score: 0.5906\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6233, Val Loss: 0.6294, F1 Score: 0.6497\n",
      "[cuda] Epoch 22/50, Train Loss: 0.6461, Val Loss: 0.6014, F1 Score: 0.6887\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6208, Val Loss: 0.6180, F1 Score: 0.6710\n",
      "[cuda] Epoch 24/50, Train Loss: 0.6165, Val Loss: 0.6167, F1 Score: 0.6536\n",
      "[cuda] Epoch 25/50, Train Loss: 0.6184, Val Loss: 0.6430, F1 Score: 0.6065\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6294, Val Loss: 0.6371, F1 Score: 0.6710\n",
      "[cuda] Epoch 27/50, Train Loss: 0.6203, Val Loss: 0.6315, F1 Score: 0.6752\n",
      "[cuda] Epoch 28/50, Train Loss: 0.6199, Val Loss: 0.6325, F1 Score: 0.6093\n",
      "[cuda] Epoch 29/50, Train Loss: 0.6188, Val Loss: 0.6313, F1 Score: 0.6494\n",
      "[cuda] Epoch 30/50, Train Loss: 0.6267, Val Loss: 0.6325, F1 Score: 0.6710\n",
      "[cuda] Epoch 31/50, Train Loss: 0.6247, Val Loss: 0.6444, F1 Score: 0.6405\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6126, Val Loss: 0.6199, F1 Score: 0.6452\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6085, Val Loss: 0.6260, F1 Score: 0.6533\n",
      "[cuda] Epoch 34/50, Train Loss: 0.6045, Val Loss: 0.6247, F1 Score: 0.6624\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6099, Val Loss: 0.6297, F1 Score: 0.6667\n",
      "[cuda] Epoch 36/50, Train Loss: 0.5897, Val Loss: 0.6231, F1 Score: 0.6395\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6188, Val Loss: 0.6263, F1 Score: 0.6538\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6239, Val Loss: 0.6141, F1 Score: 0.6452\n",
      "[cuda] Epoch 39/50, Train Loss: 0.6205, Val Loss: 0.6480, F1 Score: 0.5811\n",
      "[cuda] Epoch 40/50, Train Loss: 0.6153, Val Loss: 0.6176, F1 Score: 0.6753\n",
      "[cuda] Epoch 41/50, Train Loss: 0.5981, Val Loss: 0.6360, F1 Score: 0.6883\n",
      "[cuda] Epoch 42/50, Train Loss: 0.6096, Val Loss: 0.6001, F1 Score: 0.7051\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6090, Val Loss: 0.6148, F1 Score: 0.6709\n",
      "[cuda] Epoch 44/50, Train Loss: 0.6000, Val Loss: 0.6098, F1 Score: 0.6309\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6035, Val Loss: 0.6180, F1 Score: 0.6395\n",
      "[cuda] Epoch 46/50, Train Loss: 0.6022, Val Loss: 0.6154, F1 Score: 0.6752\n",
      "[cuda] Epoch 47/50, Train Loss: 0.6236, Val Loss: 0.6271, F1 Score: 0.6623\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6099, Val Loss: 0.6097, F1 Score: 0.6800\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6233, Val Loss: 0.6340, F1 Score: 0.6581\n",
      "[cuda] Epoch 50/50, Train Loss: 0.6107, Val Loss: 0.6219, F1 Score: 0.6753\n",
      "\n",
      "Starting fold 5/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6920, Val Loss: 0.6934, F1 Score: 0.4627\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6929, Val Loss: 0.6905, F1 Score: 0.5373\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6899, Val Loss: 0.6898, F1 Score: 0.4965\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6894, Val Loss: 0.6894, F1 Score: 0.4681\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6854, Val Loss: 0.6846, F1 Score: 0.6081\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6830, Val Loss: 0.6797, F1 Score: 0.6164\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6792, Val Loss: 0.6764, F1 Score: 0.5205\n",
      "[cuda] Epoch 8/50, Train Loss: 0.6721, Val Loss: 0.6748, F1 Score: 0.5844\n",
      "[cuda] Epoch 9/50, Train Loss: 0.6696, Val Loss: 0.6654, F1 Score: 0.6667\n",
      "[cuda] Epoch 10/50, Train Loss: 0.6585, Val Loss: 0.6605, F1 Score: 0.6497\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6611, Val Loss: 0.6556, F1 Score: 0.6711\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6486, Val Loss: 0.6583, F1 Score: 0.6289\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6435, Val Loss: 0.6438, F1 Score: 0.6711\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6423, Val Loss: 0.6414, F1 Score: 0.6400\n",
      "[cuda] Epoch 15/50, Train Loss: 0.6425, Val Loss: 0.6475, F1 Score: 0.6536\n",
      "[cuda] Epoch 16/50, Train Loss: 0.6386, Val Loss: 0.6455, F1 Score: 0.6093\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6436, Val Loss: 0.6320, F1 Score: 0.7407\n",
      "[cuda] Epoch 18/50, Train Loss: 0.6272, Val Loss: 0.6279, F1 Score: 0.6835\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6303, Val Loss: 0.6221, F1 Score: 0.7000\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6176, Val Loss: 0.6062, F1 Score: 0.7342\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6192, Val Loss: 0.6413, F1 Score: 0.6709\n",
      "[cuda] Epoch 22/50, Train Loss: 0.6310, Val Loss: 0.6320, F1 Score: 0.6711\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6142, Val Loss: 0.6127, F1 Score: 0.6795\n",
      "[cuda] Epoch 24/50, Train Loss: 0.6279, Val Loss: 0.6284, F1 Score: 0.6267\n",
      "[cuda] Epoch 25/50, Train Loss: 0.6397, Val Loss: 0.5872, F1 Score: 0.7296\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6171, Val Loss: 0.6008, F1 Score: 0.7190\n",
      "[cuda] Epoch 27/50, Train Loss: 0.6088, Val Loss: 0.6143, F1 Score: 0.7000\n",
      "[cuda] Epoch 28/50, Train Loss: 0.6258, Val Loss: 0.6130, F1 Score: 0.6790\n",
      "[cuda] Epoch 29/50, Train Loss: 0.6180, Val Loss: 0.6035, F1 Score: 0.7000\n",
      "[cuda] Epoch 30/50, Train Loss: 0.6108, Val Loss: 0.5920, F1 Score: 0.7375\n",
      "[cuda] Epoch 31/50, Train Loss: 0.6019, Val Loss: 0.6385, F1 Score: 0.6316\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6100, Val Loss: 0.5951, F1 Score: 0.6752\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6262, Val Loss: 0.5995, F1 Score: 0.6839\n",
      "[cuda] Epoch 34/50, Train Loss: 0.5945, Val Loss: 0.6201, F1 Score: 0.6267\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6159, Val Loss: 0.6161, F1 Score: 0.6962\n",
      "[cuda] Epoch 36/50, Train Loss: 0.6008, Val Loss: 0.6192, F1 Score: 0.6875\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6255, Val Loss: 0.6051, F1 Score: 0.7000\n",
      "[cuda] Epoch 38/50, Train Loss: 0.5926, Val Loss: 0.5938, F1 Score: 0.6839\n",
      "[cuda] Epoch 39/50, Train Loss: 0.5930, Val Loss: 0.5920, F1 Score: 0.7320\n",
      "[cuda] Epoch 40/50, Train Loss: 0.6089, Val Loss: 0.6175, F1 Score: 0.6832\n",
      "[cuda] Epoch 41/50, Train Loss: 0.6234, Val Loss: 0.6121, F1 Score: 0.6974\n",
      "[cuda] Epoch 42/50, Train Loss: 0.6220, Val Loss: 0.5790, F1 Score: 0.7170\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6284, Val Loss: 0.5977, F1 Score: 0.7051\n",
      "[cuda] Epoch 44/50, Train Loss: 0.6142, Val Loss: 0.6107, F1 Score: 0.6832\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6083, Val Loss: 0.6171, F1 Score: 0.6623\n",
      "[cuda] Epoch 46/50, Train Loss: 0.6221, Val Loss: 0.6275, F1 Score: 0.6497\n",
      "[cuda] Epoch 47/50, Train Loss: 0.6226, Val Loss: 0.6034, F1 Score: 0.6486\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6196, Val Loss: 0.5911, F1 Score: 0.6835\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6294, Val Loss: 0.6164, F1 Score: 0.6752\n",
      "[cuda] Epoch 50/50, Train Loss: 0.6175, Val Loss: 0.6255, F1 Score: 0.6709\n",
      "\n",
      "Average metrics over 5 folds in test set:\n",
      "Accuracy: 0.7140 ± 0.0486\n",
      "Precision: 0.7600 ± 0.0438\n",
      "Recall: 0.6783 ± 0.0621\n",
      "F1 Score: 0.7164 ± 0.0525\n",
      "Training Time per Fold: 1921.89 ± 31.55 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run the 5-fold cross-validation\n",
    "metrics = cross_validation(best_params, train_val_dataset, test_loader, num_epochs=50, folds=5)\n",
    "\n",
    "# Calculate average and standard deviation of metrics across folds\n",
    "avg_metrics = metrics.mean(axis=0)\n",
    "std_metrics = metrics.std(axis=0)\n",
    "\n",
    "print(f\"\\nAverage metrics over 5 folds in test set:\\n\"\n",
    "      f\"Accuracy: {avg_metrics[0]:.4f} ± {std_metrics[0]:.4f}\\n\"\n",
    "      f\"Precision: {avg_metrics[1]:.4f} ± {std_metrics[1]:.4f}\\n\"\n",
    "      f\"Recall: {avg_metrics[2]:.4f} ± {std_metrics[2]:.4f}\\n\"\n",
    "      f\"F1 Score: {avg_metrics[3]:.4f} ± {std_metrics[3]:.4f}\\n\"\n",
    "      f\"Training Time per Fold: {avg_metrics[4]:.2f} ± {std_metrics[4]:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8100e171-722d-4875-bd6b-eeaf72178470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.44186047e-01, 7.85714286e-01, 7.17391304e-01, 7.50000000e-01,\n",
       "        1.98234977e+03],\n",
       "       [6.97674419e-01, 7.63157895e-01, 6.30434783e-01, 6.90476190e-01,\n",
       "        1.88929990e+03],\n",
       "       [7.32558140e-01, 7.80487805e-01, 6.95652174e-01, 7.35632184e-01,\n",
       "        1.91234425e+03],\n",
       "       [7.67441860e-01, 7.95454545e-01, 7.60869565e-01, 7.77777778e-01,\n",
       "        1.91313160e+03],\n",
       "       [6.27906977e-01, 6.75000000e-01, 5.86956522e-01, 6.27906977e-01,\n",
       "        1.91234071e+03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06b70805-0d9f-41ed-a33d-9d23c147c4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>training time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1982.349771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>1889.299904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>1912.344251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1913.131603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>1912.340708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall        f1  training time\n",
       "0  0.744186   0.785714  0.717391  0.750000    1982.349771\n",
       "1  0.697674   0.763158  0.630435  0.690476    1889.299904\n",
       "2  0.732558   0.780488  0.695652  0.735632    1912.344251\n",
       "3  0.767442   0.795455  0.760870  0.777778    1913.131603\n",
       "4  0.627907   0.675000  0.586957  0.627907    1912.340708"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(metrics, columns=['accuracy', 'precision', 'recall', 'f1', 'training time'])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c59cbb0-4d5f-40de-94e6-f66d6e3d0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/byol_fine_tuned_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c580b57-893c-4ff1-8b6f-d11c9393c61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
