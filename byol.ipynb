{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a115a2-48e0-4eb7-b55b-f5bd08c804d8",
   "metadata": {},
   "source": [
    "# BYOL pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ec9d08-6b6f-41cb-9248-92b921d7b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def reproducibility(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "\n",
    "def define_param_groups(model, weight_decay, optimizer_name):\n",
    "    def exclude_from_wd_and_adaptation(name):\n",
    "        if 'bn' in name:\n",
    "            return True\n",
    "        if optimizer_name == 'lars' and 'bias' in name:\n",
    "            return True\n",
    "\n",
    "    param_groups = [\n",
    "        {\n",
    "            'params': [p for name, p in model.named_parameters() if not exclude_from_wd_and_adaptation(name)],\n",
    "            'weight_decay': weight_decay,\n",
    "            'layer_adaptation': True,\n",
    "        },\n",
    "        {\n",
    "            'params': [p for name, p in model.named_parameters() if exclude_from_wd_and_adaptation(name)],\n",
    "            'weight_decay': 0.,\n",
    "            'layer_adaptation': False,\n",
    "        },\n",
    "    ]\n",
    "    return param_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568aa243-1b87-4f7c-904a-d41c7af0f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "\n",
    "class Augment:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module\n",
    "    Transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, s=1):\n",
    "        color_jitter = T.ColorJitter(\n",
    "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
    "        )\n",
    "        blur = T.GaussianBlur((3, 3), (0.1, 2.0))\n",
    "\n",
    "        self.train_transform = T.transforms.Compose([\n",
    "            T.RandomResizedCrop(img_size, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "            T.RandomApply(\n",
    "                    [T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                    p=0.8\n",
    "                ),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "            T.Resize((256, 256)),\n",
    "            T.CenterCrop(img_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x), \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff10a1a-766e-4b90-a303-7155cc4c5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.multiprocessing import cpu_count\n",
    "\n",
    "def get_data_loader(dataset_path, batch_size, transform=None, shuffle=False):\n",
    "    dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=cpu_count()//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dbf4ce9-8746-4b98-a2c3-5ae4b52dc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embedding_size=256, hidden_size=2048, batch_norm_mlp=False):\n",
    "        super().__init__()\n",
    "        norm = nn.BatchNorm1d(hidden_size) if batch_norm_mlp else nn.Identity()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            norm,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AddProjHead(nn.Module):\n",
    "    def __init__(self, model, in_features, hidden_size=4096,\n",
    "                 embedding_size=256, batch_norm_mlp=True):\n",
    "        super(AddProjHead, self).__init__()\n",
    "        self.backbone = model\n",
    "        # remove last layer\n",
    "        self.backbone.classifier[1] = nn.Identity()\n",
    "        self.backbone.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.backbone.maxpool = torch.nn.Identity()\n",
    "        # add mlp projection head\n",
    "        self.projection = MLP(in_features, embedding_size, hidden_size=hidden_size, batch_norm_mlp=batch_norm_mlp)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        embedding = self.backbone(x)\n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        return self.projection(embedding)\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    # L2 normalization\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.alpha + (1 - self.alpha) * new\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            net,\n",
    "            batch_norm_mlp=True,\n",
    "            in_features=512,\n",
    "            projection_size=256,\n",
    "            projection_hidden_size=2048,\n",
    "            moving_average_decay=0.99,\n",
    "            use_momentum=True,\n",
    "            device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            net: model to be trained\n",
    "            batch_norm_mlp: whether to use batchnorm1d in the mlp predictor and projector\n",
    "            in_features: the number features that are produced by the backbone net i.e. resnet\n",
    "            projection_size: the size of the output vector of the two identical MLPs\n",
    "            projection_hidden_size: the size of the hidden vector of the two identical MLPs\n",
    "            augment_fn2: apply different augmentation the second view\n",
    "            moving_average_decay: t hyperparameter to control the influence in the target network weight update\n",
    "            use_momentum: whether to update the target network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.student_model = AddProjHead(model=net, in_features=in_features,\n",
    "                                         embedding_size=projection_size,\n",
    "                                         hidden_size=projection_hidden_size,\n",
    "                                         batch_norm_mlp=batch_norm_mlp)\n",
    "        self.use_momentum = use_momentum\n",
    "        self.teacher_model = self._get_teacher()\n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "        self.student_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
    "        self.device = device\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_teacher(self):\n",
    "        return copy.deepcopy(self.student_model)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_moving_average(self):\n",
    "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum ' \\\n",
    "                                  'for the target encoder '\n",
    "        assert self.teacher_model is not None, 'target encoder has not been created yet'\n",
    "\n",
    "        for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):\n",
    "          old_weight, up_weight = teacher_params.data, student_params.data\n",
    "          teacher_params.data = self.target_ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            image_one, image_two=None,\n",
    "            return_embedding=False):\n",
    "        if return_embedding or (image_two is None):\n",
    "            return self.student_model(image_one, return_embedding=True)\n",
    "\n",
    "        image_one, image_two = image_one.to(self.device), image_two.to(self.device)\n",
    "\n",
    "        # student projections: backbone + MLP projection\n",
    "        student_proj_one = self.student_model(image_one)\n",
    "        student_proj_two = self.student_model(image_two)\n",
    "\n",
    "        # additional student's MLP head called predictor\n",
    "        student_pred_one = self.student_predictor(student_proj_one)\n",
    "        student_pred_two = self.student_predictor(student_proj_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # teacher processes the images and makes projections: backbone + MLP\n",
    "            teacher_proj_one = self.teacher_model(image_one).detach_()\n",
    "            teacher_proj_two = self.teacher_model(image_two).detach_()\n",
    "            \n",
    "        loss_one = loss_fn(student_pred_one, teacher_proj_one)\n",
    "        loss_two = loss_fn(student_pred_two, teacher_proj_two)\n",
    "\n",
    "        # Free tensors\n",
    "        del image_one, image_two\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return (loss_one + loss_two).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212a0978-95bb-438c-8489-c7798b7fa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/noahgolmant/pytorch-lars\n",
    "\"\"\" Layer-wise adaptive rate scaling for SGD in PyTorch! \"\"\"\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    r\"\"\"Implements layer-wise adaptive rate scaling for SGD.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): base learning rate (\\gamma_0)\n",
    "        momentum (float, optional): momentum factor (default: 0) (\"m\")\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "            (\"\\beta\")\n",
    "        eta (float, optional): LARS coefficient\n",
    "        max_epoch: maximum training epoch to determine polynomial LR decay.\n",
    "\n",
    "    Based on Algorithm 1 of the following paper by You, Gitman, and Ginsburg.\n",
    "    Large Batch Training of Convolutional Networks:\n",
    "        https://arxiv.org/abs/1708.03888\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = LARS(model.parameters(), lr=0.1, eta=1e-3)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, momentum=.9,\n",
    "                 weight_decay=.0005, eta=0.001, max_epoch=200):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\"\n",
    "                             .format(weight_decay))\n",
    "        if eta < 0.0:\n",
    "            raise ValueError(\"Invalid LARS coefficient value: {}\".format(eta))\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(lr=lr, momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        eta=eta, max_epoch=max_epoch)\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "            epoch: current epoch to calculate polynomial LR decay schedule.\n",
    "                   if None, uses self.epoch and increments it.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            eta = group['eta']\n",
    "            lr = group['lr']\n",
    "            max_epoch = group['max_epoch']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                weight_norm = torch.norm(p.data)\n",
    "                grad_norm = torch.norm(d_p)\n",
    "\n",
    "                # Global LR computed on polynomial decay schedule\n",
    "                decay = (1 - float(epoch) / max_epoch) ** 2\n",
    "                global_lr = lr * decay\n",
    "\n",
    "                # Compute local learning rate for this layer\n",
    "                local_lr = eta * weight_norm / \\\n",
    "                    (grad_norm + weight_decay * weight_norm)\n",
    "\n",
    "                # Update the momentum term\n",
    "                actual_lr = local_lr * global_lr\n",
    "\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = \\\n",
    "                            torch.zeros_like(p.data)\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
    "                p.data.add_(-buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b852d-1e2d-4872-bc85-caae99eda023",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4578ce04-012e-4549-8015-c6c262ac0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, data):\n",
    "    (view1, view2), _ = data\n",
    "    loss = model(view1, view2)\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(epoch, model, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    batch_loader = tqdm(train_dataloader)\n",
    "    for data in batch_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = training_step(model, data)\n",
    "        loss.backward()\n",
    "        optimizer.step(epoch)\n",
    "        # EMA update\n",
    "        model.update_moving_average()\n",
    "        total_loss += loss.item()        \n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb8f5e-b8b5-4ccd-964e-093cac3a38f1",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b977c879-bd56-40d9-bfad-56816a74f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536dce19-d503-4790-bc50-c88ad828a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set train params\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "base_lr = 0.00025\n",
    "weight_decay = 1e-6\n",
    "load = False\n",
    "img_size = 224\n",
    "random_state = 9999\n",
    "epochs = 100\n",
    "dataset_path = './datasets/chestx-ray14-v3'\n",
    "save_path = './output/BYOL'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ade349-9c77-4c4c-8f7f-4e231dd0a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_gpus: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/378 [00:00<?, ?it/s]/tmp/user/1781838319/ipykernel_1966727/2481369168.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]  loss: 1476.0496220588684, time: 218.5489022731781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/100]  loss: 1416.528881072998, time: 218.42292022705078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/100]  loss: 1358.496309041977, time: 220.7168426513672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/100]  loss: 1301.3147213459015, time: 217.19876337051392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:36<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/100]  loss: 1245.082380771637, time: 216.11908888816833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/100]  loss: 1189.805014371872, time: 224.02072262763977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/100]  loss: 1136.2518169879913, time: 219.82707405090332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/100]  loss: 1084.2689294815063, time: 219.19778537750244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/100]  loss: 1034.1634969711304, time: 220.17089653015137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/100]  loss: 985.8675229549408, time: 221.01469039916992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/100]  loss: 939.1303775310516, time: 223.1777594089508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/100]  loss: 895.456535577774, time: 220.55511713027954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/100]  loss: 852.8173716068268, time: 220.98689007759094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/100]  loss: 813.0983912944794, time: 221.0131106376648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/100]  loss: 774.9590517282486, time: 220.21421766281128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/100]  loss: 739.1129704713821, time: 220.46480870246887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17/100]  loss: 705.0421134233475, time: 221.75275874137878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18/100]  loss: 672.6817049980164, time: 223.96609210968018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19/100]  loss: 644.1548007726669, time: 222.67795085906982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/100]  loss: 615.6733196973801, time: 219.791246175766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21/100]  loss: 589.4869517087936, time: 217.5228841304779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22/100]  loss: 564.8541812896729, time: 220.46356010437012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23/100]  loss: 542.2422502040863, time: 220.5545449256897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:44<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24/100]  loss: 521.0041689872742, time: 224.69537544250488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25/100]  loss: 501.23231506347656, time: 218.0443093776703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26/100]  loss: 482.55941474437714, time: 220.07749938964844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27/100]  loss: 465.6174545288086, time: 219.41563415527344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28/100]  loss: 449.2224737405777, time: 222.04447436332703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29/100]  loss: 433.9937278032303, time: 219.79143118858337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30/100]  loss: 419.77595925331116, time: 220.4432396888733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31/100]  loss: 406.84699630737305, time: 221.72211837768555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32/100]  loss: 393.8928542137146, time: 222.625013589859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33/100]  loss: 381.6847906112671, time: 222.80300164222717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34/100]  loss: 370.3251419663429, time: 218.2185342311859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35/100]  loss: 359.6560165286064, time: 220.17843914031982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36/100]  loss: 349.55038744211197, time: 221.4720332622528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37/100]  loss: 339.3631182909012, time: 221.60188722610474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38/100]  loss: 329.8687870502472, time: 220.13431572914124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39/100]  loss: 320.0827084183693, time: 220.59111785888672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40/100]  loss: 312.01315027475357, time: 218.36993622779846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:36<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41/100]  loss: 303.6997622847557, time: 216.7426359653473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:36<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [42/100]  loss: 295.2965255975723, time: 216.5448660850525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43/100]  loss: 287.5199142098427, time: 218.34203028678894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:35<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [44/100]  loss: 279.1954710483551, time: 215.78234457969666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [45/100]  loss: 271.63077026605606, time: 218.94269704818726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46/100]  loss: 264.0036253333092, time: 220.5144019126892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:35<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47/100]  loss: 256.93730741739273, time: 215.49614024162292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [48/100]  loss: 249.10003793239594, time: 217.35830545425415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [49/100]  loss: 242.08064019680023, time: 218.30352234840393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50/100]  loss: 234.61500799655914, time: 217.60145783424377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51/100]  loss: 226.45486617088318, time: 219.12078189849854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [52/100]  loss: 219.5613335967064, time: 221.24844408035278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [53/100]  loss: 213.5986403822899, time: 221.20472621917725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [54/100]  loss: 206.6450690329075, time: 221.06181526184082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [55/100]  loss: 199.3621475994587, time: 221.4003381729126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [56/100]  loss: 193.8524348139763, time: 219.11986422538757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [57/100]  loss: 188.1130275428295, time: 222.56556034088135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [58/100]  loss: 182.0323634147644, time: 219.4078221321106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [59/100]  loss: 176.4021344780922, time: 220.7328643798828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [60/100]  loss: 171.29819810390472, time: 223.3149757385254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:36<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [61/100]  loss: 166.28639578819275, time: 216.53901624679565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:45<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [62/100]  loss: 160.99533289670944, time: 225.53330063819885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [63/100]  loss: 156.86253944039345, time: 219.33447766304016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [64/100]  loss: 152.162741035223, time: 219.12579250335693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [65/100]  loss: 148.532780200243, time: 219.55377388000488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [66/100]  loss: 145.59745517373085, time: 220.483873128891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [67/100]  loss: 141.43593084812164, time: 220.21553254127502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [68/100]  loss: 139.44535249471664, time: 220.38431525230408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69/100]  loss: 135.43774977326393, time: 222.31989431381226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:36<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [70/100]  loss: 132.1739390194416, time: 216.58298444747925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71/100]  loss: 129.81909877061844, time: 219.42064595222473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [72/100]  loss: 126.94302451610565, time: 223.10724997520447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:47<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [73/100]  loss: 124.15543043613434, time: 227.08160519599915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [74/100]  loss: 122.4912918806076, time: 220.00597739219666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [75/100]  loss: 119.87205690145493, time: 220.82000637054443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [76/100]  loss: 117.25628317892551, time: 220.03652691841125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [77/100]  loss: 117.0462099313736, time: 219.36524772644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [78/100]  loss: 114.20403079688549, time: 218.27231621742249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [79/100]  loss: 111.10799485445023, time: 217.55916619300842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [80/100]  loss: 109.77909138798714, time: 221.43323612213135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [81/100]  loss: 109.09502084553242, time: 221.27588868141174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [82/100]  loss: 108.1492974460125, time: 223.079243183136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83/100]  loss: 106.29648295044899, time: 220.94491386413574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [84/100]  loss: 104.63852053880692, time: 219.64536476135254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [85/100]  loss: 103.92285995185375, time: 219.02569913864136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [86/100]  loss: 102.64099545776844, time: 219.38883900642395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:42<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [87/100]  loss: 100.69512122869492, time: 222.5781226158142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [88/100]  loss: 99.28371553122997, time: 223.96272087097168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [89/100]  loss: 98.6124588996172, time: 220.14168119430542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [90/100]  loss: 96.18326964974403, time: 217.87355065345764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [91/100]  loss: 97.25507733225822, time: 219.9481189250946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [92/100]  loss: 95.49864946305752, time: 220.25804114341736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [93/100]  loss: 95.29018370807171, time: 220.38589549064636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:46<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [94/100]  loss: 94.62454713881016, time: 226.017480134964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [95/100]  loss: 93.22381019592285, time: 218.91539120674133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [96/100]  loss: 91.5633347928524, time: 221.10817170143127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [97/100]  loss: 91.32360039651394, time: 223.20523691177368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:41<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [98/100]  loss: 90.66890200972557, time: 221.72404217720032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:39<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [99/100]  loss: 89.47113344073296, time: 219.22381329536438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:40<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]  loss: 88.79756358265877, time: 220.58062291145325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.efficientnet_b0(pretrained=True)\n",
    "model = BYOL(model, in_features=model.classifier[1].in_features, batch_norm_mlp=True, device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = LARS(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "\n",
    "# data\n",
    "transform = Augment(img_size)\n",
    "\n",
    "test_transform = T.Compose([\n",
    "            T.CenterCrop(img_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])\n",
    "\n",
    "\n",
    "loader_train = get_data_loader(f'{dataset_path}/train', batch_size, transform=transform)\n",
    "\n",
    "# general info\n",
    "available_gpus = len([torch.cuda.device(i) for i in range(torch.cuda.device_count())])\n",
    "print('available_gpus:', available_gpus)\n",
    "\n",
    "reproducibility(random_state)\n",
    "\n",
    "if load:\n",
    "  model.load_state_dict(torch.load(\"....ckpt\"))\n",
    "\n",
    "\n",
    "mean_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_one_epoch(epoch, model, loader_train, optimizer)\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch: [{epoch+1}/{epochs}]  loss: {np.mean(train_loss)}, time: {end_time - start_time}')\n",
    "    mean_losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3f39c3-33ee-497a-aa58-5b31805c1d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "indices = range(0,100,1)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(indices, mean_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c107fb98-ca70-46ae-8252-b22bfcfda773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): ConvNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): ConvNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): ConvNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Identity()\n",
      "  )\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (maxpool): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "model_backbone_weights = model.student_model.backbone\n",
    "print(model_backbone_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e562c1-858d-439c-b062-a9dc6668b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " ! mkdir ./output/BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293d0c53-0882-4589-bf3d-a791c07e925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({ 'model_state_dict': model_backbone_weights.state_dict() }, f'{save_path}/efficientnet_b0_backbone_weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2eea27-1701-4da6-a1ad-aee8ec8f7efe",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e80c3f-6e86-4992-a38e-f123e00cbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = './datasets/COVIDGR_1.0'\n",
    "positive_path = os.path.join(dataset_path, 'P')\n",
    "negative_path = os.path.join(dataset_path, 'N')\n",
    "\n",
    "# Data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomApply(\n",
    "            [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "            p=0.8\n",
    "        ),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "854e6e57-6e0a-479b-9e5d-c8cfdcdc2ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 852\n",
       "    Root location: ./datasets/COVIDGR_1.0\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(224, 224), scale=(0.3, 0.9), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
       "               RandomApply(\n",
       "               p=0.8\n",
       "               ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.8, 1.2], hue=[-0.1, 0.1])\n",
       "           )\n",
       "               RandomGrayscale(p=0.2)\n",
       "               RandomApply(\n",
       "               p=0.5\n",
       "               GaussianBlur(kernel_size=(25, 25), sigma=(0.1, 2.0))\n",
       "           )\n",
       "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "           )"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Map labels\n",
    "dataset.class_to_idx = {'N': 0, 'P': 1}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f13de976-7e60-41d8-9329-971b06ada1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766 256\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "batch_size = 256\n",
    "print(train_size, batch_size)\n",
    "\n",
    "# Split off the test set\n",
    "train_val_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for the test set (held out)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50283805-104d-4380-81ca-7d09bf80b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./output/BYOL/efficientnet_b0_backbone_weights.ckpt\"\n",
    "best_params = {\"learning_rate\": 0.01, \"weight_decay\": 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c2c1184-d12a-4d1b-8b17-b12562c5a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_model():\n",
    "    # Load the EfficientNet model\n",
    "    model = efficientnet_b0()\n",
    "    \n",
    "    # Modify the final classification head for your dataset\n",
    "    embed_dim = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Identity()\n",
    "    \n",
    "    # Load the pre-trained weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    # Remove unnecessary keys (e.g., DINO projection head weights)\n",
    "    state_dict = checkpoint[\"model_state_dict\"]  # Adjust the key if needed\n",
    "    # remove `module.` prefix\n",
    "    # state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    # remove `backbone.` prefix induced by multicrop wrapper\n",
    "    # state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "    \n",
    "    # Load the weights into the model\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Pretrained weights found at {} and loaded with msg: {}'.format(checkpoint_path, msg))\n",
    "\n",
    "    # Freeze model params\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13411c3d-29f8-43d0-8008-69df2c3cff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print (np.__version__)\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def train_model(model, classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "\n",
    "        # Validation phase\n",
    "        classifier.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                outputs = classifier(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "\n",
    "        print(f\"[{device}] Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss / len(val_loader):.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return best_val_f1\n",
    "\n",
    "# Evaluate model on the test set\n",
    "def evaluate_model(model, classifier, test_loader):\n",
    "    classifier.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def grid_search(train_loader, val_loader, learning_rates, weight_decays, num_epochs):\n",
    "    best_model = None\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decays:\n",
    "            model = get_model()\n",
    "            optimizer = optim.SGD(model.classifier[1].parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
    "            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            print(f\"\\nTraining with lr={lr}, weight_decay={wd}\")\n",
    "            f1_score = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "            if f1_score > best_f1:\n",
    "                best_f1 = f1_score\n",
    "                best_model = model\n",
    "                best_params = {\"learning_rate\": lr, \"weight_decay\": wd}\n",
    "    print(f\"\\nBest Model F1: {best_f1} with params {best_params}\")\n",
    "    return best_model, best_params\n",
    "\n",
    "# Step 9: 5-Fold Cross-Validation\n",
    "def cross_validation(best_params, dataset, test_loader, num_epochs=50, folds=5):\n",
    "    fold_metrics = []\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=100)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\nStarting fold {fold + 1}/{folds}\")\n",
    "\n",
    "        # Split dataset indices for training and validation\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Create DataLoaders for this fold\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model, embed_dim = get_model()\n",
    "        linear_classifier = nn.Linear(embed_dim, 2) # 2 is the number of features\n",
    "        optimizer = optim.SGD(linear_classifier.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'], momentum=0.9)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = train_model(model, linear_classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        accuracy, precision, recall, f1 = evaluate_model(model, linear_classifier, test_loader)\n",
    "        fold_metrics.append((accuracy, precision, recall, f1, end_time - start_time))\n",
    "\n",
    "    return np.array(fold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23c9d149-1296-4a37-bdd7-5e09c1b21ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7262, Val Loss: 0.7080, F1 Score: 0.4595\n",
      "[cuda] Epoch 2/50, Train Loss: 0.7140, Val Loss: 0.7638, F1 Score: 0.5059\n",
      "[cuda] Epoch 3/50, Train Loss: 0.7276, Val Loss: 0.6876, F1 Score: 0.5644\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6957, Val Loss: 0.7400, F1 Score: 0.4032\n",
      "[cuda] Epoch 5/50, Train Loss: 0.7074, Val Loss: 0.6781, F1 Score: 0.5385\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6792, Val Loss: 0.6855, F1 Score: 0.4960\n",
      "[cuda] Epoch 7/50, Train Loss: 0.7029, Val Loss: 0.7104, F1 Score: 0.6044\n",
      "[cuda] Epoch 8/50, Train Loss: 0.7457, Val Loss: 0.6686, F1 Score: 0.5649\n",
      "[cuda] Epoch 9/50, Train Loss: 0.7456, Val Loss: 0.6432, F1 Score: 0.6556\n",
      "[cuda] Epoch 10/50, Train Loss: 0.7079, Val Loss: 0.6871, F1 Score: 0.4553\n",
      "[cuda] Epoch 11/50, Train Loss: 0.7004, Val Loss: 0.6805, F1 Score: 0.5714\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6762, Val Loss: 0.6590, F1 Score: 0.4915\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6724, Val Loss: 0.7154, F1 Score: 0.6047\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6754, Val Loss: 0.6483, F1 Score: 0.5289\n",
      "[cuda] Epoch 15/50, Train Loss: 0.6649, Val Loss: 0.7902, F1 Score: 0.5650\n",
      "[cuda] Epoch 16/50, Train Loss: 0.7137, Val Loss: 0.6782, F1 Score: 0.4793\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6452, Val Loss: 0.6488, F1 Score: 0.6573\n",
      "[cuda] Epoch 18/50, Train Loss: 0.6511, Val Loss: 0.6630, F1 Score: 0.5246\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6576, Val Loss: 0.8118, F1 Score: 0.5525\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6764, Val Loss: 0.6831, F1 Score: 0.5692\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6982, Val Loss: 0.7550, F1 Score: 0.5889\n",
      "[cuda] Epoch 22/50, Train Loss: 0.7217, Val Loss: 0.7923, F1 Score: 0.3833\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6977, Val Loss: 0.7274, F1 Score: 0.6298\n",
      "[cuda] Epoch 24/50, Train Loss: 0.7479, Val Loss: 0.6844, F1 Score: 0.4576\n",
      "[cuda] Epoch 25/50, Train Loss: 0.6641, Val Loss: 0.6717, F1 Score: 0.6782\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6870, Val Loss: 0.6555, F1 Score: 0.4844\n",
      "[cuda] Epoch 27/50, Train Loss: 0.6969, Val Loss: 0.6827, F1 Score: 0.5238\n",
      "[cuda] Epoch 28/50, Train Loss: 0.7044, Val Loss: 0.6366, F1 Score: 0.6104\n",
      "[cuda] Epoch 29/50, Train Loss: 0.7108, Val Loss: 0.6949, F1 Score: 0.4839\n",
      "[cuda] Epoch 30/50, Train Loss: 0.7393, Val Loss: 0.7894, F1 Score: 0.5890\n",
      "[cuda] Epoch 31/50, Train Loss: 0.7138, Val Loss: 0.6980, F1 Score: 0.4516\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6858, Val Loss: 0.8268, F1 Score: 0.5989\n",
      "[cuda] Epoch 33/50, Train Loss: 0.7208, Val Loss: 0.8675, F1 Score: 0.3692\n",
      "[cuda] Epoch 34/50, Train Loss: 0.7173, Val Loss: 0.7119, F1 Score: 0.6215\n",
      "[cuda] Epoch 35/50, Train Loss: 0.7910, Val Loss: 0.6987, F1 Score: 0.4754\n",
      "[cuda] Epoch 36/50, Train Loss: 0.7556, Val Loss: 0.7764, F1 Score: 0.6480\n",
      "[cuda] Epoch 37/50, Train Loss: 0.7228, Val Loss: 0.7201, F1 Score: 0.5143\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6955, Val Loss: 0.6112, F1 Score: 0.5946\n",
      "[cuda] Epoch 39/50, Train Loss: 0.6825, Val Loss: 0.6756, F1 Score: 0.4833\n",
      "[cuda] Epoch 40/50, Train Loss: 0.6532, Val Loss: 0.6855, F1 Score: 0.5846\n",
      "[cuda] Epoch 41/50, Train Loss: 0.6880, Val Loss: 0.6201, F1 Score: 0.6708\n",
      "[cuda] Epoch 42/50, Train Loss: 0.6796, Val Loss: 0.6288, F1 Score: 0.5692\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6540, Val Loss: 0.7546, F1 Score: 0.6704\n",
      "[cuda] Epoch 44/50, Train Loss: 0.7087, Val Loss: 0.6942, F1 Score: 0.5270\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6542, Val Loss: 0.6100, F1 Score: 0.6259\n",
      "[cuda] Epoch 46/50, Train Loss: 0.6427, Val Loss: 0.6812, F1 Score: 0.5839\n",
      "[cuda] Epoch 47/50, Train Loss: 0.6381, Val Loss: 0.6267, F1 Score: 0.5672\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6658, Val Loss: 0.6223, F1 Score: 0.5850\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6536, Val Loss: 0.7158, F1 Score: 0.6292\n",
      "[cuda] Epoch 50/50, Train Loss: 0.6684, Val Loss: 0.7947, F1 Score: 0.5113\n",
      "\n",
      "Starting fold 2/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7361, Val Loss: 0.7438, F1 Score: 0.4805\n",
      "[cuda] Epoch 2/50, Train Loss: 0.7427, Val Loss: 0.7329, F1 Score: 0.6188\n",
      "[cuda] Epoch 3/50, Train Loss: 0.7271, Val Loss: 0.7603, F1 Score: 0.3453\n",
      "[cuda] Epoch 4/50, Train Loss: 0.7487, Val Loss: 0.6826, F1 Score: 0.5731\n",
      "[cuda] Epoch 5/50, Train Loss: 0.7239, Val Loss: 0.6584, F1 Score: 0.5752\n",
      "[cuda] Epoch 6/50, Train Loss: 0.7276, Val Loss: 0.7114, F1 Score: 0.5682\n",
      "[cuda] Epoch 7/50, Train Loss: 0.7754, Val Loss: 0.6794, F1 Score: 0.4444\n",
      "[cuda] Epoch 8/50, Train Loss: 0.7808, Val Loss: 0.6516, F1 Score: 0.6506\n",
      "[cuda] Epoch 9/50, Train Loss: 0.7857, Val Loss: 0.6499, F1 Score: 0.6296\n",
      "[cuda] Epoch 10/50, Train Loss: 0.7354, Val Loss: 0.6877, F1 Score: 0.5324\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6793, Val Loss: 0.7105, F1 Score: 0.5897\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6634, Val Loss: 0.6771, F1 Score: 0.4000\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6481, Val Loss: 0.7119, F1 Score: 0.4228\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6804, Val Loss: 0.7512, F1 Score: 0.6136\n",
      "[cuda] Epoch 15/50, Train Loss: 0.7115, Val Loss: 0.6978, F1 Score: 0.3898\n",
      "[cuda] Epoch 16/50, Train Loss: 0.6776, Val Loss: 0.8389, F1 Score: 0.6120\n",
      "[cuda] Epoch 17/50, Train Loss: 0.7386, Val Loss: 0.7951, F1 Score: 0.4375\n",
      "[cuda] Epoch 18/50, Train Loss: 0.7341, Val Loss: 0.6193, F1 Score: 0.5942\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6478, Val Loss: 0.7178, F1 Score: 0.3471\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6689, Val Loss: 0.7651, F1 Score: 0.6413\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6989, Val Loss: 0.7152, F1 Score: 0.5113\n",
      "[cuda] Epoch 22/50, Train Loss: 0.7016, Val Loss: 0.6698, F1 Score: 0.6190\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6468, Val Loss: 0.6148, F1 Score: 0.5821\n",
      "[cuda] Epoch 24/50, Train Loss: 0.6809, Val Loss: 0.6409, F1 Score: 0.5606\n",
      "[cuda] Epoch 25/50, Train Loss: 0.6939, Val Loss: 0.6113, F1 Score: 0.5271\n",
      "[cuda] Epoch 26/50, Train Loss: 0.7185, Val Loss: 0.6899, F1 Score: 0.5385\n",
      "[cuda] Epoch 27/50, Train Loss: 0.7502, Val Loss: 0.8810, F1 Score: 0.6023\n",
      "[cuda] Epoch 28/50, Train Loss: 0.7887, Val Loss: 0.6937, F1 Score: 0.4960\n",
      "[cuda] Epoch 29/50, Train Loss: 0.7800, Val Loss: 0.7432, F1 Score: 0.6480\n",
      "[cuda] Epoch 30/50, Train Loss: 0.7041, Val Loss: 0.6728, F1 Score: 0.4688\n",
      "[cuda] Epoch 31/50, Train Loss: 0.7164, Val Loss: 0.6611, F1 Score: 0.6424\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6846, Val Loss: 0.6278, F1 Score: 0.6087\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6730, Val Loss: 0.6433, F1 Score: 0.5000\n",
      "[cuda] Epoch 34/50, Train Loss: 0.6634, Val Loss: 0.6776, F1 Score: 0.6923\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6896, Val Loss: 0.6541, F1 Score: 0.5606\n",
      "[cuda] Epoch 36/50, Train Loss: 0.7031, Val Loss: 0.6535, F1 Score: 0.6053\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6486, Val Loss: 0.6757, F1 Score: 0.5481\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6861, Val Loss: 0.6148, F1 Score: 0.5271\n",
      "[cuda] Epoch 39/50, Train Loss: 0.6705, Val Loss: 0.6330, F1 Score: 0.6369\n",
      "[cuda] Epoch 40/50, Train Loss: 0.7013, Val Loss: 0.6495, F1 Score: 0.5000\n",
      "[cuda] Epoch 41/50, Train Loss: 0.6603, Val Loss: 0.7278, F1 Score: 0.5811\n",
      "[cuda] Epoch 42/50, Train Loss: 0.6699, Val Loss: 0.6895, F1 Score: 0.5522\n",
      "[cuda] Epoch 43/50, Train Loss: 0.7020, Val Loss: 0.7603, F1 Score: 0.4741\n",
      "[cuda] Epoch 44/50, Train Loss: 0.7406, Val Loss: 0.8932, F1 Score: 0.6230\n",
      "[cuda] Epoch 45/50, Train Loss: 0.7835, Val Loss: 0.8408, F1 Score: 0.4662\n",
      "[cuda] Epoch 46/50, Train Loss: 0.7834, Val Loss: 0.8878, F1 Score: 0.6180\n",
      "[cuda] Epoch 47/50, Train Loss: 0.8124, Val Loss: 0.7319, F1 Score: 0.4640\n",
      "[cuda] Epoch 48/50, Train Loss: 0.7731, Val Loss: 0.6548, F1 Score: 0.6377\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6880, Val Loss: 0.6932, F1 Score: 0.6629\n",
      "[cuda] Epoch 50/50, Train Loss: 0.7369, Val Loss: 0.8108, F1 Score: 0.4000\n",
      "\n",
      "Starting fold 3/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7553, Val Loss: 0.7256, F1 Score: 0.4354\n",
      "[cuda] Epoch 2/50, Train Loss: 0.7174, Val Loss: 0.7858, F1 Score: 0.4355\n",
      "[cuda] Epoch 3/50, Train Loss: 0.7305, Val Loss: 0.6983, F1 Score: 0.5091\n",
      "[cuda] Epoch 4/50, Train Loss: 0.7316, Val Loss: 0.7194, F1 Score: 0.4320\n",
      "[cuda] Epoch 5/50, Train Loss: 0.7450, Val Loss: 0.7020, F1 Score: 0.4604\n",
      "[cuda] Epoch 6/50, Train Loss: 0.7018, Val Loss: 0.6799, F1 Score: 0.5799\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6803, Val Loss: 0.7245, F1 Score: 0.4248\n",
      "[cuda] Epoch 8/50, Train Loss: 0.7582, Val Loss: 0.7138, F1 Score: 0.5904\n",
      "[cuda] Epoch 9/50, Train Loss: 0.7491, Val Loss: 0.7692, F1 Score: 0.4696\n",
      "[cuda] Epoch 10/50, Train Loss: 0.7143, Val Loss: 0.7523, F1 Score: 0.5926\n",
      "[cuda] Epoch 11/50, Train Loss: 0.7280, Val Loss: 0.7231, F1 Score: 0.4779\n",
      "[cuda] Epoch 12/50, Train Loss: 0.7486, Val Loss: 0.7390, F1 Score: 0.6199\n",
      "[cuda] Epoch 13/50, Train Loss: 0.7218, Val Loss: 0.8848, F1 Score: 0.3860\n",
      "[cuda] Epoch 14/50, Train Loss: 0.7586, Val Loss: 0.7748, F1 Score: 0.5732\n",
      "[cuda] Epoch 15/50, Train Loss: 0.7478, Val Loss: 0.8584, F1 Score: 0.4237\n",
      "[cuda] Epoch 16/50, Train Loss: 0.7579, Val Loss: 0.6385, F1 Score: 0.6145\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6664, Val Loss: 0.6856, F1 Score: 0.4545\n",
      "[cuda] Epoch 18/50, Train Loss: 0.7089, Val Loss: 0.6946, F1 Score: 0.5939\n",
      "[cuda] Epoch 19/50, Train Loss: 0.6793, Val Loss: 0.7396, F1 Score: 0.5000\n",
      "[cuda] Epoch 20/50, Train Loss: 0.6726, Val Loss: 0.7353, F1 Score: 0.4892\n",
      "[cuda] Epoch 21/50, Train Loss: 0.6691, Val Loss: 0.6825, F1 Score: 0.5912\n",
      "[cuda] Epoch 22/50, Train Loss: 0.6988, Val Loss: 0.7944, F1 Score: 0.4483\n",
      "[cuda] Epoch 23/50, Train Loss: 0.6901, Val Loss: 0.6615, F1 Score: 0.5926\n",
      "[cuda] Epoch 24/50, Train Loss: 0.7394, Val Loss: 0.9351, F1 Score: 0.3729\n",
      "[cuda] Epoch 25/50, Train Loss: 0.7555, Val Loss: 0.7317, F1 Score: 0.5765\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6761, Val Loss: 0.8587, F1 Score: 0.4127\n",
      "[cuda] Epoch 27/50, Train Loss: 0.7010, Val Loss: 0.8043, F1 Score: 0.5562\n",
      "[cuda] Epoch 28/50, Train Loss: 0.8008, Val Loss: 1.0768, F1 Score: 0.3485\n",
      "[cuda] Epoch 29/50, Train Loss: 0.7880, Val Loss: 0.7343, F1 Score: 0.5862\n",
      "[cuda] Epoch 30/50, Train Loss: 0.6715, Val Loss: 0.8531, F1 Score: 0.4746\n",
      "[cuda] Epoch 31/50, Train Loss: 0.7590, Val Loss: 0.7194, F1 Score: 0.6024\n",
      "[cuda] Epoch 32/50, Train Loss: 0.7108, Val Loss: 0.7442, F1 Score: 0.5211\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6257, Val Loss: 0.7346, F1 Score: 0.5963\n",
      "[cuda] Epoch 34/50, Train Loss: 0.6860, Val Loss: 0.6685, F1 Score: 0.6036\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6835, Val Loss: 0.7814, F1 Score: 0.4590\n",
      "[cuda] Epoch 36/50, Train Loss: 0.6913, Val Loss: 0.6735, F1 Score: 0.6265\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6769, Val Loss: 0.7810, F1 Score: 0.4444\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6994, Val Loss: 0.6497, F1 Score: 0.6272\n",
      "[cuda] Epoch 39/50, Train Loss: 0.6849, Val Loss: 0.6738, F1 Score: 0.5931\n",
      "[cuda] Epoch 40/50, Train Loss: 0.7211, Val Loss: 0.7157, F1 Score: 0.5568\n",
      "[cuda] Epoch 41/50, Train Loss: 0.6331, Val Loss: 0.8056, F1 Score: 0.5000\n",
      "[cuda] Epoch 42/50, Train Loss: 0.7006, Val Loss: 0.6686, F1 Score: 0.6102\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6905, Val Loss: 0.9418, F1 Score: 0.4237\n",
      "[cuda] Epoch 44/50, Train Loss: 0.7598, Val Loss: 0.8160, F1 Score: 0.5497\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6938, Val Loss: 0.7282, F1 Score: 0.5844\n",
      "[cuda] Epoch 46/50, Train Loss: 0.6299, Val Loss: 0.7449, F1 Score: 0.5595\n",
      "[cuda] Epoch 47/50, Train Loss: 0.7153, Val Loss: 0.8371, F1 Score: 0.4672\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6811, Val Loss: 0.7010, F1 Score: 0.5665\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6284, Val Loss: 0.8913, F1 Score: 0.4463\n",
      "[cuda] Epoch 50/50, Train Loss: 0.7145, Val Loss: 0.6583, F1 Score: 0.5769\n",
      "\n",
      "Starting fold 4/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7227, Val Loss: 0.6978, F1 Score: 0.5190\n",
      "[cuda] Epoch 2/50, Train Loss: 0.7172, Val Loss: 0.7228, F1 Score: 0.5876\n",
      "[cuda] Epoch 3/50, Train Loss: 0.7099, Val Loss: 0.7012, F1 Score: 0.5660\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6872, Val Loss: 0.7034, F1 Score: 0.4724\n",
      "[cuda] Epoch 5/50, Train Loss: 0.7006, Val Loss: 0.6812, F1 Score: 0.5444\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6715, Val Loss: 0.6601, F1 Score: 0.5588\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6749, Val Loss: 0.6523, F1 Score: 0.6199\n",
      "[cuda] Epoch 8/50, Train Loss: 0.6533, Val Loss: 0.7425, F1 Score: 0.3636\n",
      "[cuda] Epoch 9/50, Train Loss: 0.6913, Val Loss: 0.6238, F1 Score: 0.4800\n",
      "[cuda] Epoch 10/50, Train Loss: 0.6572, Val Loss: 0.6405, F1 Score: 0.6265\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6533, Val Loss: 0.6860, F1 Score: 0.5965\n",
      "[cuda] Epoch 12/50, Train Loss: 0.6553, Val Loss: 0.6168, F1 Score: 0.6241\n",
      "[cuda] Epoch 13/50, Train Loss: 0.6691, Val Loss: 0.6869, F1 Score: 0.6592\n",
      "[cuda] Epoch 14/50, Train Loss: 0.6450, Val Loss: 0.8177, F1 Score: 0.3680\n",
      "[cuda] Epoch 15/50, Train Loss: 0.6964, Val Loss: 0.7175, F1 Score: 0.6444\n",
      "[cuda] Epoch 16/50, Train Loss: 0.7282, Val Loss: 0.7421, F1 Score: 0.4800\n",
      "[cuda] Epoch 17/50, Train Loss: 0.7453, Val Loss: 0.7790, F1 Score: 0.6778\n",
      "[cuda] Epoch 18/50, Train Loss: 0.8287, Val Loss: 0.9239, F1 Score: 0.4094\n",
      "[cuda] Epoch 19/50, Train Loss: 0.8440, Val Loss: 0.9334, F1 Score: 0.5424\n",
      "[cuda] Epoch 20/50, Train Loss: 0.8459, Val Loss: 0.7056, F1 Score: 0.4386\n",
      "[cuda] Epoch 21/50, Train Loss: 0.9454, Val Loss: 0.6947, F1 Score: 0.5147\n",
      "[cuda] Epoch 22/50, Train Loss: 0.8219, Val Loss: 0.7499, F1 Score: 0.6592\n",
      "[cuda] Epoch 23/50, Train Loss: 0.7896, Val Loss: 0.8933, F1 Score: 0.4160\n",
      "[cuda] Epoch 24/50, Train Loss: 0.6850, Val Loss: 0.7462, F1 Score: 0.6082\n",
      "[cuda] Epoch 25/50, Train Loss: 0.7187, Val Loss: 0.7285, F1 Score: 0.5167\n",
      "[cuda] Epoch 26/50, Train Loss: 0.7258, Val Loss: 0.7435, F1 Score: 0.6480\n",
      "[cuda] Epoch 27/50, Train Loss: 0.7445, Val Loss: 0.5896, F1 Score: 0.5263\n",
      "[cuda] Epoch 28/50, Train Loss: 0.8501, Val Loss: 0.7013, F1 Score: 0.6506\n",
      "[cuda] Epoch 29/50, Train Loss: 0.8778, Val Loss: 0.6727, F1 Score: 0.5818\n",
      "[cuda] Epoch 30/50, Train Loss: 0.8971, Val Loss: 0.7405, F1 Score: 0.5280\n",
      "[cuda] Epoch 31/50, Train Loss: 0.8096, Val Loss: 0.6938, F1 Score: 0.6444\n",
      "[cuda] Epoch 32/50, Train Loss: 0.7138, Val Loss: 0.7239, F1 Score: 0.5323\n",
      "[cuda] Epoch 33/50, Train Loss: 0.7059, Val Loss: 0.7147, F1 Score: 0.6145\n",
      "[cuda] Epoch 34/50, Train Loss: 0.7421, Val Loss: 0.6493, F1 Score: 0.5857\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6275, Val Loss: 0.6319, F1 Score: 0.6452\n",
      "[cuda] Epoch 36/50, Train Loss: 0.6645, Val Loss: 0.7110, F1 Score: 0.4640\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6868, Val Loss: 0.8366, F1 Score: 0.6215\n",
      "[cuda] Epoch 38/50, Train Loss: 0.7554, Val Loss: 0.7704, F1 Score: 0.4882\n",
      "[cuda] Epoch 39/50, Train Loss: 0.7031, Val Loss: 0.7814, F1 Score: 0.6207\n",
      "[cuda] Epoch 40/50, Train Loss: 0.7265, Val Loss: 0.6999, F1 Score: 0.4800\n",
      "[cuda] Epoch 41/50, Train Loss: 0.7049, Val Loss: 0.5925, F1 Score: 0.6216\n",
      "[cuda] Epoch 42/50, Train Loss: 0.7106, Val Loss: 0.5962, F1 Score: 0.6418\n",
      "[cuda] Epoch 43/50, Train Loss: 0.7728, Val Loss: 0.7084, F1 Score: 0.5503\n",
      "[cuda] Epoch 44/50, Train Loss: 0.7336, Val Loss: 0.6859, F1 Score: 0.5828\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6862, Val Loss: 0.6850, F1 Score: 0.5926\n",
      "[cuda] Epoch 46/50, Train Loss: 0.7015, Val Loss: 0.8210, F1 Score: 0.4286\n",
      "[cuda] Epoch 47/50, Train Loss: 0.7732, Val Loss: 0.7558, F1 Score: 0.6067\n",
      "[cuda] Epoch 48/50, Train Loss: 0.8190, Val Loss: 0.7143, F1 Score: 0.4961\n",
      "[cuda] Epoch 49/50, Train Loss: 0.8651, Val Loss: 0.7107, F1 Score: 0.5899\n",
      "[cuda] Epoch 50/50, Train Loss: 0.7621, Val Loss: 0.6801, F1 Score: 0.6480\n",
      "\n",
      "Starting fold 5/5\n",
      "Pretrained weights found at ./output/BYOL/efficientnet_b0_backbone_weights.ckpt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['conv1.weight'])\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6976, Val Loss: 0.6941, F1 Score: 0.6316\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6979, Val Loss: 0.7196, F1 Score: 0.5537\n",
      "[cuda] Epoch 3/50, Train Loss: 0.7018, Val Loss: 0.7039, F1 Score: 0.6044\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6975, Val Loss: 0.7049, F1 Score: 0.4713\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6701, Val Loss: 0.7764, F1 Score: 0.5618\n",
      "[cuda] Epoch 6/50, Train Loss: 0.7129, Val Loss: 0.7085, F1 Score: 0.4409\n",
      "[cuda] Epoch 7/50, Train Loss: 0.6953, Val Loss: 0.8536, F1 Score: 0.5843\n",
      "[cuda] Epoch 8/50, Train Loss: 0.7146, Val Loss: 0.7317, F1 Score: 0.4320\n",
      "[cuda] Epoch 9/50, Train Loss: 0.7107, Val Loss: 0.7655, F1 Score: 0.5810\n",
      "[cuda] Epoch 10/50, Train Loss: 0.6819, Val Loss: 0.7870, F1 Score: 0.3969\n",
      "[cuda] Epoch 11/50, Train Loss: 0.6938, Val Loss: 0.8795, F1 Score: 0.6064\n",
      "[cuda] Epoch 12/50, Train Loss: 0.7799, Val Loss: 0.7399, F1 Score: 0.3802\n",
      "[cuda] Epoch 13/50, Train Loss: 0.7095, Val Loss: 0.6940, F1 Score: 0.6480\n",
      "[cuda] Epoch 14/50, Train Loss: 0.7170, Val Loss: 0.7246, F1 Score: 0.4333\n",
      "[cuda] Epoch 15/50, Train Loss: 0.7680, Val Loss: 0.6945, F1 Score: 0.4651\n",
      "[cuda] Epoch 16/50, Train Loss: 0.6286, Val Loss: 0.6998, F1 Score: 0.4355\n",
      "[cuda] Epoch 17/50, Train Loss: 0.6921, Val Loss: 0.6926, F1 Score: 0.5957\n",
      "[cuda] Epoch 18/50, Train Loss: 0.6770, Val Loss: 0.7330, F1 Score: 0.6323\n",
      "[cuda] Epoch 19/50, Train Loss: 0.7441, Val Loss: 0.8321, F1 Score: 0.5147\n",
      "[cuda] Epoch 20/50, Train Loss: 0.7842, Val Loss: 0.9602, F1 Score: 0.5856\n",
      "[cuda] Epoch 21/50, Train Loss: 0.7595, Val Loss: 0.7536, F1 Score: 0.5224\n",
      "[cuda] Epoch 22/50, Train Loss: 0.8317, Val Loss: 0.9470, F1 Score: 0.5746\n",
      "[cuda] Epoch 23/50, Train Loss: 0.7756, Val Loss: 0.7691, F1 Score: 0.4286\n",
      "[cuda] Epoch 24/50, Train Loss: 0.7538, Val Loss: 0.6935, F1 Score: 0.6203\n",
      "[cuda] Epoch 25/50, Train Loss: 0.7303, Val Loss: 0.7267, F1 Score: 0.6000\n",
      "[cuda] Epoch 26/50, Train Loss: 0.6988, Val Loss: 0.7048, F1 Score: 0.4628\n",
      "[cuda] Epoch 27/50, Train Loss: 0.6803, Val Loss: 0.7653, F1 Score: 0.6484\n",
      "[cuda] Epoch 28/50, Train Loss: 0.6984, Val Loss: 0.7564, F1 Score: 0.4615\n",
      "[cuda] Epoch 29/50, Train Loss: 0.6971, Val Loss: 0.9644, F1 Score: 0.6344\n",
      "[cuda] Epoch 30/50, Train Loss: 0.7760, Val Loss: 0.7392, F1 Score: 0.4516\n",
      "[cuda] Epoch 31/50, Train Loss: 0.6489, Val Loss: 0.6679, F1 Score: 0.6289\n",
      "[cuda] Epoch 32/50, Train Loss: 0.6176, Val Loss: 0.7617, F1 Score: 0.6034\n",
      "[cuda] Epoch 33/50, Train Loss: 0.6522, Val Loss: 0.7543, F1 Score: 0.4762\n",
      "[cuda] Epoch 34/50, Train Loss: 0.6677, Val Loss: 0.6972, F1 Score: 0.6813\n",
      "[cuda] Epoch 35/50, Train Loss: 0.6767, Val Loss: 0.6542, F1 Score: 0.6460\n",
      "[cuda] Epoch 36/50, Train Loss: 0.6701, Val Loss: 0.6573, F1 Score: 0.6098\n",
      "[cuda] Epoch 37/50, Train Loss: 0.6335, Val Loss: 0.6243, F1 Score: 0.4806\n",
      "[cuda] Epoch 38/50, Train Loss: 0.6752, Val Loss: 0.6819, F1 Score: 0.4390\n",
      "[cuda] Epoch 39/50, Train Loss: 0.6557, Val Loss: 0.6912, F1 Score: 0.6818\n",
      "[cuda] Epoch 40/50, Train Loss: 0.6103, Val Loss: 0.7556, F1 Score: 0.4615\n",
      "[cuda] Epoch 41/50, Train Loss: 0.6663, Val Loss: 0.7399, F1 Score: 0.6026\n",
      "[cuda] Epoch 42/50, Train Loss: 0.6724, Val Loss: 0.7217, F1 Score: 0.4964\n",
      "[cuda] Epoch 43/50, Train Loss: 0.6593, Val Loss: 0.7265, F1 Score: 0.4160\n",
      "[cuda] Epoch 44/50, Train Loss: 0.6454, Val Loss: 0.6553, F1 Score: 0.6456\n",
      "[cuda] Epoch 45/50, Train Loss: 0.6610, Val Loss: 0.7488, F1 Score: 0.6118\n",
      "[cuda] Epoch 46/50, Train Loss: 0.6674, Val Loss: 0.6831, F1 Score: 0.4878\n",
      "[cuda] Epoch 47/50, Train Loss: 0.6669, Val Loss: 0.7678, F1 Score: 0.6420\n",
      "[cuda] Epoch 48/50, Train Loss: 0.6731, Val Loss: 0.6699, F1 Score: 0.5152\n",
      "[cuda] Epoch 49/50, Train Loss: 0.6731, Val Loss: 0.7691, F1 Score: 0.5401\n",
      "[cuda] Epoch 50/50, Train Loss: 0.6837, Val Loss: 0.6686, F1 Score: 0.5414\n",
      "\n",
      "Average metrics over 5 folds in test set:\n",
      "Accuracy: 0.6395 ± 0.0628\n",
      "Precision: 0.7175 ± 0.1006\n",
      "Recall: 0.5696 ± 0.1496\n",
      "F1 Score: 0.6193 ± 0.0973\n",
      "Training Time per Fold: 1886.80 ± 13.96 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run the 5-fold cross-validation\n",
    "metrics = cross_validation(best_params, train_val_dataset, test_loader, num_epochs=50, folds=5)\n",
    "\n",
    "# Calculate average and standard deviation of metrics across folds\n",
    "avg_metrics = metrics.mean(axis=0)\n",
    "std_metrics = metrics.std(axis=0)\n",
    "\n",
    "print(f\"\\nAverage metrics over 5 folds in test set:\\n\"\n",
    "      f\"Accuracy: {avg_metrics[0]:.4f} ± {std_metrics[0]:.4f}\\n\"\n",
    "      f\"Precision: {avg_metrics[1]:.4f} ± {std_metrics[1]:.4f}\\n\"\n",
    "      f\"Recall: {avg_metrics[2]:.4f} ± {std_metrics[2]:.4f}\\n\"\n",
    "      f\"F1 Score: {avg_metrics[3]:.4f} ± {std_metrics[3]:.4f}\\n\"\n",
    "      f\"Training Time per Fold: {avg_metrics[4]:.2f} ± {std_metrics[4]:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8100e171-722d-4875-bd6b-eeaf72178470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.09302326e-01, 8.18181818e-01, 5.86956522e-01, 6.83544304e-01,\n",
       "        1.86407737e+03],\n",
       "       [5.34883721e-01, 6.25000000e-01, 3.26086957e-01, 4.28571429e-01,\n",
       "        1.89779209e+03],\n",
       "       [6.62790698e-01, 6.88888889e-01, 6.73913043e-01, 6.81318681e-01,\n",
       "        1.87697351e+03],\n",
       "       [6.04651163e-01, 6.03448276e-01, 7.60869565e-01, 6.73076923e-01,\n",
       "        1.89954106e+03],\n",
       "       [6.86046512e-01, 8.51851852e-01, 5.00000000e-01, 6.30136986e-01,\n",
       "        1.89561574e+03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06b70805-0d9f-41ed-a33d-9d23c147c4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>training time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>1864.077372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1897.792086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>1876.973509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>1899.541061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>1895.615739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall        f1  training time\n",
       "0  0.709302   0.818182  0.586957  0.683544    1864.077372\n",
       "1  0.534884   0.625000  0.326087  0.428571    1897.792086\n",
       "2  0.662791   0.688889  0.673913  0.681319    1876.973509\n",
       "3  0.604651   0.603448  0.760870  0.673077    1899.541061\n",
       "4  0.686047   0.851852  0.500000  0.630137    1895.615739"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(metrics, columns=['accuracy', 'precision', 'recall', 'f1', 'training time'])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c59cbb0-4d5f-40de-94e6-f66d6e3d0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/byol_fine_tuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c580b57-893c-4ff1-8b6f-d11c9393c61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
