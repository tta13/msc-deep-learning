{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a115a2-48e0-4eb7-b55b-f5bd08c804d8",
   "metadata": {},
   "source": [
    "# BYOL pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ec9d08-6b6f-41cb-9248-92b921d7b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def reproducibility(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "\n",
    "def define_param_groups(model, weight_decay, optimizer_name):\n",
    "    def exclude_from_wd_and_adaptation(name):\n",
    "        if 'bn' in name:\n",
    "            return True\n",
    "        if optimizer_name == 'lars' and 'bias' in name:\n",
    "            return True\n",
    "\n",
    "    param_groups = [\n",
    "        {\n",
    "            'params': [p for name, p in model.named_parameters() if not exclude_from_wd_and_adaptation(name)],\n",
    "            'weight_decay': weight_decay,\n",
    "            'layer_adaptation': True,\n",
    "        },\n",
    "        {\n",
    "            'params': [p for name, p in model.named_parameters() if exclude_from_wd_and_adaptation(name)],\n",
    "            'weight_decay': 0.,\n",
    "            'layer_adaptation': False,\n",
    "        },\n",
    "    ]\n",
    "    return param_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568aa243-1b87-4f7c-904a-d41c7af0f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "\n",
    "class Augment:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module\n",
    "    Transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, s=1):\n",
    "        color_jitter = T.ColorJitter(\n",
    "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
    "        )\n",
    "        blur = T.GaussianBlur((3, 3), (0.1, 2.0))\n",
    "\n",
    "        self.train_transform = T.transforms.Compose([\n",
    "            T.RandomResizedCrop(img_size, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "            T.RandomApply(\n",
    "                    [T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                    p=0.8\n",
    "                ),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "            T.Resize((256, 256)),\n",
    "            T.CenterCrop(img_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x), \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff10a1a-766e-4b90-a303-7155cc4c5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.multiprocessing import cpu_count\n",
    "\n",
    "def get_data_loader(dataset_path, batch_size, transform=None, shuffle=False):\n",
    "    dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=cpu_count()//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dbf4ce9-8746-4b98-a2c3-5ae4b52dc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embedding_size=256, hidden_size=2048, batch_norm_mlp=False):\n",
    "        super().__init__()\n",
    "        norm = nn.BatchNorm1d(hidden_size) if batch_norm_mlp else nn.Identity()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            norm,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AddProjHead(nn.Module):\n",
    "    def __init__(self, model, in_features, hidden_size=4096,\n",
    "                 embedding_size=256, batch_norm_mlp=True):\n",
    "        super(AddProjHead, self).__init__()\n",
    "        self.backbone = model\n",
    "        # remove last layer\n",
    "        self.backbone.classifier[1] = nn.Identity()\n",
    "        self.backbone.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.backbone.maxpool = torch.nn.Identity()\n",
    "        # add mlp projection head\n",
    "        self.projection = MLP(in_features, embedding_size, hidden_size=hidden_size, batch_norm_mlp=batch_norm_mlp)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        embedding = self.backbone(x)\n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        return self.projection(embedding)\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    # L2 normalization\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.alpha + (1 - self.alpha) * new\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            net,\n",
    "            batch_norm_mlp=True,\n",
    "            in_features=512,\n",
    "            projection_size=256,\n",
    "            projection_hidden_size=2048,\n",
    "            moving_average_decay=0.99,\n",
    "            use_momentum=True,\n",
    "            device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            net: model to be trained\n",
    "            batch_norm_mlp: whether to use batchnorm1d in the mlp predictor and projector\n",
    "            in_features: the number features that are produced by the backbone net i.e. resnet\n",
    "            projection_size: the size of the output vector of the two identical MLPs\n",
    "            projection_hidden_size: the size of the hidden vector of the two identical MLPs\n",
    "            augment_fn2: apply different augmentation the second view\n",
    "            moving_average_decay: t hyperparameter to control the influence in the target network weight update\n",
    "            use_momentum: whether to update the target network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.student_model = AddProjHead(model=net, in_features=in_features,\n",
    "                                         embedding_size=projection_size,\n",
    "                                         hidden_size=projection_hidden_size,\n",
    "                                         batch_norm_mlp=batch_norm_mlp)\n",
    "        self.use_momentum = use_momentum\n",
    "        self.teacher_model = self._get_teacher()\n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "        self.student_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
    "        self.device = device\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_teacher(self):\n",
    "        return copy.deepcopy(self.student_model)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_moving_average(self):\n",
    "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum ' \\\n",
    "                                  'for the target encoder '\n",
    "        assert self.teacher_model is not None, 'target encoder has not been created yet'\n",
    "\n",
    "        for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):\n",
    "          old_weight, up_weight = teacher_params.data, student_params.data\n",
    "          teacher_params.data = self.target_ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            image_one, image_two=None,\n",
    "            return_embedding=False):\n",
    "        if return_embedding or (image_two is None):\n",
    "            return self.student_model(image_one, return_embedding=True)\n",
    "\n",
    "        image_one, image_two = image_one.to(self.device), image_two.to(self.device)\n",
    "\n",
    "        # student projections: backbone + MLP projection\n",
    "        student_proj_one = self.student_model(image_one)\n",
    "        student_proj_two = self.student_model(image_two)\n",
    "\n",
    "        # additional student's MLP head called predictor\n",
    "        student_pred_one = self.student_predictor(student_proj_one)\n",
    "        student_pred_two = self.student_predictor(student_proj_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # teacher processes the images and makes projections: backbone + MLP\n",
    "            teacher_proj_one = self.teacher_model(image_one).detach_()\n",
    "            teacher_proj_two = self.teacher_model(image_two).detach_()\n",
    "            \n",
    "        loss_one = loss_fn(student_pred_one, teacher_proj_one)\n",
    "        loss_two = loss_fn(student_pred_two, teacher_proj_two)\n",
    "\n",
    "        # Free tensors\n",
    "        del image_one, image_two\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        return (loss_one + loss_two).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212a0978-95bb-438c-8489-c7798b7fa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/noahgolmant/pytorch-lars\n",
    "\"\"\" Layer-wise adaptive rate scaling for SGD in PyTorch! \"\"\"\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    r\"\"\"Implements layer-wise adaptive rate scaling for SGD.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): base learning rate (\\gamma_0)\n",
    "        momentum (float, optional): momentum factor (default: 0) (\"m\")\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "            (\"\\beta\")\n",
    "        eta (float, optional): LARS coefficient\n",
    "        max_epoch: maximum training epoch to determine polynomial LR decay.\n",
    "\n",
    "    Based on Algorithm 1 of the following paper by You, Gitman, and Ginsburg.\n",
    "    Large Batch Training of Convolutional Networks:\n",
    "        https://arxiv.org/abs/1708.03888\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = LARS(model.parameters(), lr=0.1, eta=1e-3)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, momentum=.9,\n",
    "                 weight_decay=.0005, eta=0.001, max_epoch=200):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\"\n",
    "                             .format(weight_decay))\n",
    "        if eta < 0.0:\n",
    "            raise ValueError(\"Invalid LARS coefficient value: {}\".format(eta))\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(lr=lr, momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        eta=eta, max_epoch=max_epoch)\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "            epoch: current epoch to calculate polynomial LR decay schedule.\n",
    "                   if None, uses self.epoch and increments it.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            eta = group['eta']\n",
    "            lr = group['lr']\n",
    "            max_epoch = group['max_epoch']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                weight_norm = torch.norm(p.data)\n",
    "                grad_norm = torch.norm(d_p)\n",
    "\n",
    "                # Global LR computed on polynomial decay schedule\n",
    "                decay = (1 - float(epoch) / max_epoch) ** 2\n",
    "                global_lr = lr * decay\n",
    "\n",
    "                # Compute local learning rate for this layer\n",
    "                local_lr = eta * weight_norm / \\\n",
    "                    (grad_norm + weight_decay * weight_norm)\n",
    "\n",
    "                # Update the momentum term\n",
    "                actual_lr = local_lr * global_lr\n",
    "\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = \\\n",
    "                            torch.zeros_like(p.data)\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
    "                p.data.add_(-buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b852d-1e2d-4872-bc85-caae99eda023",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4578ce04-012e-4549-8015-c6c262ac0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, data):\n",
    "    (view1, view2), _ = data\n",
    "    loss = model(view1, view2)\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    batch_loader = tqdm(train_dataloader)\n",
    "    for data in batch_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = training_step(model, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # EMA update\n",
    "        model.update_moving_average()\n",
    "        total_loss += loss.item()        \n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb8f5e-b8b5-4ccd-964e-093cac3a38f1",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b977c879-bd56-40d9-bfad-56816a74f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536dce19-d503-4790-bc50-c88ad828a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set train params\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "base_lr = 0.00025\n",
    "weight_decay = 1e-6\n",
    "load = False\n",
    "img_size = 224\n",
    "random_state = 9999\n",
    "epochs = 100\n",
    "dataset_path = './datasets/chestx-ray14-v3'\n",
    "save_path = './output/BYOL'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ade349-9c77-4c4c-8f7f-4e231dd0a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_gpus: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████▎                                                              | 78/378 [00:48<03:06,  1.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     32\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 33\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(train_loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mend_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_dataloader, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m      9\u001b[0m batch_loader \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m batch_loader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m training_step(model, data)\n",
      "File \u001b[0;32m~/msc-deep-learning/deep_learning/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/msc-deep-learning/deep_learning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/msc-deep-learning/deep_learning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/msc-deep-learning/deep_learning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1173\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1173\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1175\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/msc-deep-learning/deep_learning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = models.efficientnet_b0(pretrained=True)\n",
    "model = BYOL(model, in_features=model.classifier[1].in_features, batch_norm_mlp=True, device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = LARS(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "\n",
    "# data\n",
    "transform = Augment(img_size)\n",
    "\n",
    "test_transform = T.Compose([\n",
    "            T.CenterCrop(img_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])\n",
    "\n",
    "\n",
    "loader_train = get_data_loader(f'{dataset_path}/train', batch_size, transform=transform)\n",
    "\n",
    "# general info\n",
    "available_gpus = len([torch.cuda.device(i) for i in range(torch.cuda.device_count())])\n",
    "print('available_gpus:', available_gpus)\n",
    "\n",
    "reproducibility(random_state)\n",
    "\n",
    "if load:\n",
    "  model.load_state_dict(torch.load(\"....ckpt\"))\n",
    "\n",
    "\n",
    "mean_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_one_epoch(model, loader_train, optimizer)\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch: [{epoch+1}/{epochs}]  loss: {np.mean(train_loss)}, time: {start_time - end_time}')\n",
    "    mean_losses.append(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f39c3-33ee-497a-aa58-5b31805c1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "indices = range(0,100,1)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(indices, mean_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107fb98-ca70-46ae-8252-b22bfcfda773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "model_backbone_weights = model.backbone\n",
    "print(model_backbone_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e562c1-858d-439c-b062-a9dc6668b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " ! mkdir ./output/BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d0c53-0882-4589-bf3d-a791c07e925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({ 'model_state_dict': model_backbone_weights.state_dict() }, f'{save_path}/efficientnet_b0_backbone_weights_v1.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
