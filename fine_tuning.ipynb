{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOuCnbt9pEeyrqADjil5YCy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Projeto deep learning"],"metadata":{"id":"cVC2aAJNeuSv"}},{"cell_type":"markdown","source":["# Fine tuning da EfficientNet usando o dataset COVIDGR"],"metadata":{"id":"Uh3mHSzCe0_K"}},{"cell_type":"markdown","source":["## Step 1: Setup Google Drive and Libraries"],"metadata":{"id":"be9JF08kfCFA"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMJKPj8hez9w","executionInfo":{"status":"ok","timestamp":1731165367982,"user_tz":180,"elapsed":34521,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}},"outputId":"535e3773-e787-48c7-d10d-9df9d112b76d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Step 2: Data Loading and Augmentation\n","Define Dataset Paths: Set up the paths to the P and N folders for loading images.\n","\n","Data Augmentation: Implement the augmentation protocol, including random resized cropping, color jittering, color dropping, and Gaussian blurring.\n","\n","Load Dataset: Use torchvision.datasets.ImageFolder with the custom transformations."],"metadata":{"id":"yNNwAbZifxRO"}},{"cell_type":"code","source":["import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset, random_split\n","import torch\n","\n","# Path to the dataset\n","dataset_path = '/content/drive/MyDrive/Pós-Grad/2024.2/Deep Learning/Projeto/Datasets/COVIDGR_1.0'\n","positive_path = os.path.join(dataset_path, 'P')\n","negative_path = os.path.join(dataset_path, 'N')\n","\n","# Data transformations\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n","    transforms.RandomApply([\n","        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n","        transforms.RandomGrayscale(p=0.2)\n","    ]),\n","    transforms.RandomApply([transforms.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n","    transforms.Resize((256, 256)),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor()\n","])\n","\n","# Load dataset with ImageFolder\n","dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)"],"metadata":{"id":"Fe3UtqVbfN4x","executionInfo":{"status":"ok","timestamp":1731165557140,"user_tz":180,"elapsed":2407,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liyCPRwbgSeh","executionInfo":{"status":"ok","timestamp":1731165565094,"user_tz":180,"elapsed":496,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}},"outputId":"170df247-2bc6-47ec-daa9-b6964d09ecc5"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 852\n","    Root location: /content/drive/MyDrive/Pós-Grad/2024.2/Deep Learning/Projeto/Datasets/COVIDGR_1.0\n","    StandardTransform\n","Transform: Compose(\n","               RandomResizedCrop(size=(224, 224), scale=(0.3, 0.9), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n","               RandomApply(\n","               p=0.5\n","               ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=(-0.1, 0.1))\n","               RandomGrayscale(p=0.2)\n","           )\n","               RandomApply(\n","               p=0.5\n","               GaussianBlur(kernel_size=(25, 25), sigma=(0.1, 2.0))\n","           )\n","               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n","               CenterCrop(size=(224, 224))\n","               ToTensor()\n","           )"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Step 4: Map labels: 'P' to 1, 'N' to 0\n","dataset.class_to_idx = {'N': 0, 'P': 1}\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXPHNbkWkxtP","executionInfo":{"status":"ok","timestamp":1731166703323,"user_tz":180,"elapsed":665,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}},"outputId":"a9e7985a-56e5-4e95-f554-8fdbc58ab8a4"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 852\n","    Root location: /content/drive/MyDrive/Pós-Grad/2024.2/Deep Learning/Projeto/Datasets/COVIDGR_1.0\n","    StandardTransform\n","Transform: Compose(\n","               RandomResizedCrop(size=(224, 224), scale=(0.3, 0.9), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n","               RandomApply(\n","               p=0.5\n","               ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=(-0.1, 0.1))\n","               RandomGrayscale(p=0.2)\n","           )\n","               RandomApply(\n","               p=0.5\n","               GaussianBlur(kernel_size=(25, 25), sigma=(0.1, 2.0))\n","           )\n","               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n","               CenterCrop(size=(224, 224))\n","               ToTensor()\n","           )"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Step 3: Data Splitting\n","90-10 Train-Test Split and Train-Validation Split."],"metadata":{"id":"RnnlRKRDglD1"}},{"cell_type":"code","source":["# Splitting the dataset\n","train_size = int(0.9 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","# Further split training dataset for validation\n","val_size = int(0.1 * len(train_dataset))\n","train_size = len(train_dataset) - val_size\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","# Dataloaders\n","batch_size = train_size // 4\n","print(train_size, batch_size)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzHskMAKgixH","executionInfo":{"status":"ok","timestamp":1731166714901,"user_tz":180,"elapsed":523,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}},"outputId":"99199cb8-2eb0-4a74-b1fa-0b3d6c47a427"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["690 172\n"]}]},{"cell_type":"code","source":["len(dataset) * 0.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lZR_fx7zg4ev","executionInfo":{"status":"ok","timestamp":1731165674882,"user_tz":180,"elapsed":452,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}},"outputId":"e1d526eb-e0c9-481c-e0c8-5c0fa8b4b4ba"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["766.8000000000001"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Step 4: Model Setup\n","Load Pretrained EfficientNet: Load EfficientNet with ImageNet pre-trained weights and adapt the final layer for binary classification.\n","\n","Define Optimizer: Set up the SGD optimizer with momentum.\n","\n","Hyperparameter Grid Search: You’ll need to run a grid search loop over learning rates and weight decay values."],"metadata":{"id":"9JouWae3hF0c"}},{"cell_type":"code","source":["from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","def get_model():\n","    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)  # Load pre-trained EfficientNet B0\n","    for param in model.parameters():\n","        param.requires_grad = False  # Freeze all layers\n","    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)  # Modify final layer adding a linear binary classifier\n","    return model"],"metadata":{"id":"2oHSWr7Pg8ZF","executionInfo":{"status":"ok","timestamp":1731167693830,"user_tz":180,"elapsed":511,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Training and Validation\n","Define Training and Evaluation Loops: Track metrics like precision, recall, accuracy, and F1-score per epoch.\n","\n","Cross-Validation: Implement 5-fold cross-validation, recording average and standard deviation metrics."],"metadata":{"id":"i0GhWA3_iusT"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import numpy as np\n","import time\n","from sklearn.model_selection import KFold\n","\n","\n","def calculate_metrics(true_labels, predictions):\n","    accuracy = accuracy_score(true_labels, predictions)\n","    precision = precision_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","    return accuracy, precision, recall, f1\n","\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    train_losses, val_losses = [], []\n","    best_val_f1 = 0.0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        scheduler.step()  # Adjust learning rate\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        all_preds, all_labels = [], []\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                preds = torch.argmax(outputs, dim=1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # Metrics\n","        accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n","        train_losses.append(running_loss / len(train_loader))\n","        val_losses.append(val_loss / len(val_loader))\n","\n","        if f1 > best_val_f1:\n","            best_val_f1 = f1\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, \"\n","              f\"Val Loss: {val_loss / len(val_loader):.4f}, F1 Score: {f1:.4f}\")\n","\n","    return best_val_f1\n","\n","# Evaluate model on the test set\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='binary')\n","    recall = recall_score(all_labels, all_preds, average='binary')\n","    f1 = f1_score(all_labels, all_preds, average='binary')\n","    return accuracy, precision, recall, f1\n","\n","\n","def grid_search(train_loader, val_loader, learning_rates, weight_decays, num_epochs):\n","    best_model = None\n","    best_f1 = 0\n","    best_params = {}\n","    for lr in learning_rates:\n","        for wd in weight_decays:\n","            model = get_model()\n","            optimizer = optim.SGD(model.classifier[1].parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n","            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n","            criterion = nn.CrossEntropyLoss()\n","            print(f\"\\nTraining with lr={lr}, weight_decay={wd}\")\n","            f1_score = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n","            if f1_score > best_f1:\n","                best_f1 = f1_score\n","                best_model = model\n","                best_params = {\"learning_rate\": lr, \"weight_decay\": wd}\n","    print(f\"\\nBest Model F1: {best_f1} with params {best_params}\")\n","    return best_model, best_params\n","\n","# Step 9: 5-Fold Cross-Validation\n","def cross_validation(best_params, dataset, num_epochs=50, folds=5):\n","    fold_metrics = []\n","    kfold = KFold(n_splits=folds, shuffle=True, random_state=100)\n","\n","    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n","        print(f\"\\nStarting fold {fold + 1}/{folds}\")\n","\n","        # Split dataset indices for training and validation\n","        train_subset = Subset(dataset, train_idx)\n","        val_subset = Subset(dataset, val_idx)\n","\n","        # Create DataLoaders for this fold\n","        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n","\n","        model = get_model()\n","        optimizer = optim.SGD(model.classifier[1].parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'], momentum=0.9)\n","        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n","        criterion = nn.CrossEntropyLoss()\n","\n","        start_time = time.time()\n","        _ = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n","        end_time = time.time()\n","\n","        # Evaluate on test set\n","        accuracy, precision, recall, f1 = evaluate_model(best_model, val_loader)\n","        fold_metrics.append((accuracy, precision, recall, f1, end_time - start_time))\n","\n","    return np.array(fold_metrics)"],"metadata":{"id":"e3uwIrQpitWU","executionInfo":{"status":"ok","timestamp":1731167462223,"user_tz":180,"elapsed":4711,"user":{"displayName":"Tales Alves","userId":"14376949597734287892"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["Run grid search to get best params\n"],"metadata":{"id":"2AMnLXcfl5hX"}},{"cell_type":"code","source":["learning_rates = [1e-2, 1e-3, 1e-4]\n","weight_decays = [1e-3, 1e-4, 1e-5]\n","num_epochs = 50\n","best_model, best_params = grid_search(train_loader, val_loader, learning_rates, weight_decays, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6t_3ytzWEwdM","outputId":"277891ed-d659-4f41-a29d-ba1b9fae0135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training with lr=0.01, weight_decay=0.001\n","Epoch 1/50, Train Loss: 0.6926, Val Loss: 0.6975, F1 Score: 0.6602\n","Epoch 2/50, Train Loss: 0.6919, Val Loss: 0.6974, F1 Score: 0.5957\n","Epoch 3/50, Train Loss: 0.6895, Val Loss: 0.7011, F1 Score: 0.5682\n","Epoch 4/50, Train Loss: 0.6931, Val Loss: 0.6973, F1 Score: 0.4800\n","Epoch 5/50, Train Loss: 0.6503, Val Loss: 0.7152, F1 Score: 0.5301\n","Epoch 6/50, Train Loss: 0.6311, Val Loss: 0.7052, F1 Score: 0.4935\n","Epoch 7/50, Train Loss: 0.6930, Val Loss: 0.7281, F1 Score: 0.4932\n","Epoch 8/50, Train Loss: 0.5705, Val Loss: 0.6801, F1 Score: 0.6735\n","Epoch 9/50, Train Loss: 0.6205, Val Loss: 0.6791, F1 Score: 0.6596\n","Epoch 10/50, Train Loss: 0.7640, Val Loss: 0.6095, F1 Score: 0.7294\n","Epoch 11/50, Train Loss: 0.7228, Val Loss: 0.6572, F1 Score: 0.7021\n","Epoch 12/50, Train Loss: 0.5917, Val Loss: 0.7530, F1 Score: 0.6727\n","Epoch 13/50, Train Loss: 0.7115, Val Loss: 0.6500, F1 Score: 0.6804\n","Epoch 14/50, Train Loss: 0.6104, Val Loss: 0.7014, F1 Score: 0.5676\n","Epoch 15/50, Train Loss: 0.8355, Val Loss: 0.7097, F1 Score: 0.6000\n","Epoch 16/50, Train Loss: 0.5869, Val Loss: 0.8579, F1 Score: 0.6903\n","Epoch 17/50, Train Loss: 0.8912, Val Loss: 0.7205, F1 Score: 0.6364\n","Epoch 18/50, Train Loss: 0.6148, Val Loss: 0.8788, F1 Score: 0.4643\n","Epoch 19/50, Train Loss: 0.7081, Val Loss: 0.6444, F1 Score: 0.6585\n","Epoch 20/50, Train Loss: 0.8297, Val Loss: 0.6695, F1 Score: 0.6809\n","Epoch 21/50, Train Loss: 0.6273, Val Loss: 0.6951, F1 Score: 0.5625\n","Epoch 22/50, Train Loss: 0.6804, Val Loss: 0.6539, F1 Score: 0.6582\n","Epoch 23/50, Train Loss: 0.6250, Val Loss: 0.7822, F1 Score: 0.6535\n","Epoch 24/50, Train Loss: 0.6496, Val Loss: 0.6577, F1 Score: 0.6076\n","Epoch 25/50, Train Loss: 0.6241, Val Loss: 0.6752, F1 Score: 0.5333\n","Epoch 26/50, Train Loss: 0.6968, Val Loss: 0.6729, F1 Score: 0.6216\n","Epoch 27/50, Train Loss: 0.5360, Val Loss: 0.6640, F1 Score: 0.6176\n","Epoch 28/50, Train Loss: 0.5715, Val Loss: 0.6264, F1 Score: 0.6753\n","Epoch 29/50, Train Loss: 0.6067, Val Loss: 0.5764, F1 Score: 0.6933\n","Epoch 30/50, Train Loss: 0.6776, Val Loss: 0.7390, F1 Score: 0.6316\n","Epoch 31/50, Train Loss: 0.6583, Val Loss: 0.6522, F1 Score: 0.6757\n","Epoch 32/50, Train Loss: 0.5865, Val Loss: 0.6988, F1 Score: 0.6377\n","Epoch 33/50, Train Loss: 0.6368, Val Loss: 0.6549, F1 Score: 0.6329\n","Epoch 34/50, Train Loss: 0.5426, Val Loss: 0.6995, F1 Score: 0.5867\n","Epoch 35/50, Train Loss: 0.6130, Val Loss: 0.6155, F1 Score: 0.6377\n","Epoch 36/50, Train Loss: 0.7964, Val Loss: 0.5753, F1 Score: 0.7123\n","Epoch 37/50, Train Loss: 0.5819, Val Loss: 0.6389, F1 Score: 0.7234\n","Epoch 38/50, Train Loss: 0.5994, Val Loss: 0.7084, F1 Score: 0.6479\n","Epoch 39/50, Train Loss: 0.8112, Val Loss: 0.6669, F1 Score: 0.5574\n","Epoch 40/50, Train Loss: 0.8078, Val Loss: 0.6458, F1 Score: 0.7826\n","Epoch 41/50, Train Loss: 0.6306, Val Loss: 0.7200, F1 Score: 0.5758\n"]}]},{"cell_type":"markdown","source":["Run 5-fold cross validation"],"metadata":{"id":"BIc2-qwNFDAh"}},{"cell_type":"code","source":["# Run the cross-validation\n","metrics = cross_validation(best_params, dataset, num_epochs=50, folds=5)\n","\n","# Calculate average and standard deviation of metrics across folds\n","avg_metrics = metrics.mean(axis=0)\n","std_metrics = metrics.std(axis=0)\n","\n","print(f\"\\nAverage metrics over 5 folds:\\n\"\n","      f\"Accuracy: {avg_metrics[0]:.4f} ± {std_metrics[0]:.4f}\\n\"\n","      f\"Precision: {avg_metrics[1]:.4f} ± {std_metrics[1]:.4f}\\n\"\n","      f\"Recall: {avg_metrics[2]:.4f} ± {std_metrics[2]:.4f}\\n\"\n","      f\"F1 Score: {avg_metrics[3]:.4f} ± {std_metrics[3]:.4f}\\n\"\n","      f\"Training Time per Fold: {avg_metrics[4]:.2f} ± {std_metrics[4]:.2f} seconds\")"],"metadata":{"id":"lYQ17dxPl49n"},"execution_count":null,"outputs":[]}]}