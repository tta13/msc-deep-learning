{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b36d38-753e-473e-8f4d-c9bbed815488",
   "metadata": {},
   "source": [
    "# Projeto deep learning\n",
    "\n",
    "## DINO\n",
    "Utilizando DINO para pré-treinar a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203ce8fb-19c2-424f-b0b1-64cf833a1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack dataset\n",
    "import zipfile\n",
    "\n",
    "path_to_zip_file = './datasets/chestx-ray14-v3.zip'\n",
    "directory_to_extract_to = './datasets/chestx-ray14-v3'\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49979e6-efda-49ca-91bb-7ff91f4c8d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext50_32x4d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models as torchvision_models\n",
    "\n",
    "torchvision_archs = sorted(name for name in torchvision_models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(torchvision_models.__dict__[name]))\n",
    "\n",
    "torchvision_archs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df5cac5a-9d52-49bd-afc3-dbc45d1655e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in ./deep_learning/lib/python3.10/site-packages (from timm) (0.12.0)\n",
      "Requirement already satisfied: pyyaml in ./deep_learning/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in ./deep_learning/lib/python3.10/site-packages (from timm) (1.11.0)\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in ./deep_learning/lib/python3.10/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in ./deep_learning/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./deep_learning/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.10.0)\n",
      "Requirement already satisfied: filelock in ./deep_learning/lib/python3.10/site-packages (from huggingface_hub->timm) (3.16.1)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./deep_learning/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./deep_learning/lib/python3.10/site-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: numpy in ./deep_learning/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./deep_learning/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./deep_learning/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./deep_learning/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./deep_learning/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Installing collected packages: tqdm, safetensors, huggingface_hub, timm\n",
      "Successfully installed huggingface_hub-0.26.2 safetensors-0.4.5 timm-1.0.11 tqdm-4.67.0\n"
     ]
    }
   ],
   "source": [
    "! pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "830aa5ee-bc24-416b-9c1b-3d0b8cc87228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/CIN/tta/.cache/torch/hub/facebookresearch_xcit_main\n",
      "/home/CIN/tta/msc-deep-learning/deep_learning/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/CIN/tta/msc-deep-learning/deep_learning/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "usage: DINO [-h]\n",
      "            [--arch {vit_tiny,vit_small,vit_base,xcit,deit_tiny,deit_small,alexnet,convnext_base,convnext_large,convnext_small,convnext_tiny,densenet121,densenet161,densenet169,densenet201,efficientnet_b0,efficientnet_b1,efficientnet_b2,efficientnet_b3,efficientnet_b4,efficientnet_b5,efficientnet_b6,efficientnet_b7,googlenet,inception_v3,mnasnet0_5,mnasnet0_75,mnasnet1_0,mnasnet1_3,mobilenet_v2,mobilenet_v3_large,mobilenet_v3_small,regnet_x_16gf,regnet_x_1_6gf,regnet_x_32gf,regnet_x_3_2gf,regnet_x_400mf,regnet_x_800mf,regnet_x_8gf,regnet_y_128gf,regnet_y_16gf,regnet_y_1_6gf,regnet_y_32gf,regnet_y_3_2gf,regnet_y_400mf,regnet_y_800mf,regnet_y_8gf,resnet101,resnet152,resnet18,resnet34,resnet50,resnext101_32x8d,resnext50_32x4d,shufflenet_v2_x0_5,shufflenet_v2_x1_0,shufflenet_v2_x1_5,shufflenet_v2_x2_0,squeezenet1_0,squeezenet1_1,vgg11,vgg11_bn,vgg13,vgg13_bn,vgg16,vgg16_bn,vgg19,vgg19_bn,vit_b_16,vit_b_32,vit_l_16,vit_l_32,wide_resnet101_2,wide_resnet50_2,xcit_large_24_p16,xcit_large_24_p8,xcit_medium_24_p16,xcit_medium_24_p8,xcit_nano_12_p16,xcit_nano_12_p8,xcit_small_12_p16,xcit_small_12_p8,xcit_small_24_p16,xcit_small_24_p8,xcit_tiny_12_p16,xcit_tiny_12_p8,xcit_tiny_24_p16,xcit_tiny_24_p8}]\n",
      "            [--pretrained PRETRAINED] [--patch_size PATCH_SIZE]\n",
      "            [--out_dim OUT_DIM] [--norm_last_layer NORM_LAST_LAYER]\n",
      "            [--momentum_teacher MOMENTUM_TEACHER]\n",
      "            [--use_bn_in_head USE_BN_IN_HEAD]\n",
      "            [--warmup_teacher_temp WARMUP_TEACHER_TEMP]\n",
      "            [--teacher_temp TEACHER_TEMP]\n",
      "            [--warmup_teacher_temp_epochs WARMUP_TEACHER_TEMP_EPOCHS]\n",
      "            [--use_fp16 USE_FP16] [--weight_decay WEIGHT_DECAY]\n",
      "            [--weight_decay_end WEIGHT_DECAY_END] [--clip_grad CLIP_GRAD]\n",
      "            [--batch_size_per_gpu BATCH_SIZE_PER_GPU] [--epochs EPOCHS]\n",
      "            [--freeze_last_layer FREEZE_LAST_LAYER] [--lr LR]\n",
      "            [--warmup_epochs WARMUP_EPOCHS] [--min_lr MIN_LR]\n",
      "            [--optimizer {adamw,sgd,lars}] [--drop_path_rate DROP_PATH_RATE]\n",
      "            [--global_crops_scale GLOBAL_CROPS_SCALE [GLOBAL_CROPS_SCALE ...]]\n",
      "            [--local_crops_number LOCAL_CROPS_NUMBER]\n",
      "            [--local_crops_scale LOCAL_CROPS_SCALE [LOCAL_CROPS_SCALE ...]]\n",
      "            [--data_path DATA_PATH] [--output_dir OUTPUT_DIR]\n",
      "            [--saveckp_freq SAVECKP_FREQ] [--seed SEED]\n",
      "            [--num_workers NUM_WORKERS] [--dist_url DIST_URL]\n",
      "            [--local_rank LOCAL_RANK]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --arch {vit_tiny,vit_small,vit_base,xcit,deit_tiny,deit_small,alexnet,convnext_base,convnext_large,convnext_small,convnext_tiny,densenet121,densenet161,densenet169,densenet201,efficientnet_b0,efficientnet_b1,efficientnet_b2,efficientnet_b3,efficientnet_b4,efficientnet_b5,efficientnet_b6,efficientnet_b7,googlenet,inception_v3,mnasnet0_5,mnasnet0_75,mnasnet1_0,mnasnet1_3,mobilenet_v2,mobilenet_v3_large,mobilenet_v3_small,regnet_x_16gf,regnet_x_1_6gf,regnet_x_32gf,regnet_x_3_2gf,regnet_x_400mf,regnet_x_800mf,regnet_x_8gf,regnet_y_128gf,regnet_y_16gf,regnet_y_1_6gf,regnet_y_32gf,regnet_y_3_2gf,regnet_y_400mf,regnet_y_800mf,regnet_y_8gf,resnet101,resnet152,resnet18,resnet34,resnet50,resnext101_32x8d,resnext50_32x4d,shufflenet_v2_x0_5,shufflenet_v2_x1_0,shufflenet_v2_x1_5,shufflenet_v2_x2_0,squeezenet1_0,squeezenet1_1,vgg11,vgg11_bn,vgg13,vgg13_bn,vgg16,vgg16_bn,vgg19,vgg19_bn,vit_b_16,vit_b_32,vit_l_16,vit_l_32,wide_resnet101_2,wide_resnet50_2,xcit_large_24_p16,xcit_large_24_p8,xcit_medium_24_p16,xcit_medium_24_p8,xcit_nano_12_p16,xcit_nano_12_p8,xcit_small_12_p16,xcit_small_12_p8,xcit_small_24_p16,xcit_small_24_p8,xcit_tiny_12_p16,xcit_tiny_12_p8,xcit_tiny_24_p16,xcit_tiny_24_p8}\n",
      "                        Name of architecture to train. For quick experiments\n",
      "                        with ViTs, we recommend using vit_tiny or vit_small.\n",
      "  --pretrained PRETRAINED\n",
      "                        If should load pre-trained version of the\n",
      "                        architecture.\n",
      "  --patch_size PATCH_SIZE\n",
      "                        Size in pixels of input square patches - default 16\n",
      "                        (for 16x16 patches). Using smaller values leads to\n",
      "                        better performance but requires more memory. Applies\n",
      "                        only for ViTs (vit_tiny, vit_small and vit_base). If\n",
      "                        <16, we recommend disabling mixed precision training\n",
      "                        (--use_fp16 false) to avoid unstabilities.\n",
      "  --out_dim OUT_DIM     Dimensionality of the DINO head output. For complex\n",
      "                        and large datasets large values (like 65k) work well.\n",
      "  --norm_last_layer NORM_LAST_LAYER\n",
      "                        Whether or not to weight normalize the last layer of\n",
      "                        the DINO head. Not normalizing leads to better\n",
      "                        performance but can make the training unstable. In our\n",
      "                        experiments, we typically set this paramater to False\n",
      "                        with vit_small and True with vit_base.\n",
      "  --momentum_teacher MOMENTUM_TEACHER\n",
      "                        Base EMA parameter for teacher update. The value is\n",
      "                        increased to 1 during training with cosine schedule.\n",
      "                        We recommend setting a higher value with small\n",
      "                        batches: for example use 0.9995 with batch size of\n",
      "                        256.\n",
      "  --use_bn_in_head USE_BN_IN_HEAD\n",
      "                        Whether to use batch normalizations in projection head\n",
      "                        (Default: False)\n",
      "  --warmup_teacher_temp WARMUP_TEACHER_TEMP\n",
      "                        Initial value for the teacher temperature: 0.04 works\n",
      "                        well in most cases. Try decreasing it if the training\n",
      "                        loss does not decrease.\n",
      "  --teacher_temp TEACHER_TEMP\n",
      "                        Final value (after linear warmup) of the teacher\n",
      "                        temperature. For most experiments, anything above 0.07\n",
      "                        is unstable. We recommend starting with the default\n",
      "                        value of 0.04 and increase this slightly if needed.\n",
      "  --warmup_teacher_temp_epochs WARMUP_TEACHER_TEMP_EPOCHS\n",
      "                        Number of warmup epochs for the teacher temperature\n",
      "                        (Default: 30).\n",
      "  --use_fp16 USE_FP16   Whether or not to use half precision for training.\n",
      "                        Improves training time and memory requirements, but\n",
      "                        can provoke instability and slight decay of\n",
      "                        performance. We recommend disabling mixed precision if\n",
      "                        the loss is unstable, if reducing the patch size or if\n",
      "                        training with bigger ViTs.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Initial value of the weight decay. With ViT, a smaller\n",
      "                        value at the beginning of training works well.\n",
      "  --weight_decay_end WEIGHT_DECAY_END\n",
      "                        Final value of the weight decay. We use a cosine\n",
      "                        schedule for WD and using a larger decay by the end of\n",
      "                        training improves performance for ViTs.\n",
      "  --clip_grad CLIP_GRAD\n",
      "                        Maximal parameter gradient norm if using gradient\n",
      "                        clipping. Clipping with norm .3 ~ 1.0 can help\n",
      "                        optimization for larger ViT architectures. 0 for\n",
      "                        disabling.\n",
      "  --batch_size_per_gpu BATCH_SIZE_PER_GPU\n",
      "                        Per-GPU batch-size : number of distinct images loaded\n",
      "                        on one GPU.\n",
      "  --epochs EPOCHS       Number of epochs of training.\n",
      "  --freeze_last_layer FREEZE_LAST_LAYER\n",
      "                        Number of epochs during which we keep the output layer\n",
      "                        fixed. Typically doing so during the first epoch helps\n",
      "                        training. Try increasing this value if the loss does\n",
      "                        not decrease.\n",
      "  --lr LR               Learning rate at the end of linear warmup (highest LR\n",
      "                        used during training). The learning rate is linearly\n",
      "                        scaled with the batch size, and specified here for a\n",
      "                        reference batch size of 256.\n",
      "  --warmup_epochs WARMUP_EPOCHS\n",
      "                        Number of epochs for the linear learning-rate warm up.\n",
      "  --min_lr MIN_LR       Target LR at the end of optimization. We use a cosine\n",
      "                        LR schedule with linear warmup.\n",
      "  --optimizer {adamw,sgd,lars}\n",
      "                        Type of optimizer. We recommend using adamw with ViTs.\n",
      "  --drop_path_rate DROP_PATH_RATE\n",
      "                        stochastic depth rate\n",
      "  --global_crops_scale GLOBAL_CROPS_SCALE [GLOBAL_CROPS_SCALE ...]\n",
      "                        Scale range of the cropped image before resizing,\n",
      "                        relatively to the origin image. Used for large global\n",
      "                        view cropping. When disabling multi-crop\n",
      "                        (--local_crops_number 0), we recommand using a wider\n",
      "                        range of scale (\"--global_crops_scale 0.14 1.\" for\n",
      "                        example)\n",
      "  --local_crops_number LOCAL_CROPS_NUMBER\n",
      "                        Number of small local views to generate. Set this\n",
      "                        parameter to 0 to disable multi-crop training. When\n",
      "                        disabling multi-crop we recommend to use \"--\n",
      "                        global_crops_scale 0.14 1.\"\n",
      "  --local_crops_scale LOCAL_CROPS_SCALE [LOCAL_CROPS_SCALE ...]\n",
      "                        Scale range of the cropped image before resizing,\n",
      "                        relatively to the origin image. Used for small local\n",
      "                        view cropping of multi-crop.\n",
      "  --data_path DATA_PATH\n",
      "                        Please specify path to the ImageNet training data.\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        Path to save logs and checkpoints.\n",
      "  --saveckp_freq SAVECKP_FREQ\n",
      "                        Save checkpoint every x epochs.\n",
      "  --seed SEED           Random seed.\n",
      "  --num_workers NUM_WORKERS\n",
      "                        Number of data loading workers per GPU.\n",
      "  --dist_url DIST_URL   url used to set up distributed training; see\n",
      "                        https://pytorch.org/docs/stable/distributed.html\n",
      "  --local_rank LOCAL_RANK\n",
      "                        Please ignore and do not set this argument.\n"
     ]
    }
   ],
   "source": [
    "! python -m main_dino -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1eaf35-07ad-4ad9-b41b-1172006959e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: mkdier: command not found\n",
      "mkdir: cannot create directory ‘./output/DINOXray’: File exists\n",
      "datasets\t\t eval_video_segmentation.py  README.md\n",
      "deep_learning\t\t fine_tuning.ipynb\t     results\n",
      "dino.ipynb\t\t hubconf.py\t\t     run_with_submitit.py\n",
      "DINO_README.md\t\t __init__.py\t\t     utils.py\n",
      "eval_copy_detection.py\t LICENSE\t\t     video_generation.py\n",
      "eval_image_retrieval.py  main_dino.py\t\t     vision_transformer.py\n",
      "eval_knn.py\t\t output\t\t\t     visualize_attention.py\n",
      "eval_linear.py\t\t __pycache__\n"
     ]
    }
   ],
   "source": [
    "! mkdir ./output/\n",
    "! mkdir ./output/DINOXray\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe528a3b-c8e8-44fb-bec5-368620bfaf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvNormActivation(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU(inplace=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "    )\n",
      "    (3): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (8): ConvNormActivation(\n",
      "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# Load EfficientNet-B0 pre-trained on ImageNet\n",
    "model = efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Access the feature extraction layers\n",
    "feature_extractor = model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0acadd24-33af-4c6c-83c1-36a821d151fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd85b2b-a25b-4ac4-bc70-fa3daf980d6d",
   "metadata": {},
   "source": [
    "## Treinamento com o DINO-CXR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e295735-1e90-46e7-8c42-eb56ddc36e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CIN/tta/msc-deep-learning/deep_learning/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Using cache found in /home/CIN/tta/.cache/torch/hub/facebookresearch_xcit_main\n",
      "/home/CIN/tta/msc-deep-learning/deep_learning/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/CIN/tta/msc-deep-learning/deep_learning/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "| distributed init (rank 0): env://\n",
      "git:\n",
      "  sha: a9e2f61f2c256188825bb23a8d91287d25834190, status: has uncommited changes, branch: main\n",
      "\n",
      "arch: efficientnet_b0\n",
      "batch_size_per_gpu: 64\n",
      "clip_grad: 3.0\n",
      "data_path: ./datasets/chestx-ray14-v3/train/\n",
      "dist_url: env://\n",
      "drop_path_rate: 0.1\n",
      "epochs: 100\n",
      "freeze_last_layer: 1\n",
      "global_crops_scale: (0.4, 1.0)\n",
      "gpu: 0\n",
      "local_crops_number: 8\n",
      "local_crops_scale: (0.05, 0.4)\n",
      "local_rank: 0\n",
      "lr: 0.0005\n",
      "min_lr: 0.00025\n",
      "momentum_teacher: 0.996\n",
      "norm_last_layer: True\n",
      "num_workers: 8\n",
      "optimizer: adam\n",
      "out_dim: 640\n",
      "output_dir: ./output/DINOXray\n",
      "patch_size: 16\n",
      "pretrained: True\n",
      "rank: 0\n",
      "resume_training: False\n",
      "saveckp_freq: 20\n",
      "seed: 0\n",
      "teacher_temp: 0.04\n",
      "use_bn_in_head: False\n",
      "use_fp16: True\n",
      "warmup_epochs: 10\n",
      "warmup_teacher_temp: 0.04\n",
      "warmup_teacher_temp_epochs: 0\n",
      "weight_decay: 1e-06\n",
      "weight_decay_end: 1e-05\n",
      "world_size: 1\n",
      "Data loaded: there are 12096 images.\n",
      "Using cache found in /home/CIN/tta/.cache/torch/hub/facebookresearch_xcit_main\n",
      "Student and Teacher are built: they are both efficientnet_b0 network.\n",
      "Loss, optimizer and schedulers ready.\n",
      "Starting DINO training !\n",
      "Epoch: [0/100]  [  0/189]  eta: 0:14:39  loss: 6.368867 (6.368867)  lr: 0.000000 (0.000000)  wd: 0.000001 (0.000001)  time: 4.653090  data: 2.958415  max mem: 4023\n",
      "Epoch: [0/100]  [ 20/189]  eta: 0:02:01  loss: 6.569712 (6.545473)  lr: 0.000003 (0.000003)  wd: 0.000001 (0.000001)  time: 0.520251  data: 0.001823  max mem: 4023\n",
      "Epoch: [0/100]  [ 40/189]  eta: 0:01:31  loss: 6.641033 (6.591986)  lr: 0.000008 (0.000005)  wd: 0.000001 (0.000001)  time: 0.506384  data: 0.001426  max mem: 4023\n",
      "Epoch: [0/100]  [ 60/189]  eta: 0:01:14  loss: 6.631047 (6.607245)  lr: 0.000013 (0.000008)  wd: 0.000001 (0.000001)  time: 0.507018  data: 0.000530  max mem: 4023\n",
      "Epoch: [0/100]  [ 80/189]  eta: 0:01:01  loss: 6.607897 (6.607726)  lr: 0.000019 (0.000011)  wd: 0.000001 (0.000001)  time: 0.506941  data: 0.000611  max mem: 4023\n",
      "Epoch: [0/100]  [100/189]  eta: 0:00:48  loss: 6.572028 (6.601214)  lr: 0.000024 (0.000013)  wd: 0.000001 (0.000001)  time: 0.504929  data: 0.000186  max mem: 4023\n",
      "Epoch: [0/100]  [120/189]  eta: 0:00:37  loss: 6.546976 (6.592632)  lr: 0.000029 (0.000016)  wd: 0.000001 (0.000001)  time: 0.506877  data: 0.000144  max mem: 4023\n",
      "Epoch: [0/100]  [140/189]  eta: 0:00:26  loss: 6.530891 (6.583697)  lr: 0.000034 (0.000019)  wd: 0.000001 (0.000001)  time: 0.507082  data: 0.000971  max mem: 4023\n",
      "Epoch: [0/100]  [160/189]  eta: 0:00:15  loss: 6.519709 (6.576068)  lr: 0.000040 (0.000021)  wd: 0.000001 (0.000001)  time: 0.505689  data: 0.000169  max mem: 4023\n",
      "Epoch: [0/100]  [180/189]  eta: 0:00:04  loss: 6.512286 (6.569485)  lr: 0.000045 (0.000024)  wd: 0.000001 (0.000001)  time: 0.498418  data: 0.000188  max mem: 4023\n",
      "Epoch: [0/100]  [188/189]  eta: 0:00:00  loss: 6.512214 (6.567096)  lr: 0.000047 (0.000025)  wd: 0.000001 (0.000001)  time: 0.490465  data: 0.000148  max mem: 4023\n",
      "Epoch: [0/100] Total time: 0:01:39 (0.528415 s / it)\n",
      "Averaged stats: loss: 6.512214 (6.567096)  lr: 0.000047 (0.000025)  wd: 0.000001 (0.000001)\n",
      "Epoch: [1/100]  [  0/189]  eta: 0:12:01  loss: 6.498361 (6.498361)  lr: 0.000050 (0.000050)  wd: 0.000001 (0.000001)  time: 3.819522  data: 3.220698  max mem: 4023\n",
      "Epoch: [1/100]  [ 20/189]  eta: 0:01:59  loss: 6.495526 (6.498387)  lr: 0.000053 (0.000053)  wd: 0.000001 (0.000001)  time: 0.550293  data: 0.033416  max mem: 4023\n",
      "Epoch: [1/100]  [ 40/189]  eta: 0:01:30  loss: 6.482755 (6.491614)  lr: 0.000058 (0.000055)  wd: 0.000001 (0.000001)  time: 0.508484  data: 0.001821  max mem: 4023\n",
      "Epoch: [1/100]  [ 60/189]  eta: 0:01:14  loss: 6.475389 (6.486120)  lr: 0.000063 (0.000058)  wd: 0.000001 (0.000001)  time: 0.504829  data: 0.000329  max mem: 4023\n",
      "Epoch: [1/100]  [ 80/189]  eta: 0:01:00  loss: 6.469744 (6.482229)  lr: 0.000069 (0.000061)  wd: 0.000001 (0.000001)  time: 0.505567  data: 0.000146  max mem: 4023\n",
      "Epoch: [1/100]  [100/189]  eta: 0:00:48  loss: 6.468421 (6.479400)  lr: 0.000074 (0.000063)  wd: 0.000001 (0.000001)  time: 0.506579  data: 0.000169  max mem: 4023\n",
      "Epoch: [1/100]  [120/189]  eta: 0:00:37  loss: 6.466108 (6.477281)  lr: 0.000079 (0.000066)  wd: 0.000001 (0.000001)  time: 0.505990  data: 0.000173  max mem: 4023\n",
      "Epoch: [1/100]  [140/189]  eta: 0:00:26  loss: 6.466070 (6.475691)  lr: 0.000084 (0.000069)  wd: 0.000001 (0.000001)  time: 0.504857  data: 0.000302  max mem: 4023\n",
      "Epoch: [1/100]  [160/189]  eta: 0:00:15  loss: 6.465050 (6.474408)  lr: 0.000090 (0.000071)  wd: 0.000001 (0.000001)  time: 0.509904  data: 0.000164  max mem: 4023\n",
      "Epoch: [1/100]  [180/189]  eta: 0:00:04  loss: 6.464486 (6.473335)  lr: 0.000095 (0.000074)  wd: 0.000001 (0.000001)  time: 0.501664  data: 0.000131  max mem: 4023\n",
      "Epoch: [1/100]  [188/189]  eta: 0:00:00  loss: 6.464558 (6.472960)  lr: 0.000097 (0.000075)  wd: 0.000001 (0.000001)  time: 0.497293  data: 0.000082  max mem: 4023\n",
      "Epoch: [1/100] Total time: 0:01:39 (0.528372 s / it)\n",
      "Averaged stats: loss: 6.464558 (6.472960)  lr: 0.000097 (0.000075)  wd: 0.000001 (0.000001)\n",
      "Epoch: [2/100]  [  0/189]  eta: 0:13:04  loss: 6.463844 (6.463844)  lr: 0.000100 (0.000100)  wd: 0.000001 (0.000001)  time: 4.152099  data: 3.576351  max mem: 4023\n",
      "Epoch: [2/100]  [ 20/189]  eta: 0:01:58  loss: 6.464355 (6.464409)  lr: 0.000103 (0.000103)  wd: 0.000001 (0.000001)  time: 0.527282  data: 0.010162  max mem: 4023\n",
      "Epoch: [2/100]  [ 40/189]  eta: 0:01:30  loss: 6.464012 (6.464252)  lr: 0.000108 (0.000105)  wd: 0.000001 (0.000001)  time: 0.509757  data: 0.000373  max mem: 4023\n",
      "Epoch: [2/100]  [ 60/189]  eta: 0:01:14  loss: 6.463611 (6.464069)  lr: 0.000113 (0.000108)  wd: 0.000001 (0.000001)  time: 0.505429  data: 0.000933  max mem: 4023\n",
      "Epoch: [2/100]  [ 80/189]  eta: 0:01:00  loss: 6.463494 (6.463935)  lr: 0.000119 (0.000111)  wd: 0.000001 (0.000001)  time: 0.508757  data: 0.000179  max mem: 4023\n",
      "Epoch: [2/100]  [100/189]  eta: 0:00:48  loss: 6.463348 (6.463825)  lr: 0.000124 (0.000113)  wd: 0.000001 (0.000001)  time: 0.514856  data: 0.000239  max mem: 4023\n",
      "Epoch: [2/100]  [120/189]  eta: 0:00:37  loss: 6.463095 (6.463706)  lr: 0.000129 (0.000116)  wd: 0.000001 (0.000001)  time: 0.519703  data: 0.000137  max mem: 4023\n",
      "Epoch: [2/100]  [140/189]  eta: 0:00:26  loss: 6.463061 (6.463615)  lr: 0.000134 (0.000119)  wd: 0.000001 (0.000001)  time: 0.510428  data: 0.000169  max mem: 4023\n",
      "Epoch: [2/100]  [160/189]  eta: 0:00:15  loss: 6.462950 (6.463532)  lr: 0.000140 (0.000121)  wd: 0.000001 (0.000001)  time: 0.518424  data: 0.000386  max mem: 4023\n",
      "Epoch: [2/100]  [180/189]  eta: 0:00:04  loss: 6.462854 (6.463458)  lr: 0.000145 (0.000124)  wd: 0.000001 (0.000001)  time: 0.505364  data: 0.000256  max mem: 4023\n",
      "Epoch: [2/100]  [188/189]  eta: 0:00:00  loss: 6.462854 (6.463429)  lr: 0.000147 (0.000125)  wd: 0.000001 (0.000001)  time: 0.507111  data: 0.000076  max mem: 4023\n",
      "Epoch: [2/100] Total time: 0:01:40 (0.532990 s / it)\n",
      "Averaged stats: loss: 6.462854 (6.463429)  lr: 0.000147 (0.000125)  wd: 0.000001 (0.000001)\n",
      "Epoch: [3/100]  [  0/189]  eta: 0:13:03  loss: 6.462670 (6.462670)  lr: 0.000150 (0.000150)  wd: 0.000001 (0.000001)  time: 4.147149  data: 3.572783  max mem: 4023\n",
      "Epoch: [3/100]  [ 20/189]  eta: 0:01:57  loss: 6.462634 (6.462678)  lr: 0.000153 (0.000153)  wd: 0.000001 (0.000001)  time: 0.522177  data: 0.000165  max mem: 4023\n",
      "Epoch: [3/100]  [ 40/189]  eta: 0:01:30  loss: 6.462621 (6.462672)  lr: 0.000158 (0.000155)  wd: 0.000001 (0.000001)  time: 0.511282  data: 0.000236  max mem: 4023\n",
      "Epoch: [3/100]  [ 60/189]  eta: 0:01:14  loss: 6.462534 (6.462627)  lr: 0.000163 (0.000158)  wd: 0.000001 (0.000001)  time: 0.509434  data: 0.000322  max mem: 4023\n",
      "Epoch: [3/100]  [ 80/189]  eta: 0:01:01  loss: 6.462507 (6.462598)  lr: 0.000169 (0.000161)  wd: 0.000001 (0.000001)  time: 0.520296  data: 0.000155  max mem: 4023\n",
      "Epoch: [3/100]  [100/189]  eta: 0:00:48  loss: 6.462418 (6.462567)  lr: 0.000174 (0.000163)  wd: 0.000001 (0.000001)  time: 0.509493  data: 0.000150  max mem: 4023\n",
      "Epoch: [3/100]  [120/189]  eta: 0:00:37  loss: 6.462313 (6.462534)  lr: 0.000179 (0.000166)  wd: 0.000001 (0.000001)  time: 0.511926  data: 0.000241  max mem: 4023\n",
      "Epoch: [3/100]  [140/189]  eta: 0:00:26  loss: 6.462350 (6.462508)  lr: 0.000184 (0.000169)  wd: 0.000001 (0.000001)  time: 0.512057  data: 0.000192  max mem: 4023\n",
      "Epoch: [3/100]  [160/189]  eta: 0:00:15  loss: 6.462289 (6.462481)  lr: 0.000190 (0.000171)  wd: 0.000001 (0.000001)  time: 0.520973  data: 0.000221  max mem: 4023\n",
      "Epoch: [3/100]  [180/189]  eta: 0:00:04  loss: 6.462271 (6.462460)  lr: 0.000195 (0.000174)  wd: 0.000001 (0.000001)  time: 0.509018  data: 0.000134  max mem: 4023\n",
      "Epoch: [3/100]  [188/189]  eta: 0:00:00  loss: 6.462234 (6.462449)  lr: 0.000197 (0.000175)  wd: 0.000001 (0.000001)  time: 0.502760  data: 0.000086  max mem: 4023\n",
      "Epoch: [3/100] Total time: 0:01:40 (0.532908 s / it)\n",
      "Averaged stats: loss: 6.462234 (6.462449)  lr: 0.000197 (0.000175)  wd: 0.000001 (0.000001)\n",
      "Epoch: [4/100]  [  0/189]  eta: 0:12:54  loss: 6.462271 (6.462271)  lr: 0.000200 (0.000200)  wd: 0.000001 (0.000001)  time: 4.096707  data: 3.488271  max mem: 4023\n",
      "Epoch: [4/100]  [ 20/189]  eta: 0:01:57  loss: 6.462176 (6.462179)  lr: 0.000203 (0.000203)  wd: 0.000001 (0.000001)  time: 0.524620  data: 0.008767  max mem: 4023\n",
      "Epoch: [4/100]  [ 40/189]  eta: 0:01:30  loss: 6.462130 (6.462159)  lr: 0.000208 (0.000205)  wd: 0.000001 (0.000001)  time: 0.513705  data: 0.003363  max mem: 4023\n",
      "Epoch: [4/100]  [ 60/189]  eta: 0:01:14  loss: 6.462108 (6.462150)  lr: 0.000213 (0.000208)  wd: 0.000001 (0.000001)  time: 0.510120  data: 0.000161  max mem: 4023\n",
      "Epoch: [4/100]  [ 80/189]  eta: 0:01:01  loss: 6.462088 (6.462140)  lr: 0.000219 (0.000211)  wd: 0.000001 (0.000001)  time: 0.523051  data: 0.000286  max mem: 4023\n",
      "Epoch: [4/100]  [100/189]  eta: 0:00:49  loss: 6.462107 (6.462129)  lr: 0.000224 (0.000213)  wd: 0.000001 (0.000001)  time: 0.511347  data: 0.000339  max mem: 4023\n",
      "Epoch: [4/100]  [120/189]  eta: 0:00:37  loss: 6.462038 (6.462114)  lr: 0.000229 (0.000216)  wd: 0.000001 (0.000001)  time: 0.514381  data: 0.000353  max mem: 4023\n",
      "Epoch: [4/100]  [140/189]  eta: 0:00:26  loss: 6.462033 (6.462102)  lr: 0.000235 (0.000219)  wd: 0.000001 (0.000001)  time: 0.519387  data: 0.000305  max mem: 4023\n",
      "Epoch: [4/100]  [160/189]  eta: 0:00:15  loss: 6.461984 (6.462088)  lr: 0.000240 (0.000221)  wd: 0.000001 (0.000001)  time: 0.519372  data: 0.000149  max mem: 4023\n",
      "Epoch: [4/100]  [180/189]  eta: 0:00:04  loss: 6.461959 (6.462073)  lr: 0.000245 (0.000224)  wd: 0.000001 (0.000001)  time: 0.508103  data: 0.000589  max mem: 4023\n",
      "Epoch: [4/100]  [188/189]  eta: 0:00:00  loss: 6.461948 (6.462068)  lr: 0.000247 (0.000225)  wd: 0.000001 (0.000001)  time: 0.501627  data: 0.000126  max mem: 4023\n",
      "Epoch: [4/100] Total time: 0:01:41 (0.534434 s / it)\n",
      "Averaged stats: loss: 6.461948 (6.462068)  lr: 0.000247 (0.000225)  wd: 0.000001 (0.000001)\n",
      "Epoch: [5/100]  [  0/189]  eta: 0:13:27  loss: 6.461991 (6.461991)  lr: 0.000250 (0.000250)  wd: 0.000001 (0.000001)  time: 4.273926  data: 3.674400  max mem: 4023\n",
      "Epoch: [5/100]  [ 20/189]  eta: 0:01:57  loss: 6.461948 (6.461955)  lr: 0.000253 (0.000253)  wd: 0.000001 (0.000001)  time: 0.518053  data: 0.002460  max mem: 4023\n",
      "Epoch: [5/100]  [ 40/189]  eta: 0:01:29  loss: 6.461918 (6.461939)  lr: 0.000258 (0.000255)  wd: 0.000001 (0.000001)  time: 0.505019  data: 0.000879  max mem: 4023\n",
      "Epoch: [5/100]  [ 60/189]  eta: 0:01:13  loss: 6.461879 (6.461928)  lr: 0.000263 (0.000258)  wd: 0.000001 (0.000001)  time: 0.512787  data: 0.000430  max mem: 4023\n",
      "Epoch: [5/100]  [ 80/189]  eta: 0:01:00  loss: 6.461901 (6.461921)  lr: 0.000269 (0.000261)  wd: 0.000001 (0.000001)  time: 0.512420  data: 0.000167  max mem: 4023\n",
      "Epoch: [5/100]  [100/189]  eta: 0:00:48  loss: 6.461865 (6.461912)  lr: 0.000274 (0.000263)  wd: 0.000001 (0.000001)  time: 0.515647  data: 0.000451  max mem: 4023\n",
      "Epoch: [5/100]  [120/189]  eta: 0:00:37  loss: 6.461834 (6.461901)  lr: 0.000279 (0.000266)  wd: 0.000001 (0.000001)  time: 0.522788  data: 0.000419  max mem: 4023\n",
      "Epoch: [5/100]  [140/189]  eta: 0:00:26  loss: 6.461855 (6.461896)  lr: 0.000285 (0.000269)  wd: 0.000001 (0.000001)  time: 0.522414  data: 0.000372  max mem: 4023\n",
      "Epoch: [5/100]  [160/189]  eta: 0:00:15  loss: 6.461850 (6.461889)  lr: 0.000290 (0.000271)  wd: 0.000001 (0.000001)  time: 0.519180  data: 0.000393  max mem: 4023\n",
      "Epoch: [5/100]  [180/189]  eta: 0:00:04  loss: 6.461834 (6.461885)  lr: 0.000295 (0.000274)  wd: 0.000001 (0.000001)  time: 0.505488  data: 0.000427  max mem: 4023\n",
      "Epoch: [5/100]  [188/189]  eta: 0:00:00  loss: 6.461860 (6.461884)  lr: 0.000297 (0.000275)  wd: 0.000001 (0.000001)  time: 0.501296  data: 0.000238  max mem: 4023\n",
      "Epoch: [5/100] Total time: 0:01:41 (0.534406 s / it)\n",
      "Averaged stats: loss: 6.461860 (6.461884)  lr: 0.000297 (0.000275)  wd: 0.000001 (0.000001)\n",
      "Epoch: [6/100]  [  0/189]  eta: 0:14:43  loss: 6.461861 (6.461861)  lr: 0.000300 (0.000300)  wd: 0.000001 (0.000001)  time: 4.672652  data: 4.068309  max mem: 4023\n",
      "Epoch: [6/100]  [ 20/189]  eta: 0:02:00  loss: 6.461805 (6.461814)  lr: 0.000303 (0.000303)  wd: 0.000001 (0.000001)  time: 0.517074  data: 0.000428  max mem: 4023\n",
      "Epoch: [6/100]  [ 40/189]  eta: 0:01:31  loss: 6.461781 (6.461797)  lr: 0.000308 (0.000305)  wd: 0.000001 (0.000001)  time: 0.511307  data: 0.000886  max mem: 4023\n",
      "Epoch: [6/100]  [ 60/189]  eta: 0:01:15  loss: 6.461765 (6.461789)  lr: 0.000313 (0.000308)  wd: 0.000001 (0.000001)  time: 0.512288  data: 0.000880  max mem: 4023\n",
      "Epoch: [6/100]  [ 80/189]  eta: 0:01:01  loss: 6.461763 (6.461785)  lr: 0.000319 (0.000311)  wd: 0.000001 (0.000001)  time: 0.512160  data: 0.000866  max mem: 4023\n",
      "Epoch: [6/100]  [100/189]  eta: 0:00:49  loss: 6.461782 (6.461786)  lr: 0.000324 (0.000313)  wd: 0.000001 (0.000001)  time: 0.517478  data: 0.000225  max mem: 4023\n",
      "Epoch: [6/100]  [120/189]  eta: 0:00:37  loss: 6.461758 (6.461781)  lr: 0.000329 (0.000316)  wd: 0.000001 (0.000001)  time: 0.509645  data: 0.000162  max mem: 4023\n",
      "Epoch: [6/100]  [140/189]  eta: 0:00:26  loss: 6.461771 (6.461781)  lr: 0.000335 (0.000319)  wd: 0.000001 (0.000001)  time: 0.511893  data: 0.000360  max mem: 4023\n",
      "Epoch: [6/100]  [160/189]  eta: 0:00:15  loss: 6.461771 (6.461781)  lr: 0.000340 (0.000321)  wd: 0.000001 (0.000001)  time: 0.534560  data: 0.000920  max mem: 4023\n",
      "Epoch: [6/100]  [180/189]  eta: 0:00:04  loss: 6.461769 (6.461779)  lr: 0.000345 (0.000324)  wd: 0.000001 (0.000001)  time: 0.508123  data: 0.000113  max mem: 4023\n",
      "Epoch: [6/100]  [188/189]  eta: 0:00:00  loss: 6.461743 (6.461777)  lr: 0.000347 (0.000325)  wd: 0.000001 (0.000001)  time: 0.504259  data: 0.000079  max mem: 4023\n",
      "Epoch: [6/100] Total time: 0:01:41 (0.536632 s / it)\n",
      "Averaged stats: loss: 6.461743 (6.461777)  lr: 0.000347 (0.000325)  wd: 0.000001 (0.000001)\n",
      "Epoch: [7/100]  [  0/189]  eta: 0:11:05  loss: 6.461734 (6.461734)  lr: 0.000350 (0.000350)  wd: 0.000001 (0.000001)  time: 3.523688  data: 2.915729  max mem: 4023\n",
      "Epoch: [7/100]  [ 20/189]  eta: 0:01:55  loss: 6.461724 (6.461728)  lr: 0.000353 (0.000353)  wd: 0.000001 (0.000001)  time: 0.538684  data: 0.017113  max mem: 4023\n",
      "Epoch: [7/100]  [ 40/189]  eta: 0:01:29  loss: 6.461730 (6.461727)  lr: 0.000358 (0.000355)  wd: 0.000001 (0.000001)  time: 0.518055  data: 0.000978  max mem: 4023\n",
      "Epoch: [7/100]  [ 60/189]  eta: 0:01:14  loss: 6.461733 (6.461729)  lr: 0.000363 (0.000358)  wd: 0.000001 (0.000001)  time: 0.522836  data: 0.000221  max mem: 4023\n",
      "Epoch: [7/100]  [ 80/189]  eta: 0:01:00  loss: 6.461712 (6.461726)  lr: 0.000369 (0.000361)  wd: 0.000001 (0.000001)  time: 0.510567  data: 0.000161  max mem: 4023\n",
      "Epoch: [7/100]  [100/189]  eta: 0:00:49  loss: 6.461708 (6.461721)  lr: 0.000374 (0.000363)  wd: 0.000001 (0.000001)  time: 0.518994  data: 0.000178  max mem: 4023\n",
      "Epoch: [7/100]  [120/189]  eta: 0:00:37  loss: 6.461711 (6.461718)  lr: 0.000379 (0.000366)  wd: 0.000001 (0.000001)  time: 0.520469  data: 0.000134  max mem: 4023\n",
      "Epoch: [7/100]  [140/189]  eta: 0:00:26  loss: 6.461705 (6.461717)  lr: 0.000385 (0.000369)  wd: 0.000001 (0.000001)  time: 0.513434  data: 0.000151  max mem: 4023\n",
      "Epoch: [7/100]  [160/189]  eta: 0:00:15  loss: 6.461676 (6.461712)  lr: 0.000390 (0.000371)  wd: 0.000001 (0.000001)  time: 0.511234  data: 0.000146  max mem: 4023\n",
      "Epoch: [7/100]  [180/189]  eta: 0:00:04  loss: 6.461679 (6.461709)  lr: 0.000395 (0.000374)  wd: 0.000001 (0.000001)  time: 0.511931  data: 0.000123  max mem: 4023\n",
      "Epoch: [7/100]  [188/189]  eta: 0:00:00  loss: 6.461685 (6.461708)  lr: 0.000397 (0.000375)  wd: 0.000001 (0.000001)  time: 0.504369  data: 0.000071  max mem: 4023\n",
      "Epoch: [7/100] Total time: 0:01:40 (0.534002 s / it)\n",
      "Averaged stats: loss: 6.461685 (6.461708)  lr: 0.000397 (0.000375)  wd: 0.000001 (0.000001)\n",
      "Epoch: [8/100]  [  0/189]  eta: 0:10:43  loss: 6.461691 (6.461691)  lr: 0.000400 (0.000400)  wd: 0.000001 (0.000001)  time: 3.402998  data: 2.807149  max mem: 4023\n",
      "Epoch: [8/100]  [ 20/189]  eta: 0:01:58  loss: 6.461672 (6.461671)  lr: 0.000403 (0.000403)  wd: 0.000001 (0.000001)  time: 0.563714  data: 0.046288  max mem: 4023\n",
      "Epoch: [8/100]  [ 40/189]  eta: 0:01:30  loss: 6.461672 (6.461670)  lr: 0.000408 (0.000406)  wd: 0.000001 (0.000001)  time: 0.509165  data: 0.002031  max mem: 4023\n",
      "Epoch: [8/100]  [ 60/189]  eta: 0:01:14  loss: 6.461669 (6.461668)  lr: 0.000413 (0.000408)  wd: 0.000001 (0.000001)  time: 0.515209  data: 0.000347  max mem: 4023\n",
      "Epoch: [8/100]  [ 80/189]  eta: 0:01:01  loss: 6.461669 (6.461669)  lr: 0.000419 (0.000411)  wd: 0.000001 (0.000001)  time: 0.534457  data: 0.000503  max mem: 4023\n",
      "Epoch: [8/100]  [100/189]  eta: 0:00:49  loss: 6.461659 (6.461666)  lr: 0.000424 (0.000413)  wd: 0.000001 (0.000001)  time: 0.516846  data: 0.000264  max mem: 4023\n",
      "Epoch: [8/100]  [120/189]  eta: 0:00:37  loss: 6.461642 (6.461663)  lr: 0.000429 (0.000416)  wd: 0.000001 (0.000001)  time: 0.508801  data: 0.000159  max mem: 4023\n",
      "Epoch: [8/100]  [140/189]  eta: 0:00:26  loss: 6.461643 (6.461661)  lr: 0.000435 (0.000419)  wd: 0.000001 (0.000001)  time: 0.512003  data: 0.000257  max mem: 4023\n",
      "Epoch: [8/100]  [160/189]  eta: 0:00:15  loss: 6.461645 (6.461659)  lr: 0.000440 (0.000421)  wd: 0.000001 (0.000001)  time: 0.512463  data: 0.000499  max mem: 4023\n",
      "Epoch: [8/100]  [180/189]  eta: 0:00:04  loss: 6.461626 (6.461656)  lr: 0.000445 (0.000424)  wd: 0.000001 (0.000001)  time: 0.514040  data: 0.000176  max mem: 4023\n",
      "Epoch: [8/100]  [188/189]  eta: 0:00:00  loss: 6.461632 (6.461655)  lr: 0.000447 (0.000425)  wd: 0.000001 (0.000001)  time: 0.516704  data: 0.000079  max mem: 4023\n",
      "Epoch: [8/100] Total time: 0:01:41 (0.536272 s / it)\n",
      "Averaged stats: loss: 6.461632 (6.461655)  lr: 0.000447 (0.000425)  wd: 0.000001 (0.000001)\n",
      "Epoch: [9/100]  [  0/189]  eta: 0:11:33  loss: 6.461617 (6.461617)  lr: 0.000450 (0.000450)  wd: 0.000001 (0.000001)  time: 3.669537  data: 3.022094  max mem: 4023\n",
      "Epoch: [9/100]  [ 20/189]  eta: 0:01:53  loss: 6.461646 (6.461645)  lr: 0.000453 (0.000453)  wd: 0.000001 (0.000001)  time: 0.524722  data: 0.000397  max mem: 4023\n",
      "Epoch: [9/100]  [ 40/189]  eta: 0:01:28  loss: 6.461634 (6.461640)  lr: 0.000458 (0.000456)  wd: 0.000001 (0.000001)  time: 0.513269  data: 0.000827  max mem: 4023\n",
      "Epoch: [9/100]  [ 60/189]  eta: 0:01:13  loss: 6.461619 (6.461634)  lr: 0.000463 (0.000458)  wd: 0.000001 (0.000001)  time: 0.511181  data: 0.000337  max mem: 4023\n",
      "Epoch: [9/100]  [ 80/189]  eta: 0:01:00  loss: 6.461639 (6.461635)  lr: 0.000469 (0.000461)  wd: 0.000001 (0.000001)  time: 0.514477  data: 0.000155  max mem: 4023\n",
      "Epoch: [9/100]  [100/189]  eta: 0:00:48  loss: 6.461635 (6.461636)  lr: 0.000474 (0.000463)  wd: 0.000001 (0.000001)  time: 0.531096  data: 0.000153  max mem: 4023\n",
      "Epoch: [9/100]  [120/189]  eta: 0:00:37  loss: 6.461638 (6.461637)  lr: 0.000479 (0.000466)  wd: 0.000001 (0.000001)  time: 0.519084  data: 0.000138  max mem: 4023\n",
      "Epoch: [9/100]  [140/189]  eta: 0:00:26  loss: 6.461623 (6.461635)  lr: 0.000485 (0.000469)  wd: 0.000001 (0.000001)  time: 0.521675  data: 0.000621  max mem: 4023\n",
      "Epoch: [9/100]  [160/189]  eta: 0:00:15  loss: 6.461622 (6.461634)  lr: 0.000490 (0.000471)  wd: 0.000001 (0.000001)  time: 0.512130  data: 0.000401  max mem: 4023\n",
      "Epoch: [9/100]  [180/189]  eta: 0:00:04  loss: 6.461628 (6.461633)  lr: 0.000495 (0.000474)  wd: 0.000001 (0.000001)  time: 0.522814  data: 0.000834  max mem: 4023\n",
      "Epoch: [9/100]  [188/189]  eta: 0:00:00  loss: 6.461630 (6.461633)  lr: 0.000497 (0.000475)  wd: 0.000001 (0.000001)  time: 0.502212  data: 0.000133  max mem: 4023\n",
      "Epoch: [9/100] Total time: 0:01:41 (0.534762 s / it)\n",
      "Averaged stats: loss: 6.461630 (6.461633)  lr: 0.000497 (0.000475)  wd: 0.000001 (0.000001)\n",
      "Epoch: [10/100]  [  0/189]  eta: 0:13:12  loss: 6.461638 (6.461638)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 4.192966  data: 3.543404  max mem: 4023\n",
      "Epoch: [10/100]  [ 20/189]  eta: 0:01:58  loss: 6.461623 (6.461623)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.524359  data: 0.000177  max mem: 4023\n",
      "Epoch: [10/100]  [ 40/189]  eta: 0:01:30  loss: 6.461611 (6.461617)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.515904  data: 0.001754  max mem: 4023\n",
      "Epoch: [10/100]  [ 60/189]  eta: 0:01:14  loss: 6.461609 (6.461615)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.521250  data: 0.000265  max mem: 4023\n",
      "Epoch: [10/100]  [ 80/189]  eta: 0:01:01  loss: 6.461602 (6.461613)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.524762  data: 0.000166  max mem: 4023\n",
      "Epoch: [10/100]  [100/189]  eta: 0:00:49  loss: 6.461607 (6.461612)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.510214  data: 0.000161  max mem: 4023\n",
      "Epoch: [10/100]  [120/189]  eta: 0:00:37  loss: 6.461601 (6.461610)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.511434  data: 0.000173  max mem: 4023\n",
      "Epoch: [10/100]  [140/189]  eta: 0:00:26  loss: 6.461599 (6.461609)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.533721  data: 0.000374  max mem: 4023\n",
      "Epoch: [10/100]  [160/189]  eta: 0:00:15  loss: 6.461593 (6.461608)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.522793  data: 0.000188  max mem: 4023\n",
      "Epoch: [10/100]  [180/189]  eta: 0:00:04  loss: 6.461600 (6.461607)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.507853  data: 0.000121  max mem: 4023\n",
      "Epoch: [10/100]  [188/189]  eta: 0:00:00  loss: 6.461592 (6.461606)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.511291  data: 0.000078  max mem: 4023\n",
      "Epoch: [10/100] Total time: 0:01:41 (0.538949 s / it)\n",
      "Averaged stats: loss: 6.461592 (6.461606)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)\n",
      "Epoch: [11/100]  [  0/189]  eta: 0:14:50  loss: 6.461573 (6.461573)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 4.709631  data: 4.106738  max mem: 4023\n",
      "Epoch: [11/100]  [ 20/189]  eta: 0:02:01  loss: 6.461592 (6.461591)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.517359  data: 0.002673  max mem: 4023\n",
      "Epoch: [11/100]  [ 40/189]  eta: 0:01:31  loss: 6.461595 (6.461594)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.508605  data: 0.001756  max mem: 4023\n",
      "Epoch: [11/100]  [ 60/189]  eta: 0:01:14  loss: 6.461592 (6.461593)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.508067  data: 0.000160  max mem: 4023\n",
      "Epoch: [11/100]  [ 80/189]  eta: 0:01:01  loss: 6.461581 (6.461591)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.522505  data: 0.000399  max mem: 4023\n",
      "Epoch: [11/100]  [100/189]  eta: 0:00:49  loss: 6.461588 (6.461591)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.513390  data: 0.000370  max mem: 4023\n",
      "Epoch: [11/100]  [120/189]  eta: 0:00:37  loss: 6.461575 (6.461588)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.514927  data: 0.000180  max mem: 4023\n",
      "Epoch: [11/100]  [140/189]  eta: 0:00:26  loss: 6.461579 (6.461587)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.518840  data: 0.000253  max mem: 4023\n",
      "Epoch: [11/100]  [160/189]  eta: 0:00:15  loss: 6.461567 (6.461585)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.516303  data: 0.000159  max mem: 4023\n",
      "Epoch: [11/100]  [180/189]  eta: 0:00:04  loss: 6.461578 (6.461584)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.511397  data: 0.000322  max mem: 4023\n",
      "Epoch: [11/100]  [188/189]  eta: 0:00:00  loss: 6.461581 (6.461584)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.508788  data: 0.000087  max mem: 4023\n",
      "Epoch: [11/100] Total time: 0:01:41 (0.536819 s / it)\n",
      "Averaged stats: loss: 6.461581 (6.461584)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)\n",
      "Epoch: [12/100]  [  0/189]  eta: 0:12:00  loss: 6.461577 (6.461577)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 3.812675  data: 3.197405  max mem: 4023\n",
      "Epoch: [12/100]  [ 20/189]  eta: 0:01:54  loss: 6.461573 (6.461573)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.521464  data: 0.000650  max mem: 4023\n",
      "Epoch: [12/100]  [ 40/189]  eta: 0:01:29  loss: 6.461574 (6.461574)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.525327  data: 0.000799  max mem: 4023\n",
      "Epoch: [12/100]  [ 60/189]  eta: 0:01:14  loss: 6.461577 (6.461574)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.514808  data: 0.000365  max mem: 4023\n",
      "Epoch: [12/100]  [ 80/189]  eta: 0:01:00  loss: 6.461570 (6.461572)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.509905  data: 0.000167  max mem: 4023\n",
      "Epoch: [12/100]  [100/189]  eta: 0:00:48  loss: 6.461576 (6.461573)  lr: 0.000500 (0.000500)  wd: 0.000001 (0.000001)  time: 0.513321  data: 0.000157  max mem: 4023\n",
      "Epoch: [12/100]  [120/189]  eta: 0:00:37  loss: 6.461575 (6.461574)  lr: 0.000499 (0.000500)  wd: 0.000001 (0.000001)  time: 0.517559  data: 0.000210  max mem: 4023\n",
      "Epoch: [12/100]  [140/189]  eta: 0:00:26  loss: 6.461565 (6.461573)  lr: 0.000499 (0.000500)  wd: 0.000001 (0.000001)  time: 0.523095  data: 0.000170  max mem: 4023\n",
      "Epoch: [12/100]  [160/189]  eta: 0:00:15  loss: 6.461568 (6.461573)  lr: 0.000499 (0.000500)  wd: 0.000001 (0.000001)  time: 0.525815  data: 0.000442  max mem: 4023\n",
      "Epoch: [12/100]  [180/189]  eta: 0:00:04  loss: 6.461573 (6.461573)  lr: 0.000499 (0.000500)  wd: 0.000001 (0.000001)  time: 0.508555  data: 0.000358  max mem: 4023\n",
      "Epoch: [12/100]  [188/189]  eta: 0:00:00  loss: 6.461573 (6.461573)  lr: 0.000499 (0.000500)  wd: 0.000001 (0.000001)  time: 0.507686  data: 0.000099  max mem: 4023\n",
      "Epoch: [12/100] Total time: 0:01:41 (0.535261 s / it)\n",
      "Averaged stats: loss: 6.461573 (6.461573)  lr: 0.000499 (0.000500)  wd: 0.000001 (0.000001)\n",
      "Epoch: [13/100]  [  0/189]  eta: 0:12:12  loss: 6.461571 (6.461571)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 3.875994  data: 3.307336  max mem: 4023\n",
      "Epoch: [13/100]  [ 20/189]  eta: 0:01:54  loss: 6.461564 (6.461567)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.519640  data: 0.000782  max mem: 4023\n",
      "Epoch: [13/100]  [ 40/189]  eta: 0:01:29  loss: 6.461562 (6.461564)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.521191  data: 0.000973  max mem: 4023\n",
      "Epoch: [13/100]  [ 60/189]  eta: 0:01:14  loss: 6.461563 (6.461563)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.516907  data: 0.000339  max mem: 4023\n",
      "Epoch: [13/100]  [ 80/189]  eta: 0:01:01  loss: 6.461555 (6.461562)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.522255  data: 0.000222  max mem: 4023\n",
      "Epoch: [13/100]  [100/189]  eta: 0:00:49  loss: 6.461559 (6.461561)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.519790  data: 0.000197  max mem: 4023\n",
      "Epoch: [13/100]  [120/189]  eta: 0:00:37  loss: 6.461556 (6.461561)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.512148  data: 0.000154  max mem: 4023\n",
      "Epoch: [13/100]  [140/189]  eta: 0:00:26  loss: 6.461555 (6.461560)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.522717  data: 0.000161  max mem: 4023\n",
      "Epoch: [13/100]  [160/189]  eta: 0:00:15  loss: 6.461554 (6.461560)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.532572  data: 0.000297  max mem: 4023\n",
      "Epoch: [13/100]  [180/189]  eta: 0:00:04  loss: 6.461742 (6.461643)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.518112  data: 0.000160  max mem: 4023\n",
      "Epoch: [13/100]  [188/189]  eta: 0:00:00  loss: 6.462054 (6.461760)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.507691  data: 0.000121  max mem: 4023\n",
      "Epoch: [13/100] Total time: 0:01:41 (0.537363 s / it)\n",
      "Averaged stats: loss: 6.462054 (6.461760)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)\n",
      "Epoch: [14/100]  [  0/189]  eta: 0:11:00  loss: 6.463547 (6.463547)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 3.493856  data: 2.916061  max mem: 4023\n",
      "Epoch: [14/100]  [ 20/189]  eta: 0:01:52  loss: 6.461882 (6.462234)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.525069  data: 0.000867  max mem: 4023\n",
      "Epoch: [14/100]  [ 40/189]  eta: 0:01:29  loss: 6.461662 (6.462097)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.528048  data: 0.000923  max mem: 4023\n",
      "Epoch: [14/100]  [ 60/189]  eta: 0:01:14  loss: 6.467179 (6.464388)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.522121  data: 0.000140  max mem: 4023\n",
      "Epoch: [14/100]  [ 80/189]  eta: 0:01:00  loss: 6.465084 (6.464704)  lr: 0.000499 (0.000499)  wd: 0.000001 (0.000001)  time: 0.512085  data: 0.000156  max mem: 4023\n",
      "Epoch: [14/100]  [100/189]  eta: 0:00:48  loss: 6.468699 (6.466032)  lr: 0.000498 (0.000499)  wd: 0.000001 (0.000001)  time: 0.511674  data: 0.000153  max mem: 4023\n",
      "Epoch: [14/100]  [120/189]  eta: 0:00:37  loss: 6.463671 (6.465811)  lr: 0.000498 (0.000499)  wd: 0.000001 (0.000001)  time: 0.517993  data: 0.000143  max mem: 4023\n",
      "Epoch: [14/100]  [140/189]  eta: 0:00:26  loss: 6.462728 (6.465405)  lr: 0.000498 (0.000499)  wd: 0.000001 (0.000001)  time: 0.513291  data: 0.000338  max mem: 4023\n",
      "Epoch: [14/100]  [160/189]  eta: 0:00:15  loss: 6.461962 (6.465055)  lr: 0.000498 (0.000499)  wd: 0.000001 (0.000001)  time: 0.532454  data: 0.000135  max mem: 4023\n",
      "Epoch: [14/100]  [180/189]  eta: 0:00:04  loss: 6.462271 (6.464791)  lr: 0.000498 (0.000498)  wd: 0.000001 (0.000001)  time: 0.517156  data: 0.000143  max mem: 4023\n",
      "Epoch: [14/100]  [188/189]  eta: 0:00:00  loss: 6.462044 (6.464669)  lr: 0.000498 (0.000498)  wd: 0.000001 (0.000001)  time: 0.508209  data: 0.000072  max mem: 4023\n",
      "Epoch: [14/100] Total time: 0:01:41 (0.535982 s / it)\n",
      "Averaged stats: loss: 6.462044 (6.464669)  lr: 0.000498 (0.000498)  wd: 0.000001 (0.000001)\n",
      "Epoch: [15/100]  [  0/189]  eta: 0:12:38  loss: 6.461718 (6.461718)  lr: 0.000498 (0.000498)  wd: 0.000001 (0.000001)  time: 4.013865  data: 3.426380  max mem: 4023\n",
      "Epoch: [15/100]  [ 20/189]  eta: 0:01:55  loss: 6.461641 (6.461677)  lr: 0.000498 (0.000498)  wd: 0.000001 (0.000001)  time: 0.517964  data: 0.000595  max mem: 4023\n",
      "Epoch: [15/100]  [ 40/189]  eta: 0:01:29  loss: 6.461600 (6.461647)  lr: 0.000498 (0.000498)  wd: 0.000002 (0.000001)  time: 0.507464  data: 0.000190  max mem: 4023\n",
      "Epoch: [15/100]  [ 60/189]  eta: 0:01:14  loss: 6.461665 (6.461847)  lr: 0.000498 (0.000498)  wd: 0.000002 (0.000002)  time: 0.523984  data: 0.000164  max mem: 4023\n",
      "Epoch: [15/100]  [ 80/189]  eta: 0:01:01  loss: 6.461865 (6.461887)  lr: 0.000498 (0.000498)  wd: 0.000002 (0.000002)  time: 0.519902  data: 0.000279  max mem: 4023\n",
      "Epoch: [15/100]  [100/189]  eta: 0:00:49  loss: 6.461622 (6.461837)  lr: 0.000498 (0.000498)  wd: 0.000002 (0.000002)  time: 0.517890  data: 0.000467  max mem: 4023\n",
      "Epoch: [15/100]  [120/189]  eta: 0:00:37  loss: 6.461567 (6.461809)  lr: 0.000498 (0.000498)  wd: 0.000002 (0.000002)  time: 0.516316  data: 0.000139  max mem: 4023\n",
      "Epoch: [15/100]  [140/189]  eta: 0:00:26  loss: 6.461531 (6.461784)  lr: 0.000498 (0.000498)  wd: 0.000002 (0.000002)  time: 0.513147  data: 0.000335  max mem: 4023\n",
      "Epoch: [15/100]  [160/189]  eta: 0:00:15  loss: 6.461524 (6.461752)  lr: 0.000497 (0.000498)  wd: 0.000002 (0.000002)  time: 0.511306  data: 0.000334  max mem: 4023\n",
      "Epoch: [15/100]  [180/189]  eta: 0:00:04  loss: 6.461532 (6.461732)  lr: 0.000497 (0.000498)  wd: 0.000002 (0.000002)  time: 0.530994  data: 0.000269  max mem: 4023\n",
      "Epoch: [15/100]  [188/189]  eta: 0:00:00  loss: 6.461529 (6.461723)  lr: 0.000497 (0.000498)  wd: 0.000002 (0.000002)  time: 0.530116  data: 0.000082  max mem: 4023\n",
      "Epoch: [15/100] Total time: 0:01:41 (0.536366 s / it)\n",
      "Averaged stats: loss: 6.461529 (6.461723)  lr: 0.000497 (0.000498)  wd: 0.000002 (0.000002)\n",
      "Epoch: [16/100]  [  0/189]  eta: 0:13:42  loss: 6.461523 (6.461523)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 4.352247  data: 3.731004  max mem: 4023\n",
      "Epoch: [16/100]  [ 20/189]  eta: 0:01:59  loss: 6.461518 (6.461523)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 0.522712  data: 0.001607  max mem: 4023\n",
      "Epoch: [16/100]  [ 40/189]  eta: 0:01:31  loss: 6.461538 (6.461846)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 0.522847  data: 0.001315  max mem: 4023\n",
      "Epoch: [16/100]  [ 60/189]  eta: 0:01:15  loss: 6.461971 (6.461912)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 0.530401  data: 0.000351  max mem: 4023\n",
      "Epoch: [16/100]  [ 80/189]  eta: 0:01:02  loss: 6.461635 (6.461876)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 0.522135  data: 0.000345  max mem: 4023\n",
      "Epoch: [16/100]  [100/189]  eta: 0:00:50  loss: 6.461527 (6.461845)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 0.521939  data: 0.000830  max mem: 4023\n",
      "Epoch: [16/100]  [120/189]  eta: 0:00:38  loss: 6.461521 (6.461820)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 0.525865  data: 0.000227  max mem: 4023\n",
      "Epoch: [16/100]  [140/189]  eta: 0:00:26  loss: 6.461521 (6.461782)  lr: 0.000497 (0.000497)  wd: 0.000002 (0.000002)  time: 0.519744  data: 0.000150  max mem: 4023\n",
      "Epoch: [16/100]  [160/189]  eta: 0:00:15  loss: 6.461515 (6.461761)  lr: 0.000496 (0.000497)  wd: 0.000002 (0.000002)  time: 0.533277  data: 0.001016  max mem: 4023\n",
      "Epoch: [16/100]  [180/189]  eta: 0:00:04  loss: 6.461535 (6.461747)  lr: 0.000496 (0.000497)  wd: 0.000002 (0.000002)  time: 0.522478  data: 0.000119  max mem: 4023\n",
      "Epoch: [16/100]  [188/189]  eta: 0:00:00  loss: 6.461533 (6.461742)  lr: 0.000496 (0.000497)  wd: 0.000002 (0.000002)  time: 0.506594  data: 0.000082  max mem: 4023\n",
      "Epoch: [16/100] Total time: 0:01:42 (0.544001 s / it)\n",
      "Averaged stats: loss: 6.461533 (6.461742)  lr: 0.000496 (0.000497)  wd: 0.000002 (0.000002)\n",
      "Epoch: [17/100]  [  0/189]  eta: 0:11:55  loss: 6.461552 (6.461552)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 3.788276  data: 3.142667  max mem: 4023\n",
      "Epoch: [17/100]  [ 20/189]  eta: 0:01:54  loss: 6.461535 (6.461580)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 0.523827  data: 0.000709  max mem: 4023\n",
      "Epoch: [17/100]  [ 40/189]  eta: 0:01:29  loss: 6.461545 (6.461625)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 0.523973  data: 0.000184  max mem: 4023\n",
      "Epoch: [17/100]  [ 60/189]  eta: 0:01:14  loss: 6.461532 (6.461598)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 0.518908  data: 0.000209  max mem: 4023\n",
      "Epoch: [17/100]  [ 80/189]  eta: 0:01:01  loss: 6.461532 (6.461586)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 0.531516  data: 0.002171  max mem: 4023\n",
      "Epoch: [17/100]  [100/189]  eta: 0:00:49  loss: 6.461535 (6.461579)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 0.524392  data: 0.000378  max mem: 4023\n",
      "Epoch: [17/100]  [120/189]  eta: 0:00:37  loss: 6.461530 (6.461580)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 0.509870  data: 0.000149  max mem: 4023\n",
      "Epoch: [17/100]  [140/189]  eta: 0:00:26  loss: 6.461546 (6.461583)  lr: 0.000496 (0.000496)  wd: 0.000002 (0.000002)  time: 0.514825  data: 0.000142  max mem: 4023\n",
      "Epoch: [17/100]  [160/189]  eta: 0:00:15  loss: 6.461534 (6.461578)  lr: 0.000495 (0.000496)  wd: 0.000002 (0.000002)  time: 0.514511  data: 0.000123  max mem: 4023\n",
      "Epoch: [17/100]  [180/189]  eta: 0:00:04  loss: 6.461551 (6.461590)  lr: 0.000495 (0.000496)  wd: 0.000002 (0.000002)  time: 0.517895  data: 0.001484  max mem: 4023\n",
      "Epoch: [17/100]  [188/189]  eta: 0:00:00  loss: 6.461716 (6.461601)  lr: 0.000495 (0.000496)  wd: 0.000002 (0.000002)  time: 0.505098  data: 0.000073  max mem: 4023\n",
      "Epoch: [17/100] Total time: 0:01:41 (0.536914 s / it)\n",
      "Averaged stats: loss: 6.461716 (6.461601)  lr: 0.000495 (0.000496)  wd: 0.000002 (0.000002)\n",
      "Epoch: [18/100]  [  0/189]  eta: 0:11:48  loss: 6.461642 (6.461642)  lr: 0.000495 (0.000495)  wd: 0.000002 (0.000002)  time: 3.748459  data: 3.080712  max mem: 4023\n",
      "Epoch: [18/100]  [ 20/189]  eta: 0:01:57  loss: 6.461585 (6.461608)  lr: 0.000495 (0.000495)  wd: 0.000002 (0.000002)  time: 0.540694  data: 0.021021  max mem: 4023\n",
      "Epoch: [18/100]  [ 40/189]  eta: 0:01:30  loss: 6.461615 (6.461635)  lr: 0.000495 (0.000495)  wd: 0.000002 (0.000002)  time: 0.517766  data: 0.000348  max mem: 4023\n",
      "Epoch: [18/100]  [ 60/189]  eta: 0:01:14  loss: 6.461913 (6.461838)  lr: 0.000495 (0.000495)  wd: 0.000002 (0.000002)  time: 0.513838  data: 0.000301  max mem: 4023\n",
      "Epoch: [18/100]  [ 80/189]  eta: 0:01:01  loss: 6.462152 (6.461965)  lr: 0.000495 (0.000495)  wd: 0.000002 (0.000002)  time: 0.514576  data: 0.000377  max mem: 4023\n",
      "Epoch: [18/100]  [100/189]  eta: 0:00:49  loss: 6.461648 (6.461909)  lr: 0.000495 (0.000495)  wd: 0.000002 (0.000002)  time: 0.517172  data: 0.000673  max mem: 4023\n",
      "Epoch: [18/100]  [120/189]  eta: 0:00:37  loss: 6.461783 (6.461899)  lr: 0.000494 (0.000495)  wd: 0.000002 (0.000002)  time: 0.519252  data: 0.000139  max mem: 4023\n",
      "Epoch: [18/100]  [140/189]  eta: 0:00:26  loss: 6.461705 (6.461874)  lr: 0.000494 (0.000495)  wd: 0.000002 (0.000002)  time: 0.537641  data: 0.000400  max mem: 4023\n",
      "Epoch: [18/100]  [160/189]  eta: 0:00:15  loss: 6.461685 (6.461854)  lr: 0.000494 (0.000495)  wd: 0.000002 (0.000002)  time: 0.531050  data: 0.000155  max mem: 4023\n",
      "Epoch: [18/100]  [180/189]  eta: 0:00:04  loss: 6.461597 (6.461826)  lr: 0.000494 (0.000495)  wd: 0.000002 (0.000002)  time: 0.505899  data: 0.000118  max mem: 4023\n",
      "Epoch: [18/100]  [188/189]  eta: 0:00:00  loss: 6.461551 (6.461814)  lr: 0.000494 (0.000495)  wd: 0.000002 (0.000002)  time: 0.501566  data: 0.000073  max mem: 4023\n",
      "Epoch: [18/100] Total time: 0:01:41 (0.538578 s / it)\n",
      "Averaged stats: loss: 6.461551 (6.461814)  lr: 0.000494 (0.000495)  wd: 0.000002 (0.000002)\n",
      "Epoch: [19/100]  [  0/189]  eta: 0:14:07  loss: 6.461508 (6.461508)  lr: 0.000494 (0.000494)  wd: 0.000002 (0.000002)  time: 4.482836  data: 3.878571  max mem: 4023\n",
      "Epoch: [19/100]  [ 20/189]  eta: 0:02:00  loss: 6.461533 (6.461551)  lr: 0.000494 (0.000494)  wd: 0.000002 (0.000002)  time: 0.524572  data: 0.001180  max mem: 4023\n",
      "Epoch: [19/100]  [ 40/189]  eta: 0:01:31  loss: 6.461550 (6.461603)  lr: 0.000494 (0.000494)  wd: 0.000002 (0.000002)  time: 0.516214  data: 0.000817  max mem: 4023\n",
      "Epoch: [19/100]  [ 60/189]  eta: 0:01:15  loss: 6.461642 (6.461637)  lr: 0.000494 (0.000494)  wd: 0.000002 (0.000002)  time: 0.527877  data: 0.000177  max mem: 4023\n",
      "Epoch: [19/100]  [ 80/189]  eta: 0:01:02  loss: 6.461550 (6.461616)  lr: 0.000493 (0.000494)  wd: 0.000002 (0.000002)  time: 0.516679  data: 0.000140  max mem: 4023\n",
      "Epoch: [19/100]  [100/189]  eta: 0:00:49  loss: 6.461681 (6.461663)  lr: 0.000493 (0.000494)  wd: 0.000002 (0.000002)  time: 0.513286  data: 0.000216  max mem: 4023\n",
      "Epoch: [19/100]  [120/189]  eta: 0:00:38  loss: 6.461638 (6.461666)  lr: 0.000493 (0.000493)  wd: 0.000002 (0.000002)  time: 0.510802  data: 0.000326  max mem: 4023\n",
      "Epoch: [19/100]  [140/189]  eta: 0:00:26  loss: 6.461535 (6.461648)  lr: 0.000493 (0.000493)  wd: 0.000002 (0.000002)  time: 0.527576  data: 0.000559  max mem: 4023\n",
      "Epoch: [19/100]  [160/189]  eta: 0:00:15  loss: 6.461624 (6.461649)  lr: 0.000493 (0.000493)  wd: 0.000002 (0.000002)  time: 0.517995  data: 0.000473  max mem: 4023\n",
      "Epoch: [19/100]  [180/189]  eta: 0:00:04  loss: 6.461562 (6.461642)  lr: 0.000493 (0.000493)  wd: 0.000002 (0.000002)  time: 0.526347  data: 0.000121  max mem: 4023\n",
      "Epoch: [19/100]  [188/189]  eta: 0:00:00  loss: 6.461533 (6.461637)  lr: 0.000493 (0.000493)  wd: 0.000002 (0.000002)  time: 0.518658  data: 0.000075  max mem: 4023\n",
      "Epoch: [19/100] Total time: 0:01:42 (0.540922 s / it)\n",
      "Averaged stats: loss: 6.461533 (6.461637)  lr: 0.000493 (0.000493)  wd: 0.000002 (0.000002)\n",
      "Epoch: [20/100]  [  0/189]  eta: 0:12:54  loss: 6.461508 (6.461508)  lr: 0.000492 (0.000492)  wd: 0.000002 (0.000002)  time: 4.096129  data: 3.438677  max mem: 4023\n",
      "Epoch: [20/100]  [ 20/189]  eta: 0:01:56  loss: 6.461521 (6.461523)  lr: 0.000492 (0.000492)  wd: 0.000002 (0.000002)  time: 0.520384  data: 0.000847  max mem: 4023\n",
      "Epoch: [20/100]  [ 40/189]  eta: 0:01:30  loss: 6.461507 (6.461518)  lr: 0.000492 (0.000492)  wd: 0.000002 (0.000002)  time: 0.522885  data: 0.000170  max mem: 4023\n",
      "Epoch: [20/100]  [ 60/189]  eta: 0:01:14  loss: 6.461502 (6.461514)  lr: 0.000492 (0.000492)  wd: 0.000002 (0.000002)  time: 0.515997  data: 0.000158  max mem: 4023\n",
      "Epoch: [20/100]  [ 80/189]  eta: 0:01:01  loss: 6.461504 (6.461513)  lr: 0.000492 (0.000492)  wd: 0.000002 (0.000002)  time: 0.516247  data: 0.000138  max mem: 4023\n",
      "Epoch: [20/100]  [100/189]  eta: 0:00:49  loss: 6.461511 (6.461514)  lr: 0.000492 (0.000492)  wd: 0.000002 (0.000002)  time: 0.514707  data: 0.000144  max mem: 4023\n",
      "Epoch: [20/100]  [120/189]  eta: 0:00:37  loss: 6.461499 (6.461513)  lr: 0.000492 (0.000492)  wd: 0.000002 (0.000002)  time: 0.521653  data: 0.000164  max mem: 4023\n",
      "Epoch: [20/100]  [140/189]  eta: 0:00:26  loss: 6.461498 (6.461512)  lr: 0.000491 (0.000492)  wd: 0.000002 (0.000002)  time: 0.519971  data: 0.000149  max mem: 4023\n",
      "Epoch: [20/100]  [160/189]  eta: 0:00:15  loss: 6.461496 (6.461510)  lr: 0.000491 (0.000492)  wd: 0.000002 (0.000002)  time: 0.525165  data: 0.000135  max mem: 4023\n",
      "Epoch: [20/100]  [180/189]  eta: 0:00:04  loss: 6.461495 (6.461509)  lr: 0.000491 (0.000492)  wd: 0.000002 (0.000002)  time: 0.517411  data: 0.000377  max mem: 4023\n",
      "Epoch: [20/100]  [188/189]  eta: 0:00:00  loss: 6.461499 (6.461508)  lr: 0.000491 (0.000492)  wd: 0.000002 (0.000002)  time: 0.512150  data: 0.000139  max mem: 4023\n",
      "Epoch: [20/100] Total time: 0:01:41 (0.538511 s / it)\n",
      "Averaged stats: loss: 6.461499 (6.461508)  lr: 0.000491 (0.000492)  wd: 0.000002 (0.000002)\n",
      "Epoch: [21/100]  [  0/189]  eta: 0:12:26  loss: 6.461518 (6.461518)  lr: 0.000491 (0.000491)  wd: 0.000002 (0.000002)  time: 3.952325  data: 3.329189  max mem: 4023\n",
      "Epoch: [21/100]  [ 20/189]  eta: 0:01:56  loss: 6.461504 (6.461507)  lr: 0.000491 (0.000491)  wd: 0.000002 (0.000002)  time: 0.527557  data: 0.000704  max mem: 4023\n",
      "Epoch: [21/100]  [ 40/189]  eta: 0:01:30  loss: 6.461501 (6.461511)  lr: 0.000491 (0.000491)  wd: 0.000002 (0.000002)  time: 0.520729  data: 0.002001  max mem: 4023\n",
      "Epoch: [21/100]  [ 60/189]  eta: 0:01:14  loss: 6.461619 (6.461568)  lr: 0.000490 (0.000491)  wd: 0.000002 (0.000002)  time: 0.515252  data: 0.000149  max mem: 4023\n",
      "Epoch: [21/100]  [ 80/189]  eta: 0:01:01  loss: 6.461524 (6.461558)  lr: 0.000490 (0.000491)  wd: 0.000002 (0.000002)  time: 0.524571  data: 0.000333  max mem: 4023\n",
      "Epoch: [21/100]  [100/189]  eta: 0:00:49  loss: 6.461504 (6.461548)  lr: 0.000490 (0.000490)  wd: 0.000002 (0.000002)  time: 0.516891  data: 0.000138  max mem: 4023\n",
      "Epoch: [21/100]  [120/189]  eta: 0:00:37  loss: 6.461521 (6.461544)  lr: 0.000490 (0.000490)  wd: 0.000002 (0.000002)  time: 0.526391  data: 0.000578  max mem: 4023\n",
      "Epoch: [21/100]  [140/189]  eta: 0:00:26  loss: 6.461504 (6.461539)  lr: 0.000490 (0.000490)  wd: 0.000002 (0.000002)  time: 0.532253  data: 0.000124  max mem: 4023\n",
      "Epoch: [21/100]  [160/189]  eta: 0:00:15  loss: 6.461498 (6.461534)  lr: 0.000490 (0.000490)  wd: 0.000002 (0.000002)  time: 0.514986  data: 0.000126  max mem: 4023\n",
      "Epoch: [21/100]  [180/189]  eta: 0:00:04  loss: 6.461497 (6.461530)  lr: 0.000489 (0.000490)  wd: 0.000002 (0.000002)  time: 0.516134  data: 0.000112  max mem: 4023\n",
      "Epoch: [21/100]  [188/189]  eta: 0:00:00  loss: 6.461497 (6.461534)  lr: 0.000489 (0.000490)  wd: 0.000002 (0.000002)  time: 0.498922  data: 0.000069  max mem: 4023\n",
      "Epoch: [21/100] Total time: 0:01:41 (0.539047 s / it)\n",
      "Averaged stats: loss: 6.461497 (6.461534)  lr: 0.000489 (0.000490)  wd: 0.000002 (0.000002)\n",
      "Epoch: [22/100]  [  0/189]  eta: 0:12:59  loss: 6.461962 (6.461962)  lr: 0.000489 (0.000489)  wd: 0.000002 (0.000002)  time: 4.122652  data: 3.520497  max mem: 4023\n",
      "Epoch: [22/100]  [ 20/189]  eta: 0:01:56  loss: 6.461561 (6.461625)  lr: 0.000489 (0.000489)  wd: 0.000002 (0.000002)  time: 0.514701  data: 0.001461  max mem: 4023\n",
      "Epoch: [22/100]  [ 40/189]  eta: 0:01:29  loss: 6.461515 (6.461574)  lr: 0.000489 (0.000489)  wd: 0.000002 (0.000002)  time: 0.515041  data: 0.000831  max mem: 4023\n",
      "Epoch: [22/100]  [ 60/189]  eta: 0:01:14  loss: 6.461499 (6.461553)  lr: 0.000489 (0.000489)  wd: 0.000002 (0.000002)  time: 0.518779  data: 0.000146  max mem: 4023\n",
      "Epoch: [22/100]  [ 80/189]  eta: 0:01:01  loss: 6.461578 (6.461570)  lr: 0.000489 (0.000489)  wd: 0.000002 (0.000002)  time: 0.521491  data: 0.000184  max mem: 4023\n",
      "Epoch: [22/100]  [100/189]  eta: 0:00:49  loss: 6.461512 (6.461560)  lr: 0.000488 (0.000489)  wd: 0.000002 (0.000002)  time: 0.522358  data: 0.000225  max mem: 4023\n",
      "Epoch: [22/100]  [120/189]  eta: 0:00:37  loss: 6.461496 (6.461550)  lr: 0.000488 (0.000489)  wd: 0.000002 (0.000002)  time: 0.517271  data: 0.000158  max mem: 4023\n",
      "Epoch: [22/100]  [140/189]  eta: 0:00:26  loss: 6.461492 (6.461542)  lr: 0.000488 (0.000489)  wd: 0.000002 (0.000002)  time: 0.526245  data: 0.000149  max mem: 4023\n",
      "Epoch: [22/100]  [160/189]  eta: 0:00:15  loss: 6.461493 (6.461537)  lr: 0.000488 (0.000488)  wd: 0.000002 (0.000002)  time: 0.519628  data: 0.000654  max mem: 4023\n",
      "Epoch: [22/100]  [180/189]  eta: 0:00:04  loss: 6.461501 (6.461533)  lr: 0.000488 (0.000488)  wd: 0.000002 (0.000002)  time: 0.524845  data: 0.000121  max mem: 4023\n",
      "Epoch: [22/100]  [188/189]  eta: 0:00:00  loss: 6.461501 (6.461532)  lr: 0.000487 (0.000488)  wd: 0.000002 (0.000002)  time: 0.515562  data: 0.000084  max mem: 4023\n",
      "Epoch: [22/100] Total time: 0:01:41 (0.539581 s / it)\n",
      "Averaged stats: loss: 6.461501 (6.461532)  lr: 0.000487 (0.000488)  wd: 0.000002 (0.000002)\n",
      "Epoch: [23/100]  [  0/189]  eta: 0:13:20  loss: 6.461492 (6.461492)  lr: 0.000487 (0.000487)  wd: 0.000002 (0.000002)  time: 4.235882  data: 3.667344  max mem: 4023\n",
      "Epoch: [23/100]  [ 20/189]  eta: 0:01:59  loss: 6.461492 (6.461492)  lr: 0.000487 (0.000487)  wd: 0.000002 (0.000002)  time: 0.528130  data: 0.000524  max mem: 4023\n",
      "Epoch: [23/100]  [ 40/189]  eta: 0:01:31  loss: 6.461493 (6.461494)  lr: 0.000487 (0.000487)  wd: 0.000002 (0.000002)  time: 0.515622  data: 0.001567  max mem: 4023\n",
      "Epoch: [23/100]  [ 60/189]  eta: 0:01:14  loss: 6.461499 (6.461497)  lr: 0.000487 (0.000487)  wd: 0.000002 (0.000002)  time: 0.516318  data: 0.000219  max mem: 4023\n",
      "Epoch: [23/100]  [ 80/189]  eta: 0:01:01  loss: 6.461498 (6.461498)  lr: 0.000487 (0.000487)  wd: 0.000002 (0.000002)  time: 0.509358  data: 0.000139  max mem: 4023\n",
      "Epoch: [23/100]  [100/189]  eta: 0:00:49  loss: 6.461496 (6.461498)  lr: 0.000486 (0.000487)  wd: 0.000002 (0.000002)  time: 0.515283  data: 0.000136  max mem: 4023\n",
      "Epoch: [23/100]  [120/189]  eta: 0:00:37  loss: 6.461491 (6.461497)  lr: 0.000486 (0.000487)  wd: 0.000002 (0.000002)  time: 0.520768  data: 0.000146  max mem: 4023\n",
      "Epoch: [23/100]  [140/189]  eta: 0:00:26  loss: 6.461493 (6.461497)  lr: 0.000486 (0.000487)  wd: 0.000002 (0.000002)  time: 0.512606  data: 0.000153  max mem: 4023\n",
      "Epoch: [23/100]  [160/189]  eta: 0:00:15  loss: 6.461493 (6.461497)  lr: 0.000486 (0.000487)  wd: 0.000002 (0.000002)  time: 0.532221  data: 0.000155  max mem: 4023\n",
      "Epoch: [23/100]  [180/189]  eta: 0:00:04  loss: 6.461490 (6.461496)  lr: 0.000486 (0.000486)  wd: 0.000002 (0.000002)  time: 0.524719  data: 0.000114  max mem: 4023\n",
      "Epoch: [23/100]  [188/189]  eta: 0:00:00  loss: 6.461490 (6.461496)  lr: 0.000485 (0.000486)  wd: 0.000002 (0.000002)  time: 0.505340  data: 0.000073  max mem: 4023\n",
      "Epoch: [23/100] Total time: 0:01:41 (0.538807 s / it)\n",
      "Averaged stats: loss: 6.461490 (6.461496)  lr: 0.000485 (0.000486)  wd: 0.000002 (0.000002)\n",
      "Epoch: [24/100]  [  0/189]  eta: 0:10:26  loss: 6.461507 (6.461507)  lr: 0.000485 (0.000485)  wd: 0.000002 (0.000002)  time: 3.313240  data: 2.707273  max mem: 4023\n",
      "Epoch: [24/100]  [ 20/189]  eta: 0:01:55  loss: 6.461494 (6.461496)  lr: 0.000485 (0.000485)  wd: 0.000002 (0.000002)  time: 0.553737  data: 0.031029  max mem: 4023\n",
      "Epoch: [24/100]  [ 40/189]  eta: 0:01:30  loss: 6.461496 (6.461504)  lr: 0.000485 (0.000485)  wd: 0.000002 (0.000002)  time: 0.520847  data: 0.000921  max mem: 4023\n",
      "Epoch: [24/100]  [ 60/189]  eta: 0:01:14  loss: 6.461501 (6.461508)  lr: 0.000485 (0.000485)  wd: 0.000002 (0.000002)  time: 0.526695  data: 0.000178  max mem: 4023\n",
      "Epoch: [24/100]  [ 80/189]  eta: 0:01:01  loss: 6.461493 (6.461508)  lr: 0.000485 (0.000485)  wd: 0.000002 (0.000002)  time: 0.515261  data: 0.000426  max mem: 4023\n",
      "Epoch: [24/100]  [100/189]  eta: 0:00:49  loss: 6.461498 (6.461507)  lr: 0.000484 (0.000485)  wd: 0.000002 (0.000002)  time: 0.529712  data: 0.000151  max mem: 4023\n",
      "Epoch: [24/100]  [120/189]  eta: 0:00:38  loss: 6.461502 (6.461507)  lr: 0.000484 (0.000485)  wd: 0.000002 (0.000002)  time: 0.538334  data: 0.000135  max mem: 4023\n",
      "Epoch: [24/100]  [140/189]  eta: 0:00:26  loss: 6.461501 (6.461507)  lr: 0.000484 (0.000485)  wd: 0.000002 (0.000002)  time: 0.524084  data: 0.000460  max mem: 4023\n",
      "Epoch: [24/100]  [160/189]  eta: 0:00:15  loss: 6.461496 (6.461506)  lr: 0.000484 (0.000484)  wd: 0.000002 (0.000002)  time: 0.536623  data: 0.000143  max mem: 4023\n",
      "Epoch: [24/100]  [180/189]  eta: 0:00:04  loss: 6.461501 (6.461506)  lr: 0.000483 (0.000484)  wd: 0.000002 (0.000002)  time: 0.517505  data: 0.000121  max mem: 4023\n",
      "Epoch: [24/100]  [188/189]  eta: 0:00:00  loss: 6.461505 (6.461506)  lr: 0.000483 (0.000484)  wd: 0.000002 (0.000002)  time: 0.514899  data: 0.000080  max mem: 4023\n",
      "Epoch: [24/100] Total time: 0:01:42 (0.543120 s / it)\n",
      "Averaged stats: loss: 6.461505 (6.461506)  lr: 0.000483 (0.000484)  wd: 0.000002 (0.000002)\n",
      "Epoch: [25/100]  [  0/189]  eta: 0:12:54  loss: 6.461493 (6.461493)  lr: 0.000483 (0.000483)  wd: 0.000002 (0.000002)  time: 4.095617  data: 3.477723  max mem: 4023\n",
      "Epoch: [25/100]  [ 20/189]  eta: 0:01:56  loss: 6.461493 (6.461497)  lr: 0.000483 (0.000483)  wd: 0.000002 (0.000002)  time: 0.517170  data: 0.000895  max mem: 4023\n",
      "Epoch: [25/100]  [ 40/189]  eta: 0:01:30  loss: 6.461506 (6.461502)  lr: 0.000483 (0.000483)  wd: 0.000002 (0.000002)  time: 0.517478  data: 0.000170  max mem: 4023\n",
      "Epoch: [25/100]  [ 60/189]  eta: 0:01:14  loss: 6.461506 (6.461504)  lr: 0.000483 (0.000483)  wd: 0.000002 (0.000002)  time: 0.520122  data: 0.000146  max mem: 4023\n",
      "Epoch: [25/100]  [ 80/189]  eta: 0:01:01  loss: 6.461513 (6.461523)  lr: 0.000482 (0.000483)  wd: 0.000002 (0.000002)  time: 0.511191  data: 0.000342  max mem: 4023\n",
      "Epoch: [25/100]  [100/189]  eta: 0:00:49  loss: 6.461576 (6.461538)  lr: 0.000482 (0.000483)  wd: 0.000002 (0.000002)  time: 0.522615  data: 0.000172  max mem: 4023\n",
      "Epoch: [25/100]  [120/189]  eta: 0:00:37  loss: 6.461520 (6.461537)  lr: 0.000482 (0.000483)  wd: 0.000002 (0.000002)  time: 0.512854  data: 0.000405  max mem: 4023\n",
      "Epoch: [25/100]  [140/189]  eta: 0:00:26  loss: 6.461557 (6.461545)  lr: 0.000482 (0.000482)  wd: 0.000002 (0.000002)  time: 0.511181  data: 0.000150  max mem: 4023\n",
      "Epoch: [25/100]  [160/189]  eta: 0:00:15  loss: 6.461524 (6.461545)  lr: 0.000481 (0.000482)  wd: 0.000002 (0.000002)  time: 0.530916  data: 0.000162  max mem: 4023\n",
      "Epoch: [25/100]  [180/189]  eta: 0:00:04  loss: 6.461523 (6.461542)  lr: 0.000481 (0.000482)  wd: 0.000002 (0.000002)  time: 0.526630  data: 0.000124  max mem: 4023\n",
      "Epoch: [25/100]  [188/189]  eta: 0:00:00  loss: 6.461513 (6.461542)  lr: 0.000481 (0.000482)  wd: 0.000002 (0.000002)  time: 0.513303  data: 0.000086  max mem: 4023\n",
      "Epoch: [25/100] Total time: 0:01:41 (0.537356 s / it)\n",
      "Averaged stats: loss: 6.461513 (6.461542)  lr: 0.000481 (0.000482)  wd: 0.000002 (0.000002)\n",
      "Epoch: [26/100]  [  0/189]  eta: 0:11:57  loss: 6.461508 (6.461508)  lr: 0.000481 (0.000481)  wd: 0.000002 (0.000002)  time: 3.795073  data: 3.196006  max mem: 4023\n",
      "Epoch: [26/100]  [ 20/189]  eta: 0:01:54  loss: 6.461513 (6.461530)  lr: 0.000481 (0.000481)  wd: 0.000002 (0.000002)  time: 0.523313  data: 0.003024  max mem: 4023\n",
      "Epoch: [26/100]  [ 40/189]  eta: 0:01:29  loss: 6.461506 (6.461520)  lr: 0.000481 (0.000481)  wd: 0.000002 (0.000002)  time: 0.517636  data: 0.000902  max mem: 4023\n",
      "Epoch: [26/100]  [ 60/189]  eta: 0:01:13  loss: 6.461504 (6.461515)  lr: 0.000480 (0.000481)  wd: 0.000002 (0.000002)  time: 0.513734  data: 0.000875  max mem: 4023\n",
      "Epoch: [26/100]  [ 80/189]  eta: 0:01:01  loss: 6.461500 (6.461512)  lr: 0.000480 (0.000481)  wd: 0.000002 (0.000002)  time: 0.530548  data: 0.000149  max mem: 4023\n",
      "Epoch: [26/100]  [100/189]  eta: 0:00:49  loss: 6.461936 (6.461648)  lr: 0.000480 (0.000480)  wd: 0.000002 (0.000002)  time: 0.509984  data: 0.000492  max mem: 4023\n",
      "Epoch: [26/100]  [120/189]  eta: 0:00:37  loss: 6.461630 (6.461652)  lr: 0.000480 (0.000480)  wd: 0.000002 (0.000002)  time: 0.523743  data: 0.000911  max mem: 4023\n",
      "Epoch: [26/100]  [140/189]  eta: 0:00:26  loss: 6.461530 (6.461637)  lr: 0.000479 (0.000480)  wd: 0.000002 (0.000002)  time: 0.525901  data: 0.000267  max mem: 4023\n",
      "Epoch: [26/100]  [160/189]  eta: 0:00:15  loss: 6.461508 (6.461623)  lr: 0.000479 (0.000480)  wd: 0.000003 (0.000002)  time: 0.514582  data: 0.000152  max mem: 4023\n",
      "Epoch: [26/100]  [180/189]  eta: 0:00:04  loss: 6.461500 (6.461610)  lr: 0.000479 (0.000480)  wd: 0.000003 (0.000002)  time: 0.535173  data: 0.000179  max mem: 4023\n",
      "Epoch: [26/100]  [188/189]  eta: 0:00:00  loss: 6.461503 (6.461606)  lr: 0.000479 (0.000480)  wd: 0.000003 (0.000002)  time: 0.510861  data: 0.000141  max mem: 4023\n",
      "Epoch: [26/100] Total time: 0:01:41 (0.538291 s / it)\n",
      "Averaged stats: loss: 6.461503 (6.461606)  lr: 0.000479 (0.000480)  wd: 0.000003 (0.000002)\n",
      "Epoch: [27/100]  [  0/189]  eta: 0:14:57  loss: 6.461539 (6.461539)  lr: 0.000479 (0.000479)  wd: 0.000003 (0.000003)  time: 4.749581  data: 4.181285  max mem: 4023\n",
      "Epoch: [27/100]  [ 20/189]  eta: 0:02:01  loss: 6.461504 (6.461519)  lr: 0.000478 (0.000479)  wd: 0.000003 (0.000003)  time: 0.515126  data: 0.000180  max mem: 4023\n",
      "Epoch: [27/100]  [ 40/189]  eta: 0:01:31  loss: 6.461493 (6.461510)  lr: 0.000478 (0.000478)  wd: 0.000003 (0.000003)  time: 0.510595  data: 0.000977  max mem: 4023\n",
      "Epoch: [27/100]  [ 60/189]  eta: 0:01:15  loss: 6.461489 (6.461513)  lr: 0.000478 (0.000478)  wd: 0.000003 (0.000003)  time: 0.524232  data: 0.001320  max mem: 4023\n",
      "Epoch: [27/100]  [ 80/189]  eta: 0:01:01  loss: 6.461495 (6.461509)  lr: 0.000478 (0.000478)  wd: 0.000003 (0.000003)  time: 0.514456  data: 0.000272  max mem: 4023\n",
      "Epoch: [27/100]  [100/189]  eta: 0:00:49  loss: 6.461487 (6.461504)  lr: 0.000477 (0.000478)  wd: 0.000003 (0.000003)  time: 0.518656  data: 0.000341  max mem: 4023\n",
      "Epoch: [27/100]  [120/189]  eta: 0:00:38  loss: 6.461485 (6.461504)  lr: 0.000477 (0.000478)  wd: 0.000003 (0.000003)  time: 0.526272  data: 0.000192  max mem: 4023\n",
      "Epoch: [27/100]  [140/189]  eta: 0:00:26  loss: 6.461500 (6.461505)  lr: 0.000477 (0.000478)  wd: 0.000003 (0.000003)  time: 0.521066  data: 0.000271  max mem: 4023\n",
      "Epoch: [27/100]  [160/189]  eta: 0:00:15  loss: 6.461485 (6.461502)  lr: 0.000477 (0.000478)  wd: 0.000003 (0.000003)  time: 0.516719  data: 0.000150  max mem: 4023\n",
      "Epoch: [27/100]  [180/189]  eta: 0:00:04  loss: 6.461487 (6.461501)  lr: 0.000476 (0.000477)  wd: 0.000003 (0.000003)  time: 0.520167  data: 0.000276  max mem: 4023\n",
      "Epoch: [27/100]  [188/189]  eta: 0:00:00  loss: 6.461489 (6.461501)  lr: 0.000476 (0.000477)  wd: 0.000003 (0.000003)  time: 0.508307  data: 0.000077  max mem: 4023\n",
      "Epoch: [27/100] Total time: 0:01:42 (0.540788 s / it)\n",
      "Averaged stats: loss: 6.461489 (6.461501)  lr: 0.000476 (0.000477)  wd: 0.000003 (0.000003)\n",
      "Epoch: [28/100]  [  0/189]  eta: 0:14:08  loss: 6.461504 (6.461504)  lr: 0.000476 (0.000476)  wd: 0.000003 (0.000003)  time: 4.491286  data: 3.905516  max mem: 4023\n",
      "Epoch: [28/100]  [ 20/189]  eta: 0:01:58  loss: 6.461492 (6.461496)  lr: 0.000476 (0.000476)  wd: 0.000003 (0.000003)  time: 0.514387  data: 0.000178  max mem: 4023\n",
      "Epoch: [28/100]  [ 40/189]  eta: 0:01:31  loss: 6.461494 (6.461497)  lr: 0.000476 (0.000476)  wd: 0.000003 (0.000003)  time: 0.519396  data: 0.000175  max mem: 4023\n",
      "Epoch: [28/100]  [ 60/189]  eta: 0:01:15  loss: 6.461493 (6.461505)  lr: 0.000475 (0.000476)  wd: 0.000003 (0.000003)  time: 0.523112  data: 0.000339  max mem: 4023\n",
      "Epoch: [28/100]  [ 80/189]  eta: 0:01:01  loss: 6.461541 (6.461515)  lr: 0.000475 (0.000476)  wd: 0.000003 (0.000003)  time: 0.512954  data: 0.000866  max mem: 4023\n",
      "Epoch: [28/100]  [100/189]  eta: 0:00:49  loss: 6.461503 (6.461513)  lr: 0.000475 (0.000475)  wd: 0.000003 (0.000003)  time: 0.513334  data: 0.000164  max mem: 4023\n",
      "Epoch: [28/100]  [120/189]  eta: 0:00:37  loss: 6.461507 (6.461520)  lr: 0.000475 (0.000475)  wd: 0.000003 (0.000003)  time: 0.520716  data: 0.000304  max mem: 4023\n",
      "Epoch: [28/100]  [140/189]  eta: 0:00:26  loss: 6.461532 (6.461522)  lr: 0.000474 (0.000475)  wd: 0.000003 (0.000003)  time: 0.530328  data: 0.000225  max mem: 4023\n",
      "Epoch: [28/100]  [160/189]  eta: 0:00:15  loss: 6.461499 (6.461519)  lr: 0.000474 (0.000475)  wd: 0.000003 (0.000003)  time: 0.518399  data: 0.000135  max mem: 4023\n",
      "Epoch: [28/100]  [180/189]  eta: 0:00:04  loss: 6.461484 (6.461516)  lr: 0.000474 (0.000475)  wd: 0.000003 (0.000003)  time: 0.523302  data: 0.000215  max mem: 4023\n",
      "Epoch: [28/100]  [188/189]  eta: 0:00:00  loss: 6.461497 (6.461516)  lr: 0.000474 (0.000475)  wd: 0.000003 (0.000003)  time: 0.517733  data: 0.000175  max mem: 4023\n",
      "Epoch: [28/100] Total time: 0:01:42 (0.540778 s / it)\n",
      "Averaged stats: loss: 6.461497 (6.461516)  lr: 0.000474 (0.000475)  wd: 0.000003 (0.000003)\n",
      "Epoch: [29/100]  [  0/189]  eta: 0:12:58  loss: 6.461493 (6.461493)  lr: 0.000474 (0.000474)  wd: 0.000003 (0.000003)  time: 4.120358  data: 3.513628  max mem: 4023\n",
      "Epoch: [29/100]  [ 20/189]  eta: 0:01:57  loss: 6.461495 (6.461500)  lr: 0.000473 (0.000473)  wd: 0.000003 (0.000003)  time: 0.522821  data: 0.000176  max mem: 4023\n",
      "Epoch: [29/100]  [ 40/189]  eta: 0:01:31  loss: 6.461490 (6.461497)  lr: 0.000473 (0.000473)  wd: 0.000003 (0.000003)  time: 0.523474  data: 0.001035  max mem: 4023\n",
      "Epoch: [29/100]  [ 60/189]  eta: 0:01:14  loss: 6.461489 (6.461495)  lr: 0.000473 (0.000473)  wd: 0.000003 (0.000003)  time: 0.513334  data: 0.000334  max mem: 4023\n",
      "Epoch: [29/100]  [ 80/189]  eta: 0:01:01  loss: 6.461494 (6.461498)  lr: 0.000472 (0.000473)  wd: 0.000003 (0.000003)  time: 0.512028  data: 0.000145  max mem: 4023\n",
      "Epoch: [29/100]  [100/189]  eta: 0:00:49  loss: 6.461488 (6.461497)  lr: 0.000472 (0.000473)  wd: 0.000003 (0.000003)  time: 0.523048  data: 0.000274  max mem: 4023\n",
      "Epoch: [29/100]  [120/189]  eta: 0:00:37  loss: 6.461507 (6.461500)  lr: 0.000472 (0.000473)  wd: 0.000003 (0.000003)  time: 0.522098  data: 0.000198  max mem: 4023\n",
      "Epoch: [29/100]  [140/189]  eta: 0:00:26  loss: 6.461526 (6.461505)  lr: 0.000472 (0.000472)  wd: 0.000003 (0.000003)  time: 0.534343  data: 0.000146  max mem: 4023\n",
      "Epoch: [29/100]  [160/189]  eta: 0:00:15  loss: 6.461516 (6.461511)  lr: 0.000471 (0.000472)  wd: 0.000003 (0.000003)  time: 0.514923  data: 0.000238  max mem: 4023\n",
      "Epoch: [29/100]  [180/189]  eta: 0:00:04  loss: 6.461501 (6.461511)  lr: 0.000471 (0.000472)  wd: 0.000003 (0.000003)  time: 0.519241  data: 0.000327  max mem: 4023\n",
      "Epoch: [29/100]  [188/189]  eta: 0:00:00  loss: 6.461495 (6.461510)  lr: 0.000471 (0.000472)  wd: 0.000003 (0.000003)  time: 0.523350  data: 0.000081  max mem: 4023\n",
      "Epoch: [29/100] Total time: 0:01:42 (0.540612 s / it)\n",
      "Averaged stats: loss: 6.461495 (6.461510)  lr: 0.000471 (0.000472)  wd: 0.000003 (0.000003)\n",
      "Epoch: [30/100]  [  0/189]  eta: 0:12:51  loss: 6.461483 (6.461483)  lr: 0.000471 (0.000471)  wd: 0.000003 (0.000003)  time: 4.082528  data: 3.508738  max mem: 4023\n",
      "Epoch: [30/100]  [ 20/189]  eta: 0:01:57  loss: 6.461483 (6.461487)  lr: 0.000471 (0.000471)  wd: 0.000003 (0.000003)  time: 0.523286  data: 0.004546  max mem: 4023\n",
      "Epoch: [30/100]  [ 40/189]  eta: 0:01:30  loss: 6.461542 (6.461524)  lr: 0.000470 (0.000470)  wd: 0.000003 (0.000003)  time: 0.522548  data: 0.000884  max mem: 4023\n",
      "Epoch: [30/100]  [ 60/189]  eta: 0:01:14  loss: 6.461493 (6.461516)  lr: 0.000470 (0.000470)  wd: 0.000003 (0.000003)  time: 0.515501  data: 0.000391  max mem: 4023\n",
      "Epoch: [30/100]  [ 80/189]  eta: 0:01:01  loss: 6.461493 (6.461512)  lr: 0.000470 (0.000470)  wd: 0.000003 (0.000003)  time: 0.515852  data: 0.000246  max mem: 4023\n",
      "Epoch: [30/100]  [100/189]  eta: 0:00:49  loss: 6.461487 (6.461507)  lr: 0.000469 (0.000470)  wd: 0.000003 (0.000003)  time: 0.517510  data: 0.001313  max mem: 4023\n",
      "Epoch: [30/100]  [120/189]  eta: 0:00:37  loss: 6.461495 (6.461505)  lr: 0.000469 (0.000470)  wd: 0.000003 (0.000003)  time: 0.528362  data: 0.000156  max mem: 4023\n",
      "Epoch: [30/100]  [140/189]  eta: 0:00:26  loss: 6.461510 (6.461510)  lr: 0.000469 (0.000470)  wd: 0.000003 (0.000003)  time: 0.514934  data: 0.000165  max mem: 4023\n",
      "Epoch: [30/100]  [160/189]  eta: 0:00:15  loss: 6.461506 (6.461510)  lr: 0.000468 (0.000470)  wd: 0.000003 (0.000003)  time: 0.535414  data: 0.000156  max mem: 4023\n",
      "Epoch: [30/100]  [180/189]  eta: 0:00:04  loss: 6.461488 (6.461508)  lr: 0.000468 (0.000469)  wd: 0.000003 (0.000003)  time: 0.525601  data: 0.000100  max mem: 4023\n",
      "Epoch: [30/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461507)  lr: 0.000468 (0.000469)  wd: 0.000003 (0.000003)  time: 0.512893  data: 0.000071  max mem: 4023\n",
      "Epoch: [30/100] Total time: 0:01:42 (0.540205 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461507)  lr: 0.000468 (0.000469)  wd: 0.000003 (0.000003)\n",
      "Epoch: [31/100]  [  0/189]  eta: 0:13:47  loss: 6.461499 (6.461499)  lr: 0.000468 (0.000468)  wd: 0.000003 (0.000003)  time: 4.377602  data: 3.760109  max mem: 4023\n",
      "Epoch: [31/100]  [ 20/189]  eta: 0:01:58  loss: 6.461524 (6.461530)  lr: 0.000468 (0.000468)  wd: 0.000003 (0.000003)  time: 0.517011  data: 0.001728  max mem: 4023\n",
      "Epoch: [31/100]  [ 40/189]  eta: 0:01:31  loss: 6.461539 (6.461537)  lr: 0.000467 (0.000468)  wd: 0.000003 (0.000003)  time: 0.521498  data: 0.001058  max mem: 4023\n",
      "Epoch: [31/100]  [ 60/189]  eta: 0:01:14  loss: 6.461510 (6.461532)  lr: 0.000467 (0.000467)  wd: 0.000003 (0.000003)  time: 0.514882  data: 0.000165  max mem: 4023\n",
      "Epoch: [31/100]  [ 80/189]  eta: 0:01:01  loss: 6.461498 (6.461524)  lr: 0.000467 (0.000467)  wd: 0.000003 (0.000003)  time: 0.513443  data: 0.000388  max mem: 4023\n",
      "Epoch: [31/100]  [100/189]  eta: 0:00:49  loss: 6.461529 (6.461531)  lr: 0.000466 (0.000467)  wd: 0.000003 (0.000003)  time: 0.512206  data: 0.000182  max mem: 4023\n",
      "Epoch: [31/100]  [120/189]  eta: 0:00:37  loss: 6.461512 (6.461529)  lr: 0.000466 (0.000467)  wd: 0.000003 (0.000003)  time: 0.532054  data: 0.000260  max mem: 4023\n",
      "Epoch: [31/100]  [140/189]  eta: 0:00:26  loss: 6.461508 (6.461527)  lr: 0.000466 (0.000467)  wd: 0.000003 (0.000003)  time: 0.511232  data: 0.000896  max mem: 4023\n",
      "Epoch: [31/100]  [160/189]  eta: 0:00:15  loss: 6.461513 (6.461526)  lr: 0.000466 (0.000467)  wd: 0.000003 (0.000003)  time: 0.531932  data: 0.000286  max mem: 4023\n",
      "Epoch: [31/100]  [180/189]  eta: 0:00:04  loss: 6.461509 (6.461525)  lr: 0.000465 (0.000466)  wd: 0.000003 (0.000003)  time: 0.522251  data: 0.000114  max mem: 4023\n",
      "Epoch: [31/100]  [188/189]  eta: 0:00:00  loss: 6.461509 (6.461524)  lr: 0.000465 (0.000466)  wd: 0.000003 (0.000003)  time: 0.517687  data: 0.000071  max mem: 4023\n",
      "Epoch: [31/100] Total time: 0:01:42 (0.540178 s / it)\n",
      "Averaged stats: loss: 6.461509 (6.461524)  lr: 0.000465 (0.000466)  wd: 0.000003 (0.000003)\n",
      "Epoch: [32/100]  [  0/189]  eta: 0:13:10  loss: 6.461508 (6.461508)  lr: 0.000465 (0.000465)  wd: 0.000003 (0.000003)  time: 4.182945  data: 3.589751  max mem: 4023\n",
      "Epoch: [32/100]  [ 20/189]  eta: 0:01:58  loss: 6.461577 (6.461623)  lr: 0.000465 (0.000465)  wd: 0.000003 (0.000003)  time: 0.525546  data: 0.003154  max mem: 4023\n",
      "Epoch: [32/100]  [ 40/189]  eta: 0:01:30  loss: 6.461535 (6.461592)  lr: 0.000464 (0.000465)  wd: 0.000003 (0.000003)  time: 0.516110  data: 0.001752  max mem: 4023\n",
      "Epoch: [32/100]  [ 60/189]  eta: 0:01:14  loss: 6.461557 (6.461586)  lr: 0.000464 (0.000464)  wd: 0.000003 (0.000003)  time: 0.514913  data: 0.000136  max mem: 4023\n",
      "Epoch: [32/100]  [ 80/189]  eta: 0:01:01  loss: 6.461508 (6.461569)  lr: 0.000464 (0.000464)  wd: 0.000003 (0.000003)  time: 0.512380  data: 0.000459  max mem: 4023\n",
      "Epoch: [32/100]  [100/189]  eta: 0:00:49  loss: 6.461498 (6.461583)  lr: 0.000463 (0.000464)  wd: 0.000003 (0.000003)  time: 0.511188  data: 0.000361  max mem: 4023\n",
      "Epoch: [32/100]  [120/189]  eta: 0:00:37  loss: 6.461736 (6.461619)  lr: 0.000463 (0.000464)  wd: 0.000003 (0.000003)  time: 0.517882  data: 0.000139  max mem: 4023\n",
      "Epoch: [32/100]  [140/189]  eta: 0:00:26  loss: 6.461559 (6.461614)  lr: 0.000463 (0.000464)  wd: 0.000003 (0.000003)  time: 0.512294  data: 0.000210  max mem: 4023\n",
      "Epoch: [32/100]  [160/189]  eta: 0:00:15  loss: 6.461529 (6.461604)  lr: 0.000462 (0.000464)  wd: 0.000003 (0.000003)  time: 0.521788  data: 0.000138  max mem: 4023\n",
      "Epoch: [32/100]  [180/189]  eta: 0:00:04  loss: 6.461499 (6.461593)  lr: 0.000462 (0.000463)  wd: 0.000003 (0.000003)  time: 0.523598  data: 0.000124  max mem: 4023\n",
      "Epoch: [32/100]  [188/189]  eta: 0:00:00  loss: 6.461493 (6.461589)  lr: 0.000462 (0.000463)  wd: 0.000003 (0.000003)  time: 0.516916  data: 0.000080  max mem: 4023\n",
      "Epoch: [32/100] Total time: 0:01:41 (0.536362 s / it)\n",
      "Averaged stats: loss: 6.461493 (6.461589)  lr: 0.000462 (0.000463)  wd: 0.000003 (0.000003)\n",
      "Epoch: [33/100]  [  0/189]  eta: 0:11:33  loss: 6.461491 (6.461491)  lr: 0.000462 (0.000462)  wd: 0.000003 (0.000003)  time: 3.670125  data: 3.064423  max mem: 4023\n",
      "Epoch: [33/100]  [ 20/189]  eta: 0:01:54  loss: 6.461488 (6.461488)  lr: 0.000462 (0.000462)  wd: 0.000003 (0.000003)  time: 0.526283  data: 0.000184  max mem: 4023\n",
      "Epoch: [33/100]  [ 40/189]  eta: 0:01:29  loss: 6.461671 (6.461622)  lr: 0.000461 (0.000461)  wd: 0.000003 (0.000003)  time: 0.515440  data: 0.001293  max mem: 4023\n",
      "Epoch: [33/100]  [ 60/189]  eta: 0:01:13  loss: 6.461541 (6.461598)  lr: 0.000461 (0.000461)  wd: 0.000003 (0.000003)  time: 0.520896  data: 0.000261  max mem: 4023\n",
      "Epoch: [33/100]  [ 80/189]  eta: 0:01:00  loss: 6.461497 (6.461576)  lr: 0.000461 (0.000461)  wd: 0.000003 (0.000003)  time: 0.511780  data: 0.000321  max mem: 4023\n",
      "Epoch: [33/100]  [100/189]  eta: 0:00:48  loss: 6.461484 (6.461559)  lr: 0.000460 (0.000461)  wd: 0.000003 (0.000003)  time: 0.510943  data: 0.000389  max mem: 4023\n",
      "Epoch: [33/100]  [120/189]  eta: 0:00:37  loss: 6.461495 (6.461549)  lr: 0.000460 (0.000461)  wd: 0.000003 (0.000003)  time: 0.528482  data: 0.000162  max mem: 4023\n",
      "Epoch: [33/100]  [140/189]  eta: 0:00:26  loss: 6.461617 (6.461560)  lr: 0.000460 (0.000461)  wd: 0.000003 (0.000003)  time: 0.516663  data: 0.000200  max mem: 4023\n",
      "Epoch: [33/100]  [160/189]  eta: 0:00:15  loss: 6.461529 (6.461558)  lr: 0.000459 (0.000460)  wd: 0.000003 (0.000003)  time: 0.519610  data: 0.000155  max mem: 4023\n",
      "Epoch: [33/100]  [180/189]  eta: 0:00:04  loss: 6.461503 (6.461553)  lr: 0.000459 (0.000460)  wd: 0.000003 (0.000003)  time: 0.513714  data: 0.000113  max mem: 4023\n",
      "Epoch: [33/100]  [188/189]  eta: 0:00:00  loss: 6.461501 (6.461551)  lr: 0.000459 (0.000460)  wd: 0.000003 (0.000003)  time: 0.507540  data: 0.000084  max mem: 4023\n",
      "Epoch: [33/100] Total time: 0:01:41 (0.534700 s / it)\n",
      "Averaged stats: loss: 6.461501 (6.461551)  lr: 0.000459 (0.000460)  wd: 0.000003 (0.000003)\n",
      "Epoch: [34/100]  [  0/189]  eta: 0:12:05  loss: 6.461576 (6.461576)  lr: 0.000459 (0.000459)  wd: 0.000003 (0.000003)  time: 3.839711  data: 3.229107  max mem: 4023\n",
      "Epoch: [34/100]  [ 20/189]  eta: 0:01:57  loss: 6.461587 (6.461658)  lr: 0.000458 (0.000458)  wd: 0.000003 (0.000003)  time: 0.539701  data: 0.016077  max mem: 4023\n",
      "Epoch: [34/100]  [ 40/189]  eta: 0:01:31  loss: 6.461512 (6.461589)  lr: 0.000458 (0.000458)  wd: 0.000003 (0.000003)  time: 0.523555  data: 0.001365  max mem: 4023\n",
      "Epoch: [34/100]  [ 60/189]  eta: 0:01:14  loss: 6.461513 (6.461582)  lr: 0.000458 (0.000458)  wd: 0.000003 (0.000003)  time: 0.516396  data: 0.000332  max mem: 4023\n",
      "Epoch: [34/100]  [ 80/189]  eta: 0:01:01  loss: 6.461515 (6.461597)  lr: 0.000457 (0.000458)  wd: 0.000003 (0.000003)  time: 0.517898  data: 0.000157  max mem: 4023\n",
      "Epoch: [34/100]  [100/189]  eta: 0:00:49  loss: 6.461505 (6.461583)  lr: 0.000457 (0.000458)  wd: 0.000003 (0.000003)  time: 0.516840  data: 0.000254  max mem: 4023\n",
      "Epoch: [34/100]  [120/189]  eta: 0:00:37  loss: 6.461491 (6.461569)  lr: 0.000457 (0.000458)  wd: 0.000003 (0.000003)  time: 0.516875  data: 0.000443  max mem: 4023\n",
      "Epoch: [34/100]  [140/189]  eta: 0:00:26  loss: 6.461482 (6.461557)  lr: 0.000456 (0.000457)  wd: 0.000003 (0.000003)  time: 0.523712  data: 0.000333  max mem: 4023\n",
      "Epoch: [34/100]  [160/189]  eta: 0:00:15  loss: 6.461479 (6.461547)  lr: 0.000456 (0.000457)  wd: 0.000003 (0.000003)  time: 0.530096  data: 0.000141  max mem: 4023\n",
      "Epoch: [34/100]  [180/189]  eta: 0:00:04  loss: 6.461478 (6.461540)  lr: 0.000456 (0.000457)  wd: 0.000003 (0.000003)  time: 0.511880  data: 0.000835  max mem: 4023\n",
      "Epoch: [34/100]  [188/189]  eta: 0:00:00  loss: 6.461478 (6.461537)  lr: 0.000456 (0.000457)  wd: 0.000003 (0.000003)  time: 0.514514  data: 0.000083  max mem: 4023\n",
      "Epoch: [34/100] Total time: 0:01:41 (0.539674 s / it)\n",
      "Averaged stats: loss: 6.461478 (6.461537)  lr: 0.000456 (0.000457)  wd: 0.000003 (0.000003)\n",
      "Epoch: [35/100]  [  0/189]  eta: 0:13:11  loss: 6.461478 (6.461478)  lr: 0.000455 (0.000455)  wd: 0.000003 (0.000003)  time: 4.189823  data: 3.611634  max mem: 4023\n",
      "Epoch: [35/100]  [ 20/189]  eta: 0:01:57  loss: 6.461478 (6.461478)  lr: 0.000455 (0.000455)  wd: 0.000003 (0.000003)  time: 0.518692  data: 0.000184  max mem: 4023\n",
      "Epoch: [35/100]  [ 40/189]  eta: 0:01:30  loss: 6.461478 (6.461479)  lr: 0.000455 (0.000455)  wd: 0.000003 (0.000003)  time: 0.511690  data: 0.003022  max mem: 4023\n",
      "Epoch: [35/100]  [ 60/189]  eta: 0:01:14  loss: 6.461478 (6.461479)  lr: 0.000454 (0.000455)  wd: 0.000003 (0.000003)  time: 0.523577  data: 0.000129  max mem: 4023\n",
      "Epoch: [35/100]  [ 80/189]  eta: 0:01:01  loss: 6.461484 (6.461481)  lr: 0.000454 (0.000455)  wd: 0.000004 (0.000003)  time: 0.513073  data: 0.000142  max mem: 4023\n",
      "Epoch: [35/100]  [100/189]  eta: 0:00:49  loss: 6.461483 (6.461482)  lr: 0.000454 (0.000454)  wd: 0.000004 (0.000003)  time: 0.514842  data: 0.000153  max mem: 4023\n",
      "Epoch: [35/100]  [120/189]  eta: 0:00:37  loss: 6.461480 (6.461481)  lr: 0.000453 (0.000454)  wd: 0.000004 (0.000003)  time: 0.525430  data: 0.000182  max mem: 4023\n",
      "Epoch: [35/100]  [140/189]  eta: 0:00:26  loss: 6.461477 (6.461481)  lr: 0.000453 (0.000454)  wd: 0.000004 (0.000004)  time: 0.519765  data: 0.000245  max mem: 4023\n",
      "Epoch: [35/100]  [160/189]  eta: 0:00:15  loss: 6.461537 (6.461490)  lr: 0.000453 (0.000454)  wd: 0.000004 (0.000004)  time: 0.518190  data: 0.000479  max mem: 4023\n",
      "Epoch: [35/100]  [180/189]  eta: 0:00:04  loss: 6.461497 (6.461491)  lr: 0.000452 (0.000454)  wd: 0.000004 (0.000004)  time: 0.512951  data: 0.000117  max mem: 4023\n",
      "Epoch: [35/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461491)  lr: 0.000452 (0.000454)  wd: 0.000004 (0.000004)  time: 0.504815  data: 0.000073  max mem: 4023\n",
      "Epoch: [35/100] Total time: 0:01:41 (0.536177 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461491)  lr: 0.000452 (0.000454)  wd: 0.000004 (0.000004)\n",
      "Epoch: [36/100]  [  0/189]  eta: 0:12:45  loss: 6.461481 (6.461481)  lr: 0.000452 (0.000452)  wd: 0.000004 (0.000004)  time: 4.052034  data: 3.429301  max mem: 4023\n",
      "Epoch: [36/100]  [ 20/189]  eta: 0:01:56  loss: 6.461479 (6.461479)  lr: 0.000452 (0.000452)  wd: 0.000004 (0.000004)  time: 0.520240  data: 0.001501  max mem: 4023\n",
      "Epoch: [36/100]  [ 40/189]  eta: 0:01:30  loss: 6.461477 (6.461478)  lr: 0.000451 (0.000452)  wd: 0.000004 (0.000004)  time: 0.523602  data: 0.000183  max mem: 4023\n",
      "Epoch: [36/100]  [ 60/189]  eta: 0:01:14  loss: 6.461478 (6.461480)  lr: 0.000451 (0.000451)  wd: 0.000004 (0.000004)  time: 0.513105  data: 0.001347  max mem: 4023\n",
      "Epoch: [36/100]  [ 80/189]  eta: 0:01:01  loss: 6.461481 (6.461480)  lr: 0.000451 (0.000451)  wd: 0.000004 (0.000004)  time: 0.525109  data: 0.001663  max mem: 4023\n",
      "Epoch: [36/100]  [100/189]  eta: 0:00:49  loss: 6.461478 (6.461480)  lr: 0.000450 (0.000451)  wd: 0.000004 (0.000004)  time: 0.515837  data: 0.000147  max mem: 4023\n",
      "Epoch: [36/100]  [120/189]  eta: 0:00:37  loss: 6.461478 (6.461480)  lr: 0.000450 (0.000451)  wd: 0.000004 (0.000004)  time: 0.515123  data: 0.000141  max mem: 4023\n",
      "Epoch: [36/100]  [140/189]  eta: 0:00:26  loss: 6.461483 (6.461487)  lr: 0.000450 (0.000451)  wd: 0.000004 (0.000004)  time: 0.512102  data: 0.000308  max mem: 4023\n",
      "Epoch: [36/100]  [160/189]  eta: 0:00:15  loss: 6.461504 (6.461489)  lr: 0.000449 (0.000450)  wd: 0.000004 (0.000004)  time: 0.516646  data: 0.000336  max mem: 4023\n",
      "Epoch: [36/100]  [180/189]  eta: 0:00:04  loss: 6.461480 (6.461488)  lr: 0.000449 (0.000450)  wd: 0.000004 (0.000004)  time: 0.527723  data: 0.000099  max mem: 4023\n",
      "Epoch: [36/100]  [188/189]  eta: 0:00:00  loss: 6.461479 (6.461488)  lr: 0.000449 (0.000450)  wd: 0.000004 (0.000004)  time: 0.508997  data: 0.000070  max mem: 4023\n",
      "Epoch: [36/100] Total time: 0:01:41 (0.536837 s / it)\n",
      "Averaged stats: loss: 6.461479 (6.461488)  lr: 0.000449 (0.000450)  wd: 0.000004 (0.000004)\n",
      "Epoch: [37/100]  [  0/189]  eta: 0:11:58  loss: 6.461474 (6.461474)  lr: 0.000448 (0.000448)  wd: 0.000004 (0.000004)  time: 3.799911  data: 3.212163  max mem: 4023\n",
      "Epoch: [37/100]  [ 20/189]  eta: 0:01:54  loss: 6.461478 (6.461478)  lr: 0.000448 (0.000448)  wd: 0.000004 (0.000004)  time: 0.521351  data: 0.000887  max mem: 4023\n",
      "Epoch: [37/100]  [ 40/189]  eta: 0:01:29  loss: 6.461485 (6.461483)  lr: 0.000448 (0.000448)  wd: 0.000004 (0.000004)  time: 0.515878  data: 0.000835  max mem: 4023\n",
      "Epoch: [37/100]  [ 60/189]  eta: 0:01:13  loss: 6.461479 (6.461482)  lr: 0.000448 (0.000448)  wd: 0.000004 (0.000004)  time: 0.511747  data: 0.000143  max mem: 4023\n",
      "Epoch: [37/100]  [ 80/189]  eta: 0:01:01  loss: 6.461478 (6.461481)  lr: 0.000447 (0.000448)  wd: 0.000004 (0.000004)  time: 0.532487  data: 0.000203  max mem: 4023\n",
      "Epoch: [37/100]  [100/189]  eta: 0:00:49  loss: 6.461509 (6.461489)  lr: 0.000447 (0.000448)  wd: 0.000004 (0.000004)  time: 0.518300  data: 0.000139  max mem: 4023\n",
      "Epoch: [37/100]  [120/189]  eta: 0:00:37  loss: 6.461504 (6.461506)  lr: 0.000446 (0.000447)  wd: 0.000004 (0.000004)  time: 0.510611  data: 0.000447  max mem: 4023\n",
      "Epoch: [37/100]  [140/189]  eta: 0:00:26  loss: 6.461533 (6.461511)  lr: 0.000446 (0.000447)  wd: 0.000004 (0.000004)  time: 0.530484  data: 0.000508  max mem: 4023\n",
      "Epoch: [37/100]  [160/189]  eta: 0:00:15  loss: 6.461485 (6.461508)  lr: 0.000446 (0.000447)  wd: 0.000004 (0.000004)  time: 0.514639  data: 0.000308  max mem: 4023\n",
      "Epoch: [37/100]  [180/189]  eta: 0:00:04  loss: 6.461554 (6.461517)  lr: 0.000445 (0.000447)  wd: 0.000004 (0.000004)  time: 0.516451  data: 0.000143  max mem: 4023\n",
      "Epoch: [37/100]  [188/189]  eta: 0:00:00  loss: 6.461536 (6.461517)  lr: 0.000445 (0.000447)  wd: 0.000004 (0.000004)  time: 0.506857  data: 0.000086  max mem: 4023\n",
      "Epoch: [37/100] Total time: 0:01:41 (0.535515 s / it)\n",
      "Averaged stats: loss: 6.461536 (6.461517)  lr: 0.000445 (0.000447)  wd: 0.000004 (0.000004)\n",
      "Epoch: [38/100]  [  0/189]  eta: 0:14:18  loss: 6.461499 (6.461499)  lr: 0.000445 (0.000445)  wd: 0.000004 (0.000004)  time: 4.541329  data: 3.958434  max mem: 4023\n",
      "Epoch: [38/100]  [ 20/189]  eta: 0:01:58  loss: 6.461486 (6.461487)  lr: 0.000445 (0.000445)  wd: 0.000004 (0.000004)  time: 0.511051  data: 0.000179  max mem: 4023\n",
      "Epoch: [38/100]  [ 40/189]  eta: 0:01:30  loss: 6.461498 (6.461497)  lr: 0.000444 (0.000445)  wd: 0.000004 (0.000004)  time: 0.510133  data: 0.000970  max mem: 4023\n",
      "Epoch: [38/100]  [ 60/189]  eta: 0:01:14  loss: 6.461484 (6.461493)  lr: 0.000444 (0.000444)  wd: 0.000004 (0.000004)  time: 0.512732  data: 0.000175  max mem: 4023\n",
      "Epoch: [38/100]  [ 80/189]  eta: 0:01:01  loss: 6.461479 (6.461490)  lr: 0.000444 (0.000444)  wd: 0.000004 (0.000004)  time: 0.519544  data: 0.000731  max mem: 4023\n",
      "Epoch: [38/100]  [100/189]  eta: 0:00:49  loss: 6.461477 (6.461492)  lr: 0.000443 (0.000444)  wd: 0.000004 (0.000004)  time: 0.526511  data: 0.000170  max mem: 4023\n",
      "Epoch: [38/100]  [120/189]  eta: 0:00:37  loss: 6.461506 (6.461496)  lr: 0.000443 (0.000444)  wd: 0.000004 (0.000004)  time: 0.510971  data: 0.000157  max mem: 4023\n",
      "Epoch: [38/100]  [140/189]  eta: 0:00:26  loss: 6.461753 (6.461577)  lr: 0.000442 (0.000444)  wd: 0.000004 (0.000004)  time: 0.519038  data: 0.000151  max mem: 4023\n",
      "Epoch: [38/100]  [160/189]  eta: 0:00:15  loss: 6.461576 (6.461581)  lr: 0.000442 (0.000443)  wd: 0.000004 (0.000004)  time: 0.515758  data: 0.000284  max mem: 4023\n",
      "Epoch: [38/100]  [180/189]  eta: 0:00:04  loss: 6.461502 (6.461572)  lr: 0.000442 (0.000443)  wd: 0.000004 (0.000004)  time: 0.507230  data: 0.000122  max mem: 4023\n",
      "Epoch: [38/100]  [188/189]  eta: 0:00:00  loss: 6.461502 (6.461569)  lr: 0.000441 (0.000443)  wd: 0.000004 (0.000004)  time: 0.505167  data: 0.000078  max mem: 4023\n",
      "Epoch: [38/100] Total time: 0:01:41 (0.536135 s / it)\n",
      "Averaged stats: loss: 6.461502 (6.461569)  lr: 0.000441 (0.000443)  wd: 0.000004 (0.000004)\n",
      "Epoch: [39/100]  [  0/189]  eta: 0:12:07  loss: 6.461498 (6.461498)  lr: 0.000441 (0.000441)  wd: 0.000004 (0.000004)  time: 3.847492  data: 3.260708  max mem: 4023\n",
      "Epoch: [39/100]  [ 20/189]  eta: 0:01:55  loss: 6.461505 (6.461510)  lr: 0.000441 (0.000441)  wd: 0.000004 (0.000004)  time: 0.522877  data: 0.003029  max mem: 4023\n",
      "Epoch: [39/100]  [ 40/189]  eta: 0:01:30  loss: 6.461493 (6.461503)  lr: 0.000441 (0.000441)  wd: 0.000004 (0.000004)  time: 0.523584  data: 0.000854  max mem: 4023\n",
      "Epoch: [39/100]  [ 60/189]  eta: 0:01:14  loss: 6.461491 (6.461500)  lr: 0.000440 (0.000441)  wd: 0.000004 (0.000004)  time: 0.521642  data: 0.000167  max mem: 4023\n",
      "Epoch: [39/100]  [ 80/189]  eta: 0:01:01  loss: 6.461496 (6.461503)  lr: 0.000440 (0.000440)  wd: 0.000004 (0.000004)  time: 0.510455  data: 0.000602  max mem: 4023\n",
      "Epoch: [39/100]  [100/189]  eta: 0:00:49  loss: 6.461510 (6.461513)  lr: 0.000439 (0.000440)  wd: 0.000004 (0.000004)  time: 0.518156  data: 0.000165  max mem: 4023\n",
      "Epoch: [39/100]  [120/189]  eta: 0:00:37  loss: 6.461627 (6.461611)  lr: 0.000439 (0.000440)  wd: 0.000004 (0.000004)  time: 0.512494  data: 0.000549  max mem: 4023\n",
      "Epoch: [39/100]  [140/189]  eta: 0:00:26  loss: 6.461903 (6.461686)  lr: 0.000439 (0.000440)  wd: 0.000004 (0.000004)  time: 0.517110  data: 0.000765  max mem: 4023\n",
      "Epoch: [39/100]  [160/189]  eta: 0:00:15  loss: 6.461543 (6.461673)  lr: 0.000438 (0.000440)  wd: 0.000004 (0.000004)  time: 0.511226  data: 0.000156  max mem: 4023\n",
      "Epoch: [39/100]  [180/189]  eta: 0:00:04  loss: 6.461485 (6.461652)  lr: 0.000438 (0.000439)  wd: 0.000004 (0.000004)  time: 0.529582  data: 0.000252  max mem: 4023\n",
      "Epoch: [39/100]  [188/189]  eta: 0:00:00  loss: 6.461479 (6.461645)  lr: 0.000438 (0.000439)  wd: 0.000004 (0.000004)  time: 0.521415  data: 0.000068  max mem: 4023\n",
      "Epoch: [39/100] Total time: 0:01:41 (0.536285 s / it)\n",
      "Averaged stats: loss: 6.461479 (6.461645)  lr: 0.000438 (0.000439)  wd: 0.000004 (0.000004)\n",
      "Epoch: [40/100]  [  0/189]  eta: 0:11:42  loss: 6.461476 (6.461476)  lr: 0.000438 (0.000438)  wd: 0.000004 (0.000004)  time: 3.717096  data: 3.108282  max mem: 4023\n",
      "Epoch: [40/100]  [ 20/189]  eta: 0:01:54  loss: 6.461476 (6.461477)  lr: 0.000437 (0.000437)  wd: 0.000004 (0.000004)  time: 0.523517  data: 0.002628  max mem: 4023\n",
      "Epoch: [40/100]  [ 40/189]  eta: 0:01:28  loss: 6.461479 (6.461478)  lr: 0.000437 (0.000437)  wd: 0.000004 (0.000004)  time: 0.512534  data: 0.001075  max mem: 4023\n",
      "Epoch: [40/100]  [ 60/189]  eta: 0:01:13  loss: 6.461481 (6.461570)  lr: 0.000436 (0.000437)  wd: 0.000004 (0.000004)  time: 0.514115  data: 0.000171  max mem: 4023\n",
      "Epoch: [40/100]  [ 80/189]  eta: 0:01:00  loss: 6.461825 (6.461663)  lr: 0.000436 (0.000437)  wd: 0.000004 (0.000004)  time: 0.514282  data: 0.000954  max mem: 4023\n",
      "Epoch: [40/100]  [100/189]  eta: 0:00:48  loss: 6.461515 (6.461636)  lr: 0.000436 (0.000436)  wd: 0.000004 (0.000004)  time: 0.516999  data: 0.000137  max mem: 4023\n",
      "Epoch: [40/100]  [120/189]  eta: 0:00:37  loss: 6.461483 (6.461611)  lr: 0.000435 (0.000436)  wd: 0.000004 (0.000004)  time: 0.515833  data: 0.000133  max mem: 4023\n",
      "Epoch: [40/100]  [140/189]  eta: 0:00:26  loss: 6.461477 (6.461593)  lr: 0.000435 (0.000436)  wd: 0.000004 (0.000004)  time: 0.518746  data: 0.000145  max mem: 4023\n",
      "Epoch: [40/100]  [160/189]  eta: 0:00:15  loss: 6.461482 (6.461579)  lr: 0.000434 (0.000436)  wd: 0.000004 (0.000004)  time: 0.518471  data: 0.000141  max mem: 4023\n",
      "Epoch: [40/100]  [180/189]  eta: 0:00:04  loss: 6.461476 (6.461568)  lr: 0.000434 (0.000436)  wd: 0.000004 (0.000004)  time: 0.506610  data: 0.000113  max mem: 4023\n",
      "Epoch: [40/100]  [188/189]  eta: 0:00:00  loss: 6.461475 (6.461564)  lr: 0.000434 (0.000436)  wd: 0.000004 (0.000004)  time: 0.507377  data: 0.000080  max mem: 4023\n",
      "Epoch: [40/100] Total time: 0:01:40 (0.532748 s / it)\n",
      "Averaged stats: loss: 6.461475 (6.461564)  lr: 0.000434 (0.000436)  wd: 0.000004 (0.000004)\n",
      "Epoch: [41/100]  [  0/189]  eta: 0:14:03  loss: 6.461474 (6.461474)  lr: 0.000434 (0.000434)  wd: 0.000004 (0.000004)  time: 4.461136  data: 3.864472  max mem: 4023\n",
      "Epoch: [41/100]  [ 20/189]  eta: 0:01:59  loss: 6.461474 (6.461474)  lr: 0.000433 (0.000433)  wd: 0.000004 (0.000004)  time: 0.518934  data: 0.000929  max mem: 4023\n",
      "Epoch: [41/100]  [ 40/189]  eta: 0:01:30  loss: 6.461473 (6.461474)  lr: 0.000433 (0.000433)  wd: 0.000004 (0.000004)  time: 0.508560  data: 0.001171  max mem: 4023\n",
      "Epoch: [41/100]  [ 60/189]  eta: 0:01:14  loss: 6.461472 (6.461473)  lr: 0.000433 (0.000433)  wd: 0.000004 (0.000004)  time: 0.519106  data: 0.002266  max mem: 4023\n",
      "Epoch: [41/100]  [ 80/189]  eta: 0:01:01  loss: 6.461475 (6.461473)  lr: 0.000432 (0.000433)  wd: 0.000004 (0.000004)  time: 0.509566  data: 0.000316  max mem: 4023\n",
      "Epoch: [41/100]  [100/189]  eta: 0:00:49  loss: 6.461476 (6.461482)  lr: 0.000432 (0.000433)  wd: 0.000004 (0.000004)  time: 0.525522  data: 0.000349  max mem: 4023\n",
      "Epoch: [41/100]  [120/189]  eta: 0:00:37  loss: 6.461520 (6.461494)  lr: 0.000431 (0.000432)  wd: 0.000004 (0.000004)  time: 0.518853  data: 0.000166  max mem: 4023\n",
      "Epoch: [41/100]  [140/189]  eta: 0:00:26  loss: 6.461503 (6.461497)  lr: 0.000431 (0.000432)  wd: 0.000004 (0.000004)  time: 0.524001  data: 0.000154  max mem: 4023\n",
      "Epoch: [41/100]  [160/189]  eta: 0:00:15  loss: 6.461480 (6.461495)  lr: 0.000431 (0.000432)  wd: 0.000004 (0.000004)  time: 0.522959  data: 0.000141  max mem: 4023\n",
      "Epoch: [41/100]  [180/189]  eta: 0:00:04  loss: 6.461475 (6.461493)  lr: 0.000430 (0.000432)  wd: 0.000004 (0.000004)  time: 0.507650  data: 0.000315  max mem: 4023\n",
      "Epoch: [41/100]  [188/189]  eta: 0:00:00  loss: 6.461474 (6.461492)  lr: 0.000430 (0.000432)  wd: 0.000004 (0.000004)  time: 0.514668  data: 0.000080  max mem: 4023\n",
      "Epoch: [41/100] Total time: 0:01:41 (0.538645 s / it)\n",
      "Averaged stats: loss: 6.461474 (6.461492)  lr: 0.000430 (0.000432)  wd: 0.000004 (0.000004)\n",
      "Epoch: [42/100]  [  0/189]  eta: 0:12:35  loss: 6.461474 (6.461474)  lr: 0.000430 (0.000430)  wd: 0.000004 (0.000004)  time: 3.997085  data: 3.444889  max mem: 4023\n",
      "Epoch: [42/100]  [ 20/189]  eta: 0:01:58  loss: 6.461473 (6.461474)  lr: 0.000430 (0.000430)  wd: 0.000004 (0.000004)  time: 0.536165  data: 0.023279  max mem: 4023\n",
      "Epoch: [42/100]  [ 40/189]  eta: 0:01:30  loss: 6.461707 (6.461614)  lr: 0.000429 (0.000429)  wd: 0.000004 (0.000004)  time: 0.510790  data: 0.002395  max mem: 4023\n",
      "Epoch: [42/100]  [ 60/189]  eta: 0:01:14  loss: 6.461565 (6.461604)  lr: 0.000429 (0.000429)  wd: 0.000004 (0.000004)  time: 0.513918  data: 0.000338  max mem: 4023\n",
      "Epoch: [42/100]  [ 80/189]  eta: 0:01:01  loss: 6.461483 (6.461575)  lr: 0.000428 (0.000429)  wd: 0.000004 (0.000004)  time: 0.521331  data: 0.000491  max mem: 4023\n",
      "Epoch: [42/100]  [100/189]  eta: 0:00:49  loss: 6.461478 (6.461558)  lr: 0.000428 (0.000429)  wd: 0.000004 (0.000004)  time: 0.523495  data: 0.000410  max mem: 4023\n",
      "Epoch: [42/100]  [120/189]  eta: 0:00:37  loss: 6.461488 (6.461547)  lr: 0.000427 (0.000429)  wd: 0.000004 (0.000004)  time: 0.519979  data: 0.000339  max mem: 4023\n",
      "Epoch: [42/100]  [140/189]  eta: 0:00:26  loss: 6.461477 (6.461537)  lr: 0.000427 (0.000428)  wd: 0.000004 (0.000004)  time: 0.516918  data: 0.000135  max mem: 4023\n",
      "Epoch: [42/100]  [160/189]  eta: 0:00:15  loss: 6.461473 (6.461529)  lr: 0.000427 (0.000428)  wd: 0.000004 (0.000004)  time: 0.517660  data: 0.000137  max mem: 4023\n",
      "Epoch: [42/100]  [180/189]  eta: 0:00:04  loss: 6.461677 (6.461556)  lr: 0.000426 (0.000428)  wd: 0.000005 (0.000004)  time: 0.515751  data: 0.000457  max mem: 4023\n",
      "Epoch: [42/100]  [188/189]  eta: 0:00:00  loss: 6.461599 (6.461555)  lr: 0.000426 (0.000428)  wd: 0.000005 (0.000004)  time: 0.508627  data: 0.000315  max mem: 4023\n",
      "Epoch: [42/100] Total time: 0:01:41 (0.537087 s / it)\n",
      "Averaged stats: loss: 6.461599 (6.461555)  lr: 0.000426 (0.000428)  wd: 0.000005 (0.000004)\n",
      "Epoch: [43/100]  [  0/189]  eta: 0:14:44  loss: 6.461517 (6.461517)  lr: 0.000426 (0.000426)  wd: 0.000005 (0.000005)  time: 4.679719  data: 4.083824  max mem: 4023\n",
      "Epoch: [43/100]  [ 20/189]  eta: 0:02:01  loss: 6.461486 (6.461490)  lr: 0.000426 (0.000426)  wd: 0.000005 (0.000005)  time: 0.519768  data: 0.001624  max mem: 4023\n",
      "Epoch: [43/100]  [ 40/189]  eta: 0:01:32  loss: 6.461474 (6.461483)  lr: 0.000425 (0.000425)  wd: 0.000005 (0.000005)  time: 0.516521  data: 0.001100  max mem: 4023\n",
      "Epoch: [43/100]  [ 60/189]  eta: 0:01:15  loss: 6.461473 (6.461480)  lr: 0.000425 (0.000425)  wd: 0.000005 (0.000005)  time: 0.512421  data: 0.000145  max mem: 4023\n",
      "Epoch: [43/100]  [ 80/189]  eta: 0:01:01  loss: 6.461473 (6.461478)  lr: 0.000424 (0.000425)  wd: 0.000005 (0.000005)  time: 0.520425  data: 0.000211  max mem: 4023\n",
      "Epoch: [43/100]  [100/189]  eta: 0:00:49  loss: 6.461474 (6.461478)  lr: 0.000424 (0.000425)  wd: 0.000005 (0.000005)  time: 0.523716  data: 0.000396  max mem: 4023\n",
      "Epoch: [43/100]  [120/189]  eta: 0:00:38  loss: 6.461478 (6.461479)  lr: 0.000423 (0.000425)  wd: 0.000005 (0.000005)  time: 0.512014  data: 0.000147  max mem: 4023\n",
      "Epoch: [43/100]  [140/189]  eta: 0:00:26  loss: 6.461475 (6.461479)  lr: 0.000423 (0.000424)  wd: 0.000005 (0.000005)  time: 0.511611  data: 0.000183  max mem: 4023\n",
      "Epoch: [43/100]  [160/189]  eta: 0:00:15  loss: 6.461475 (6.461479)  lr: 0.000423 (0.000424)  wd: 0.000005 (0.000005)  time: 0.528546  data: 0.000254  max mem: 4023\n",
      "Epoch: [43/100]  [180/189]  eta: 0:00:04  loss: 6.461482 (6.461480)  lr: 0.000422 (0.000424)  wd: 0.000005 (0.000005)  time: 0.514698  data: 0.000110  max mem: 4023\n",
      "Epoch: [43/100]  [188/189]  eta: 0:00:00  loss: 6.461483 (6.461481)  lr: 0.000422 (0.000424)  wd: 0.000005 (0.000005)  time: 0.503917  data: 0.000074  max mem: 4023\n",
      "Epoch: [43/100] Total time: 0:01:41 (0.539259 s / it)\n",
      "Averaged stats: loss: 6.461483 (6.461481)  lr: 0.000422 (0.000424)  wd: 0.000005 (0.000005)\n",
      "Epoch: [44/100]  [  0/189]  eta: 0:10:26  loss: 6.461502 (6.461502)  lr: 0.000422 (0.000422)  wd: 0.000005 (0.000005)  time: 3.313147  data: 2.664111  max mem: 4023\n",
      "Epoch: [44/100]  [ 20/189]  eta: 0:01:50  loss: 6.461490 (6.461603)  lr: 0.000422 (0.000422)  wd: 0.000005 (0.000005)  time: 0.522518  data: 0.000834  max mem: 4023\n",
      "Epoch: [44/100]  [ 40/189]  eta: 0:01:27  loss: 6.461486 (6.461552)  lr: 0.000421 (0.000421)  wd: 0.000005 (0.000005)  time: 0.519859  data: 0.000178  max mem: 4023\n",
      "Epoch: [44/100]  [ 60/189]  eta: 0:01:12  loss: 6.461485 (6.461533)  lr: 0.000421 (0.000421)  wd: 0.000005 (0.000005)  time: 0.511266  data: 0.000262  max mem: 4023\n",
      "Epoch: [44/100]  [ 80/189]  eta: 0:01:00  loss: 6.461588 (6.461557)  lr: 0.000420 (0.000421)  wd: 0.000005 (0.000005)  time: 0.517185  data: 0.000145  max mem: 4023\n",
      "Epoch: [44/100]  [100/189]  eta: 0:00:48  loss: 6.461502 (6.461548)  lr: 0.000420 (0.000421)  wd: 0.000005 (0.000005)  time: 0.518636  data: 0.001945  max mem: 4023\n",
      "Epoch: [44/100]  [120/189]  eta: 0:00:37  loss: 6.461477 (6.461536)  lr: 0.000419 (0.000421)  wd: 0.000005 (0.000005)  time: 0.522221  data: 0.000326  max mem: 4023\n",
      "Epoch: [44/100]  [140/189]  eta: 0:00:26  loss: 6.461475 (6.461528)  lr: 0.000419 (0.000420)  wd: 0.000005 (0.000005)  time: 0.517069  data: 0.000171  max mem: 4023\n",
      "Epoch: [44/100]  [160/189]  eta: 0:00:15  loss: 6.461475 (6.461571)  lr: 0.000419 (0.000420)  wd: 0.000005 (0.000005)  time: 0.531190  data: 0.000230  max mem: 4023\n",
      "Epoch: [44/100]  [180/189]  eta: 0:00:04  loss: 6.461686 (6.461589)  lr: 0.000418 (0.000420)  wd: 0.000005 (0.000005)  time: 0.508464  data: 0.000137  max mem: 4023\n",
      "Epoch: [44/100]  [188/189]  eta: 0:00:00  loss: 6.461553 (6.461586)  lr: 0.000418 (0.000420)  wd: 0.000005 (0.000005)  time: 0.502997  data: 0.000096  max mem: 4023\n",
      "Epoch: [44/100] Total time: 0:01:40 (0.532971 s / it)\n",
      "Averaged stats: loss: 6.461553 (6.461586)  lr: 0.000418 (0.000420)  wd: 0.000005 (0.000005)\n",
      "Epoch: [45/100]  [  0/189]  eta: 0:12:35  loss: 6.461501 (6.461501)  lr: 0.000418 (0.000418)  wd: 0.000005 (0.000005)  time: 3.996310  data: 3.331096  max mem: 4023\n",
      "Epoch: [45/100]  [ 20/189]  eta: 0:01:56  loss: 6.461483 (6.461489)  lr: 0.000418 (0.000418)  wd: 0.000005 (0.000005)  time: 0.524146  data: 0.001095  max mem: 4023\n",
      "Epoch: [45/100]  [ 40/189]  eta: 0:01:30  loss: 6.461474 (6.461481)  lr: 0.000417 (0.000417)  wd: 0.000005 (0.000005)  time: 0.518785  data: 0.000321  max mem: 4023\n",
      "Epoch: [45/100]  [ 60/189]  eta: 0:01:14  loss: 6.461477 (6.461480)  lr: 0.000417 (0.000417)  wd: 0.000005 (0.000005)  time: 0.509714  data: 0.001365  max mem: 4023\n",
      "Epoch: [45/100]  [ 80/189]  eta: 0:01:01  loss: 6.461473 (6.461479)  lr: 0.000416 (0.000417)  wd: 0.000005 (0.000005)  time: 0.517974  data: 0.001654  max mem: 4023\n",
      "Epoch: [45/100]  [100/189]  eta: 0:00:49  loss: 6.461473 (6.461478)  lr: 0.000416 (0.000417)  wd: 0.000005 (0.000005)  time: 0.511530  data: 0.000222  max mem: 4023\n",
      "Epoch: [45/100]  [120/189]  eta: 0:00:37  loss: 6.461473 (6.461477)  lr: 0.000415 (0.000416)  wd: 0.000005 (0.000005)  time: 0.512823  data: 0.000159  max mem: 4023\n",
      "Epoch: [45/100]  [140/189]  eta: 0:00:26  loss: 6.461474 (6.461477)  lr: 0.000415 (0.000416)  wd: 0.000005 (0.000005)  time: 0.516238  data: 0.000147  max mem: 4023\n",
      "Epoch: [45/100]  [160/189]  eta: 0:00:15  loss: 6.461474 (6.461476)  lr: 0.000414 (0.000416)  wd: 0.000005 (0.000005)  time: 0.523334  data: 0.001293  max mem: 4023\n",
      "Epoch: [45/100]  [180/189]  eta: 0:00:04  loss: 6.461503 (6.461480)  lr: 0.000414 (0.000416)  wd: 0.000005 (0.000005)  time: 0.526430  data: 0.000116  max mem: 4023\n",
      "Epoch: [45/100]  [188/189]  eta: 0:00:00  loss: 6.461503 (6.461480)  lr: 0.000414 (0.000416)  wd: 0.000005 (0.000005)  time: 0.519137  data: 0.000079  max mem: 4023\n",
      "Epoch: [45/100] Total time: 0:01:41 (0.536246 s / it)\n",
      "Averaged stats: loss: 6.461503 (6.461480)  lr: 0.000414 (0.000416)  wd: 0.000005 (0.000005)\n",
      "Epoch: [46/100]  [  0/189]  eta: 0:12:41  loss: 6.461494 (6.461494)  lr: 0.000414 (0.000414)  wd: 0.000005 (0.000005)  time: 4.028281  data: 3.443356  max mem: 4023\n",
      "Epoch: [46/100]  [ 20/189]  eta: 0:02:00  loss: 6.461486 (6.461550)  lr: 0.000413 (0.000413)  wd: 0.000005 (0.000005)  time: 0.545043  data: 0.025172  max mem: 4023\n",
      "Epoch: [46/100]  [ 40/189]  eta: 0:01:31  loss: 6.461527 (6.461547)  lr: 0.000413 (0.000413)  wd: 0.000005 (0.000005)  time: 0.513504  data: 0.000182  max mem: 4023\n",
      "Epoch: [46/100]  [ 60/189]  eta: 0:01:14  loss: 6.461485 (6.461527)  lr: 0.000413 (0.000413)  wd: 0.000005 (0.000005)  time: 0.510008  data: 0.000149  max mem: 4023\n",
      "Epoch: [46/100]  [ 80/189]  eta: 0:01:01  loss: 6.461479 (6.461541)  lr: 0.000412 (0.000413)  wd: 0.000005 (0.000005)  time: 0.525513  data: 0.000347  max mem: 4023\n",
      "Epoch: [46/100]  [100/189]  eta: 0:00:49  loss: 6.461697 (6.461604)  lr: 0.000412 (0.000413)  wd: 0.000005 (0.000005)  time: 0.512754  data: 0.000172  max mem: 4023\n",
      "Epoch: [46/100]  [120/189]  eta: 0:00:37  loss: 6.461645 (6.461616)  lr: 0.000411 (0.000412)  wd: 0.000005 (0.000005)  time: 0.511541  data: 0.000507  max mem: 4023\n",
      "Epoch: [46/100]  [140/189]  eta: 0:00:26  loss: 6.461508 (6.461603)  lr: 0.000411 (0.000412)  wd: 0.000005 (0.000005)  time: 0.530187  data: 0.000479  max mem: 4023\n",
      "Epoch: [46/100]  [160/189]  eta: 0:00:15  loss: 6.461477 (6.461588)  lr: 0.000410 (0.000412)  wd: 0.000005 (0.000005)  time: 0.518814  data: 0.000137  max mem: 4023\n",
      "Epoch: [46/100]  [180/189]  eta: 0:00:04  loss: 6.461475 (6.461576)  lr: 0.000410 (0.000412)  wd: 0.000005 (0.000005)  time: 0.520978  data: 0.000128  max mem: 4023\n",
      "Epoch: [46/100]  [188/189]  eta: 0:00:00  loss: 6.461475 (6.461574)  lr: 0.000410 (0.000412)  wd: 0.000005 (0.000005)  time: 0.514532  data: 0.000086  max mem: 4023\n",
      "Epoch: [46/100] Total time: 0:01:41 (0.538806 s / it)\n",
      "Averaged stats: loss: 6.461475 (6.461574)  lr: 0.000410 (0.000412)  wd: 0.000005 (0.000005)\n",
      "Epoch: [47/100]  [  0/189]  eta: 0:14:45  loss: 6.461783 (6.461783)  lr: 0.000409 (0.000409)  wd: 0.000005 (0.000005)  time: 4.682605  data: 4.093623  max mem: 4023\n",
      "Epoch: [47/100]  [ 20/189]  eta: 0:02:00  loss: 6.461598 (6.461652)  lr: 0.000409 (0.000409)  wd: 0.000005 (0.000005)  time: 0.513183  data: 0.000257  max mem: 4023\n",
      "Epoch: [47/100]  [ 40/189]  eta: 0:01:31  loss: 6.461580 (6.461633)  lr: 0.000409 (0.000409)  wd: 0.000005 (0.000005)  time: 0.516462  data: 0.000418  max mem: 4023\n",
      "Epoch: [47/100]  [ 60/189]  eta: 0:01:15  loss: 6.461533 (6.461611)  lr: 0.000408 (0.000409)  wd: 0.000005 (0.000005)  time: 0.521411  data: 0.000189  max mem: 4023\n",
      "Epoch: [47/100]  [ 80/189]  eta: 0:01:01  loss: 6.461492 (6.461582)  lr: 0.000408 (0.000409)  wd: 0.000005 (0.000005)  time: 0.512529  data: 0.000148  max mem: 4023\n",
      "Epoch: [47/100]  [100/189]  eta: 0:00:49  loss: 6.461477 (6.461621)  lr: 0.000407 (0.000408)  wd: 0.000005 (0.000005)  time: 0.517965  data: 0.000172  max mem: 4023\n",
      "Epoch: [47/100]  [120/189]  eta: 0:00:38  loss: 6.461793 (6.461667)  lr: 0.000407 (0.000408)  wd: 0.000005 (0.000005)  time: 0.521919  data: 0.000150  max mem: 4023\n",
      "Epoch: [47/100]  [140/189]  eta: 0:00:26  loss: 6.461521 (6.461652)  lr: 0.000407 (0.000408)  wd: 0.000005 (0.000005)  time: 0.510739  data: 0.000491  max mem: 4023\n",
      "Epoch: [47/100]  [160/189]  eta: 0:00:15  loss: 6.461484 (6.461632)  lr: 0.000406 (0.000408)  wd: 0.000005 (0.000005)  time: 0.514963  data: 0.000477  max mem: 4023\n",
      "Epoch: [47/100]  [180/189]  eta: 0:00:04  loss: 6.461504 (6.461619)  lr: 0.000406 (0.000407)  wd: 0.000005 (0.000005)  time: 0.518334  data: 0.000132  max mem: 4023\n",
      "Epoch: [47/100]  [188/189]  eta: 0:00:00  loss: 6.461500 (6.461614)  lr: 0.000405 (0.000407)  wd: 0.000005 (0.000005)  time: 0.516623  data: 0.000084  max mem: 4023\n",
      "Epoch: [47/100] Total time: 0:01:41 (0.539355 s / it)\n",
      "Averaged stats: loss: 6.461500 (6.461614)  lr: 0.000405 (0.000407)  wd: 0.000005 (0.000005)\n",
      "Epoch: [48/100]  [  0/189]  eta: 0:13:04  loss: 6.461483 (6.461483)  lr: 0.000405 (0.000405)  wd: 0.000005 (0.000005)  time: 4.151352  data: 3.564033  max mem: 4023\n",
      "Epoch: [48/100]  [ 20/189]  eta: 0:01:56  loss: 6.461476 (6.461478)  lr: 0.000405 (0.000405)  wd: 0.000005 (0.000005)  time: 0.516838  data: 0.001752  max mem: 4023\n",
      "Epoch: [48/100]  [ 40/189]  eta: 0:01:29  loss: 6.461472 (6.461475)  lr: 0.000405 (0.000405)  wd: 0.000005 (0.000005)  time: 0.508131  data: 0.000473  max mem: 4023\n",
      "Epoch: [48/100]  [ 60/189]  eta: 0:01:13  loss: 6.461471 (6.461474)  lr: 0.000404 (0.000405)  wd: 0.000005 (0.000005)  time: 0.512105  data: 0.000162  max mem: 4023\n",
      "Epoch: [48/100]  [ 80/189]  eta: 0:01:01  loss: 6.461473 (6.461474)  lr: 0.000404 (0.000404)  wd: 0.000005 (0.000005)  time: 0.527253  data: 0.000318  max mem: 4023\n",
      "Epoch: [48/100]  [100/189]  eta: 0:00:49  loss: 6.461471 (6.461475)  lr: 0.000403 (0.000404)  wd: 0.000005 (0.000005)  time: 0.518007  data: 0.001219  max mem: 4023\n",
      "Epoch: [48/100]  [120/189]  eta: 0:00:37  loss: 6.461489 (6.461479)  lr: 0.000403 (0.000404)  wd: 0.000005 (0.000005)  time: 0.530032  data: 0.000143  max mem: 4023\n",
      "Epoch: [48/100]  [140/189]  eta: 0:00:26  loss: 6.461479 (6.461479)  lr: 0.000402 (0.000404)  wd: 0.000005 (0.000005)  time: 0.518170  data: 0.000294  max mem: 4023\n",
      "Epoch: [48/100]  [160/189]  eta: 0:00:15  loss: 6.461473 (6.461479)  lr: 0.000402 (0.000403)  wd: 0.000005 (0.000005)  time: 0.525543  data: 0.000313  max mem: 4023\n",
      "Epoch: [48/100]  [180/189]  eta: 0:00:04  loss: 6.461502 (6.461482)  lr: 0.000401 (0.000403)  wd: 0.000005 (0.000005)  time: 0.519239  data: 0.000128  max mem: 4023\n",
      "Epoch: [48/100]  [188/189]  eta: 0:00:00  loss: 6.461486 (6.461482)  lr: 0.000401 (0.000403)  wd: 0.000005 (0.000005)  time: 0.506160  data: 0.000089  max mem: 4023\n",
      "Epoch: [48/100] Total time: 0:01:41 (0.538057 s / it)\n",
      "Averaged stats: loss: 6.461486 (6.461482)  lr: 0.000401 (0.000403)  wd: 0.000005 (0.000005)\n",
      "Epoch: [49/100]  [  0/189]  eta: 0:10:25  loss: 6.461475 (6.461475)  lr: 0.000401 (0.000401)  wd: 0.000005 (0.000005)  time: 3.308580  data: 2.659520  max mem: 4023\n",
      "Epoch: [49/100]  [ 20/189]  eta: 0:01:55  loss: 6.461474 (6.461478)  lr: 0.000401 (0.000401)  wd: 0.000005 (0.000005)  time: 0.551710  data: 0.029227  max mem: 4023\n",
      "Epoch: [49/100]  [ 40/189]  eta: 0:01:29  loss: 6.461484 (6.461484)  lr: 0.000400 (0.000401)  wd: 0.000005 (0.000005)  time: 0.516737  data: 0.002011  max mem: 4023\n",
      "Epoch: [49/100]  [ 60/189]  eta: 0:01:14  loss: 6.461500 (6.461498)  lr: 0.000400 (0.000400)  wd: 0.000005 (0.000005)  time: 0.524492  data: 0.000474  max mem: 4023\n",
      "Epoch: [49/100]  [ 80/189]  eta: 0:01:01  loss: 6.461486 (6.461504)  lr: 0.000399 (0.000400)  wd: 0.000005 (0.000005)  time: 0.511596  data: 0.000297  max mem: 4023\n",
      "Epoch: [49/100]  [100/189]  eta: 0:00:49  loss: 6.461482 (6.461506)  lr: 0.000399 (0.000400)  wd: 0.000005 (0.000005)  time: 0.511668  data: 0.000157  max mem: 4023\n",
      "Epoch: [49/100]  [120/189]  eta: 0:00:37  loss: 6.461484 (6.461503)  lr: 0.000398 (0.000400)  wd: 0.000005 (0.000005)  time: 0.523329  data: 0.000943  max mem: 4023\n",
      "Epoch: [49/100]  [140/189]  eta: 0:00:26  loss: 6.461481 (6.461500)  lr: 0.000398 (0.000399)  wd: 0.000005 (0.000005)  time: 0.511157  data: 0.000154  max mem: 4023\n",
      "Epoch: [49/100]  [160/189]  eta: 0:00:15  loss: 6.461989 (6.461587)  lr: 0.000398 (0.000399)  wd: 0.000005 (0.000005)  time: 0.511125  data: 0.000147  max mem: 4023\n",
      "Epoch: [49/100]  [180/189]  eta: 0:00:04  loss: 6.461601 (6.461592)  lr: 0.000397 (0.000399)  wd: 0.000005 (0.000005)  time: 0.517771  data: 0.000276  max mem: 4023\n",
      "Epoch: [49/100]  [188/189]  eta: 0:00:00  loss: 6.461569 (6.461628)  lr: 0.000397 (0.000399)  wd: 0.000005 (0.000005)  time: 0.510111  data: 0.000071  max mem: 4023\n",
      "Epoch: [49/100] Total time: 0:01:41 (0.534634 s / it)\n",
      "Averaged stats: loss: 6.461569 (6.461628)  lr: 0.000397 (0.000399)  wd: 0.000005 (0.000005)\n",
      "Epoch: [50/100]  [  0/189]  eta: 0:11:30  loss: 6.462209 (6.462209)  lr: 0.000397 (0.000397)  wd: 0.000006 (0.000006)  time: 3.654578  data: 3.008843  max mem: 4023\n",
      "Epoch: [50/100]  [ 20/189]  eta: 0:01:54  loss: 6.461695 (6.461772)  lr: 0.000396 (0.000396)  wd: 0.000006 (0.000006)  time: 0.525865  data: 0.001071  max mem: 4023\n",
      "Epoch: [50/100]  [ 40/189]  eta: 0:01:30  loss: 6.461518 (6.461657)  lr: 0.000396 (0.000396)  wd: 0.000006 (0.000006)  time: 0.530564  data: 0.001129  max mem: 4023\n",
      "Epoch: [50/100]  [ 60/189]  eta: 0:01:13  loss: 6.461487 (6.461603)  lr: 0.000396 (0.000396)  wd: 0.000006 (0.000006)  time: 0.509197  data: 0.000160  max mem: 4023\n",
      "Epoch: [50/100]  [ 80/189]  eta: 0:01:00  loss: 6.461478 (6.461572)  lr: 0.000395 (0.000396)  wd: 0.000006 (0.000006)  time: 0.511932  data: 0.000179  max mem: 4023\n",
      "Epoch: [50/100]  [100/189]  eta: 0:00:49  loss: 6.461472 (6.461555)  lr: 0.000395 (0.000396)  wd: 0.000006 (0.000006)  time: 0.520575  data: 0.000372  max mem: 4023\n",
      "Epoch: [50/100]  [120/189]  eta: 0:00:37  loss: 6.461472 (6.461541)  lr: 0.000394 (0.000395)  wd: 0.000006 (0.000006)  time: 0.517622  data: 0.000310  max mem: 4023\n",
      "Epoch: [50/100]  [140/189]  eta: 0:00:26  loss: 6.461473 (6.461532)  lr: 0.000394 (0.000395)  wd: 0.000006 (0.000006)  time: 0.513471  data: 0.000135  max mem: 4023\n",
      "Epoch: [50/100]  [160/189]  eta: 0:00:15  loss: 6.461473 (6.461525)  lr: 0.000393 (0.000395)  wd: 0.000006 (0.000006)  time: 0.517947  data: 0.000164  max mem: 4023\n",
      "Epoch: [50/100]  [180/189]  eta: 0:00:04  loss: 6.461476 (6.461520)  lr: 0.000393 (0.000395)  wd: 0.000006 (0.000006)  time: 0.518827  data: 0.000163  max mem: 4023\n",
      "Epoch: [50/100]  [188/189]  eta: 0:00:00  loss: 6.461477 (6.461523)  lr: 0.000393 (0.000395)  wd: 0.000006 (0.000006)  time: 0.506800  data: 0.000081  max mem: 4023\n",
      "Epoch: [50/100] Total time: 0:01:41 (0.534591 s / it)\n",
      "Averaged stats: loss: 6.461477 (6.461523)  lr: 0.000393 (0.000395)  wd: 0.000006 (0.000006)\n",
      "Epoch: [51/100]  [  0/189]  eta: 0:11:46  loss: 6.461492 (6.461492)  lr: 0.000392 (0.000392)  wd: 0.000006 (0.000006)  time: 3.738814  data: 3.075741  max mem: 4023\n",
      "Epoch: [51/100]  [ 20/189]  eta: 0:01:53  loss: 6.461485 (6.461495)  lr: 0.000392 (0.000392)  wd: 0.000006 (0.000006)  time: 0.521141  data: 0.000781  max mem: 4023\n",
      "Epoch: [51/100]  [ 40/189]  eta: 0:01:29  loss: 6.461479 (6.461489)  lr: 0.000392 (0.000392)  wd: 0.000006 (0.000006)  time: 0.517326  data: 0.000882  max mem: 4023\n",
      "Epoch: [51/100]  [ 60/189]  eta: 0:01:14  loss: 6.461473 (6.461484)  lr: 0.000391 (0.000392)  wd: 0.000006 (0.000006)  time: 0.532087  data: 0.000610  max mem: 4023\n",
      "Epoch: [51/100]  [ 80/189]  eta: 0:01:01  loss: 6.461472 (6.461481)  lr: 0.000391 (0.000391)  wd: 0.000006 (0.000006)  time: 0.511507  data: 0.000590  max mem: 4023\n",
      "Epoch: [51/100]  [100/189]  eta: 0:00:49  loss: 6.461473 (6.461479)  lr: 0.000390 (0.000391)  wd: 0.000006 (0.000006)  time: 0.512783  data: 0.000205  max mem: 4023\n",
      "Epoch: [51/100]  [120/189]  eta: 0:00:37  loss: 6.461488 (6.461481)  lr: 0.000390 (0.000391)  wd: 0.000006 (0.000006)  time: 0.523288  data: 0.000342  max mem: 4023\n",
      "Epoch: [51/100]  [140/189]  eta: 0:00:26  loss: 6.461475 (6.461481)  lr: 0.000389 (0.000391)  wd: 0.000006 (0.000006)  time: 0.519191  data: 0.000149  max mem: 4023\n",
      "Epoch: [51/100]  [160/189]  eta: 0:00:15  loss: 6.461472 (6.461480)  lr: 0.000389 (0.000391)  wd: 0.000006 (0.000006)  time: 0.527281  data: 0.000933  max mem: 4023\n",
      "Epoch: [51/100]  [180/189]  eta: 0:00:04  loss: 6.461485 (6.461494)  lr: 0.000388 (0.000390)  wd: 0.000006 (0.000006)  time: 0.519802  data: 0.000120  max mem: 4023\n",
      "Epoch: [51/100]  [188/189]  eta: 0:00:00  loss: 6.461491 (6.461495)  lr: 0.000388 (0.000390)  wd: 0.000006 (0.000006)  time: 0.510048  data: 0.000087  max mem: 4023\n",
      "Epoch: [51/100] Total time: 0:01:41 (0.536921 s / it)\n",
      "Averaged stats: loss: 6.461491 (6.461495)  lr: 0.000388 (0.000390)  wd: 0.000006 (0.000006)\n",
      "Epoch: [52/100]  [  0/189]  eta: 0:15:49  loss: 6.461529 (6.461529)  lr: 0.000388 (0.000388)  wd: 0.000006 (0.000006)  time: 5.021544  data: 4.437652  max mem: 4023\n",
      "Epoch: [52/100]  [ 20/189]  eta: 0:02:03  loss: 6.462045 (6.462216)  lr: 0.000388 (0.000388)  wd: 0.000006 (0.000006)  time: 0.514764  data: 0.001332  max mem: 4023\n",
      "Epoch: [52/100]  [ 40/189]  eta: 0:01:32  loss: 6.461665 (6.462006)  lr: 0.000387 (0.000388)  wd: 0.000006 (0.000006)  time: 0.511817  data: 0.000853  max mem: 4023\n",
      "Epoch: [52/100]  [ 60/189]  eta: 0:01:16  loss: 6.461536 (6.461853)  lr: 0.000387 (0.000387)  wd: 0.000006 (0.000006)  time: 0.535672  data: 0.000509  max mem: 4023\n",
      "Epoch: [52/100]  [ 80/189]  eta: 0:01:02  loss: 6.461481 (6.461762)  lr: 0.000386 (0.000387)  wd: 0.000006 (0.000006)  time: 0.512581  data: 0.000211  max mem: 4023\n",
      "Epoch: [52/100]  [100/189]  eta: 0:00:50  loss: 6.461563 (6.461726)  lr: 0.000386 (0.000387)  wd: 0.000006 (0.000006)  time: 0.516643  data: 0.000277  max mem: 4023\n",
      "Epoch: [52/100]  [120/189]  eta: 0:00:38  loss: 6.461519 (6.461692)  lr: 0.000386 (0.000387)  wd: 0.000006 (0.000006)  time: 0.513335  data: 0.000158  max mem: 4023\n",
      "Epoch: [52/100]  [140/189]  eta: 0:00:26  loss: 6.461504 (6.461667)  lr: 0.000385 (0.000386)  wd: 0.000006 (0.000006)  time: 0.525562  data: 0.000129  max mem: 4023\n",
      "Epoch: [52/100]  [160/189]  eta: 0:00:15  loss: 6.461476 (6.461644)  lr: 0.000385 (0.000386)  wd: 0.000006 (0.000006)  time: 0.512752  data: 0.000146  max mem: 4023\n",
      "Epoch: [52/100]  [180/189]  eta: 0:00:04  loss: 6.461472 (6.461626)  lr: 0.000384 (0.000386)  wd: 0.000006 (0.000006)  time: 0.510037  data: 0.000111  max mem: 4023\n",
      "Epoch: [52/100]  [188/189]  eta: 0:00:00  loss: 6.461493 (6.461632)  lr: 0.000384 (0.000386)  wd: 0.000006 (0.000006)  time: 0.516761  data: 0.000080  max mem: 4023\n",
      "Epoch: [52/100] Total time: 0:01:42 (0.541535 s / it)\n",
      "Averaged stats: loss: 6.461493 (6.461632)  lr: 0.000384 (0.000386)  wd: 0.000006 (0.000006)\n",
      "Epoch: [53/100]  [  0/189]  eta: 0:13:47  loss: 6.461763 (6.461763)  lr: 0.000384 (0.000384)  wd: 0.000006 (0.000006)  time: 4.378607  data: 3.755930  max mem: 4023\n",
      "Epoch: [53/100]  [ 20/189]  eta: 0:01:58  loss: 6.461644 (6.461658)  lr: 0.000383 (0.000383)  wd: 0.000006 (0.000006)  time: 0.518930  data: 0.001207  max mem: 4023\n",
      "Epoch: [53/100]  [ 40/189]  eta: 0:01:31  loss: 6.461588 (6.461642)  lr: 0.000383 (0.000383)  wd: 0.000006 (0.000006)  time: 0.518676  data: 0.000420  max mem: 4023\n",
      "Epoch: [53/100]  [ 60/189]  eta: 0:01:14  loss: 6.461487 (6.461593)  lr: 0.000383 (0.000383)  wd: 0.000006 (0.000006)  time: 0.509939  data: 0.000225  max mem: 4023\n",
      "Epoch: [53/100]  [ 80/189]  eta: 0:01:01  loss: 6.461481 (6.461844)  lr: 0.000382 (0.000383)  wd: 0.000006 (0.000006)  time: 0.518758  data: 0.000159  max mem: 4023\n",
      "Epoch: [53/100]  [100/189]  eta: 0:00:49  loss: 6.462078 (6.461943)  lr: 0.000382 (0.000383)  wd: 0.000006 (0.000006)  time: 0.513257  data: 0.000242  max mem: 4023\n",
      "Epoch: [53/100]  [120/189]  eta: 0:00:37  loss: 6.461541 (6.461880)  lr: 0.000381 (0.000382)  wd: 0.000006 (0.000006)  time: 0.530450  data: 0.000146  max mem: 4023\n",
      "Epoch: [53/100]  [140/189]  eta: 0:00:26  loss: 6.461499 (6.461826)  lr: 0.000381 (0.000382)  wd: 0.000006 (0.000006)  time: 0.519109  data: 0.000533  max mem: 4023\n",
      "Epoch: [53/100]  [160/189]  eta: 0:00:15  loss: 6.461627 (6.461807)  lr: 0.000380 (0.000382)  wd: 0.000006 (0.000006)  time: 0.524755  data: 0.000875  max mem: 4023\n",
      "Epoch: [53/100]  [180/189]  eta: 0:00:04  loss: 6.461505 (6.461775)  lr: 0.000380 (0.000382)  wd: 0.000006 (0.000006)  time: 0.512918  data: 0.000768  max mem: 4023\n",
      "Epoch: [53/100]  [188/189]  eta: 0:00:00  loss: 6.461489 (6.461762)  lr: 0.000380 (0.000382)  wd: 0.000006 (0.000006)  time: 0.504202  data: 0.000075  max mem: 4023\n",
      "Epoch: [53/100] Total time: 0:01:41 (0.538680 s / it)\n",
      "Averaged stats: loss: 6.461489 (6.461762)  lr: 0.000380 (0.000382)  wd: 0.000006 (0.000006)\n",
      "Epoch: [54/100]  [  0/189]  eta: 0:14:37  loss: 6.461477 (6.461477)  lr: 0.000379 (0.000379)  wd: 0.000006 (0.000006)  time: 4.642873  data: 4.049054  max mem: 4023\n",
      "Epoch: [54/100]  [ 20/189]  eta: 0:02:03  loss: 6.461474 (6.461476)  lr: 0.000379 (0.000379)  wd: 0.000006 (0.000006)  time: 0.535240  data: 0.019556  max mem: 4023\n",
      "Epoch: [54/100]  [ 40/189]  eta: 0:01:33  loss: 6.461472 (6.461475)  lr: 0.000379 (0.000379)  wd: 0.000006 (0.000006)  time: 0.513452  data: 0.000168  max mem: 4023\n",
      "Epoch: [54/100]  [ 60/189]  eta: 0:01:16  loss: 6.461471 (6.461474)  lr: 0.000378 (0.000379)  wd: 0.000006 (0.000006)  time: 0.519356  data: 0.000427  max mem: 4023\n",
      "Epoch: [54/100]  [ 80/189]  eta: 0:01:02  loss: 6.461472 (6.461474)  lr: 0.000378 (0.000378)  wd: 0.000006 (0.000006)  time: 0.510297  data: 0.000152  max mem: 4023\n",
      "Epoch: [54/100]  [100/189]  eta: 0:00:49  loss: 6.461472 (6.461475)  lr: 0.000377 (0.000378)  wd: 0.000006 (0.000006)  time: 0.512998  data: 0.000316  max mem: 4023\n",
      "Epoch: [54/100]  [120/189]  eta: 0:00:38  loss: 6.461475 (6.461475)  lr: 0.000377 (0.000378)  wd: 0.000006 (0.000006)  time: 0.518905  data: 0.000158  max mem: 4023\n",
      "Epoch: [54/100]  [140/189]  eta: 0:00:26  loss: 6.461478 (6.461477)  lr: 0.000376 (0.000378)  wd: 0.000006 (0.000006)  time: 0.509934  data: 0.000158  max mem: 4023\n",
      "Epoch: [54/100]  [160/189]  eta: 0:00:15  loss: 6.461486 (6.461480)  lr: 0.000376 (0.000378)  wd: 0.000006 (0.000006)  time: 0.530240  data: 0.000303  max mem: 4023\n",
      "Epoch: [54/100]  [180/189]  eta: 0:00:04  loss: 6.461918 (6.461536)  lr: 0.000375 (0.000377)  wd: 0.000006 (0.000006)  time: 0.518201  data: 0.000117  max mem: 4023\n",
      "Epoch: [54/100]  [188/189]  eta: 0:00:00  loss: 6.461974 (6.461556)  lr: 0.000375 (0.000377)  wd: 0.000006 (0.000006)  time: 0.509920  data: 0.000087  max mem: 4023\n",
      "Epoch: [54/100] Total time: 0:01:42 (0.540156 s / it)\n",
      "Averaged stats: loss: 6.461974 (6.461556)  lr: 0.000375 (0.000377)  wd: 0.000006 (0.000006)\n",
      "Epoch: [55/100]  [  0/189]  eta: 0:13:21  loss: 6.461734 (6.461734)  lr: 0.000375 (0.000375)  wd: 0.000006 (0.000006)  time: 4.241503  data: 3.622082  max mem: 4023\n",
      "Epoch: [55/100]  [ 20/189]  eta: 0:01:58  loss: 6.461555 (6.461580)  lr: 0.000375 (0.000375)  wd: 0.000006 (0.000006)  time: 0.523369  data: 0.015508  max mem: 4023\n",
      "Epoch: [55/100]  [ 40/189]  eta: 0:01:30  loss: 6.461487 (6.461536)  lr: 0.000374 (0.000375)  wd: 0.000006 (0.000006)  time: 0.511687  data: 0.000185  max mem: 4023\n",
      "Epoch: [55/100]  [ 60/189]  eta: 0:01:14  loss: 6.461529 (6.461542)  lr: 0.000374 (0.000374)  wd: 0.000006 (0.000006)  time: 0.511364  data: 0.000242  max mem: 4023\n",
      "Epoch: [55/100]  [ 80/189]  eta: 0:01:01  loss: 6.461486 (6.461543)  lr: 0.000373 (0.000374)  wd: 0.000006 (0.000006)  time: 0.517656  data: 0.000310  max mem: 4023\n",
      "Epoch: [55/100]  [100/189]  eta: 0:00:49  loss: 6.461518 (6.461540)  lr: 0.000373 (0.000374)  wd: 0.000006 (0.000006)  time: 0.537969  data: 0.000268  max mem: 4023\n",
      "Epoch: [55/100]  [120/189]  eta: 0:00:37  loss: 6.461480 (6.461548)  lr: 0.000372 (0.000374)  wd: 0.000006 (0.000006)  time: 0.516987  data: 0.000144  max mem: 4023\n",
      "Epoch: [55/100]  [140/189]  eta: 0:00:26  loss: 6.461809 (6.461607)  lr: 0.000372 (0.000373)  wd: 0.000006 (0.000006)  time: 0.523453  data: 0.000193  max mem: 4023\n",
      "Epoch: [55/100]  [160/189]  eta: 0:00:15  loss: 6.461520 (6.461597)  lr: 0.000372 (0.000373)  wd: 0.000006 (0.000006)  time: 0.515166  data: 0.000142  max mem: 4023\n",
      "Epoch: [55/100]  [180/189]  eta: 0:00:04  loss: 6.461483 (6.461749)  lr: 0.000371 (0.000373)  wd: 0.000006 (0.000006)  time: 0.513428  data: 0.000142  max mem: 4023\n",
      "Epoch: [55/100]  [188/189]  eta: 0:00:00  loss: 6.461615 (6.461850)  lr: 0.000371 (0.000373)  wd: 0.000006 (0.000006)  time: 0.510808  data: 0.000096  max mem: 4023\n",
      "Epoch: [55/100] Total time: 0:01:41 (0.538491 s / it)\n",
      "Averaged stats: loss: 6.461615 (6.461850)  lr: 0.000371 (0.000373)  wd: 0.000006 (0.000006)\n",
      "Epoch: [56/100]  [  0/189]  eta: 0:14:16  loss: 6.462782 (6.462782)  lr: 0.000371 (0.000371)  wd: 0.000006 (0.000006)  time: 4.531075  data: 3.879987  max mem: 4023\n",
      "Epoch: [56/100]  [ 20/189]  eta: 0:01:59  loss: 6.462451 (6.462475)  lr: 0.000370 (0.000370)  wd: 0.000006 (0.000006)  time: 0.518556  data: 0.002207  max mem: 4023\n",
      "Epoch: [56/100]  [ 40/189]  eta: 0:01:31  loss: 6.461740 (6.462156)  lr: 0.000370 (0.000370)  wd: 0.000006 (0.000006)  time: 0.517252  data: 0.000465  max mem: 4023\n",
      "Epoch: [56/100]  [ 60/189]  eta: 0:01:15  loss: 6.461613 (6.462055)  lr: 0.000369 (0.000370)  wd: 0.000006 (0.000006)  time: 0.529295  data: 0.000569  max mem: 4023\n",
      "Epoch: [56/100]  [ 80/189]  eta: 0:01:02  loss: 6.462571 (6.462368)  lr: 0.000369 (0.000370)  wd: 0.000006 (0.000006)  time: 0.522286  data: 0.000303  max mem: 4023\n",
      "Epoch: [56/100]  [100/189]  eta: 0:00:49  loss: 6.461509 (6.462222)  lr: 0.000369 (0.000369)  wd: 0.000006 (0.000006)  time: 0.520092  data: 0.000372  max mem: 4023\n",
      "Epoch: [56/100]  [120/189]  eta: 0:00:38  loss: 6.461538 (6.462112)  lr: 0.000368 (0.000369)  wd: 0.000006 (0.000006)  time: 0.516093  data: 0.000189  max mem: 4023\n",
      "Epoch: [56/100]  [140/189]  eta: 0:00:26  loss: 6.461482 (6.462023)  lr: 0.000368 (0.000369)  wd: 0.000006 (0.000006)  time: 0.526602  data: 0.000141  max mem: 4023\n",
      "Epoch: [56/100]  [160/189]  eta: 0:00:15  loss: 6.461478 (6.461956)  lr: 0.000367 (0.000369)  wd: 0.000006 (0.000006)  time: 0.529046  data: 0.000235  max mem: 4023\n",
      "Epoch: [56/100]  [180/189]  eta: 0:00:04  loss: 6.461475 (6.461903)  lr: 0.000367 (0.000369)  wd: 0.000006 (0.000006)  time: 0.507882  data: 0.000116  max mem: 4023\n",
      "Epoch: [56/100]  [188/189]  eta: 0:00:00  loss: 6.461475 (6.461885)  lr: 0.000367 (0.000368)  wd: 0.000006 (0.000006)  time: 0.503917  data: 0.000081  max mem: 4023\n",
      "Epoch: [56/100] Total time: 0:01:42 (0.541626 s / it)\n",
      "Averaged stats: loss: 6.461475 (6.461885)  lr: 0.000367 (0.000368)  wd: 0.000006 (0.000006)\n",
      "Epoch: [57/100]  [  0/189]  eta: 0:11:46  loss: 6.461473 (6.461473)  lr: 0.000366 (0.000366)  wd: 0.000006 (0.000006)  time: 3.735695  data: 3.137081  max mem: 4023\n",
      "Epoch: [57/100]  [ 20/189]  eta: 0:01:55  loss: 6.461471 (6.461472)  lr: 0.000366 (0.000366)  wd: 0.000006 (0.000006)  time: 0.529301  data: 0.000890  max mem: 4023\n",
      "Epoch: [57/100]  [ 40/189]  eta: 0:01:29  loss: 6.461474 (6.461473)  lr: 0.000366 (0.000366)  wd: 0.000007 (0.000006)  time: 0.518847  data: 0.000409  max mem: 4023\n",
      "Epoch: [57/100]  [ 60/189]  eta: 0:01:14  loss: 6.461474 (6.461474)  lr: 0.000365 (0.000366)  wd: 0.000007 (0.000007)  time: 0.520408  data: 0.000973  max mem: 4023\n",
      "Epoch: [57/100]  [ 80/189]  eta: 0:01:01  loss: 6.461471 (6.461473)  lr: 0.000365 (0.000365)  wd: 0.000007 (0.000007)  time: 0.523790  data: 0.000381  max mem: 4023\n",
      "Epoch: [57/100]  [100/189]  eta: 0:00:49  loss: 6.461472 (6.461473)  lr: 0.000364 (0.000365)  wd: 0.000007 (0.000007)  time: 0.520932  data: 0.000504  max mem: 4023\n",
      "Epoch: [57/100]  [120/189]  eta: 0:00:37  loss: 6.461472 (6.461473)  lr: 0.000364 (0.000365)  wd: 0.000007 (0.000007)  time: 0.517308  data: 0.000148  max mem: 4023\n",
      "Epoch: [57/100]  [140/189]  eta: 0:00:26  loss: 6.461473 (6.461473)  lr: 0.000363 (0.000365)  wd: 0.000007 (0.000007)  time: 0.534927  data: 0.000141  max mem: 4023\n",
      "Epoch: [57/100]  [160/189]  eta: 0:00:15  loss: 6.461471 (6.461473)  lr: 0.000363 (0.000364)  wd: 0.000007 (0.000007)  time: 0.517994  data: 0.000205  max mem: 4023\n",
      "Epoch: [57/100]  [180/189]  eta: 0:00:04  loss: 6.461471 (6.461473)  lr: 0.000362 (0.000364)  wd: 0.000007 (0.000007)  time: 0.514356  data: 0.000120  max mem: 4023\n",
      "Epoch: [57/100]  [188/189]  eta: 0:00:00  loss: 6.461471 (6.461473)  lr: 0.000362 (0.000364)  wd: 0.000007 (0.000007)  time: 0.507907  data: 0.000075  max mem: 4023\n",
      "Epoch: [57/100] Total time: 0:01:41 (0.538415 s / it)\n",
      "Averaged stats: loss: 6.461471 (6.461473)  lr: 0.000362 (0.000364)  wd: 0.000007 (0.000007)\n",
      "Epoch: [58/100]  [  0/189]  eta: 0:13:34  loss: 6.461472 (6.461472)  lr: 0.000362 (0.000362)  wd: 0.000007 (0.000007)  time: 4.311819  data: 3.708716  max mem: 4023\n",
      "Epoch: [58/100]  [ 20/189]  eta: 0:01:58  loss: 6.461470 (6.461471)  lr: 0.000362 (0.000362)  wd: 0.000007 (0.000007)  time: 0.522673  data: 0.001621  max mem: 4023\n",
      "Epoch: [58/100]  [ 40/189]  eta: 0:01:31  loss: 6.461471 (6.461471)  lr: 0.000361 (0.000361)  wd: 0.000007 (0.000007)  time: 0.515842  data: 0.000845  max mem: 4023\n",
      "Epoch: [58/100]  [ 60/189]  eta: 0:01:14  loss: 6.461473 (6.461472)  lr: 0.000361 (0.000361)  wd: 0.000007 (0.000007)  time: 0.510830  data: 0.000152  max mem: 4023\n",
      "Epoch: [58/100]  [ 80/189]  eta: 0:01:01  loss: 6.461471 (6.461472)  lr: 0.000360 (0.000361)  wd: 0.000007 (0.000007)  time: 0.511720  data: 0.000172  max mem: 4023\n",
      "Epoch: [58/100]  [100/189]  eta: 0:00:49  loss: 6.461470 (6.461471)  lr: 0.000360 (0.000361)  wd: 0.000007 (0.000007)  time: 0.527328  data: 0.000148  max mem: 4023\n",
      "Epoch: [58/100]  [120/189]  eta: 0:00:37  loss: 6.461470 (6.461471)  lr: 0.000359 (0.000361)  wd: 0.000007 (0.000007)  time: 0.524647  data: 0.000151  max mem: 4023\n",
      "Epoch: [58/100]  [140/189]  eta: 0:00:26  loss: 6.461471 (6.461471)  lr: 0.000359 (0.000360)  wd: 0.000007 (0.000007)  time: 0.537457  data: 0.000154  max mem: 4023\n",
      "Epoch: [58/100]  [160/189]  eta: 0:00:15  loss: 6.461470 (6.461471)  lr: 0.000358 (0.000360)  wd: 0.000007 (0.000007)  time: 0.525325  data: 0.000308  max mem: 4023\n",
      "Epoch: [58/100]  [180/189]  eta: 0:00:04  loss: 6.461470 (6.461471)  lr: 0.000358 (0.000360)  wd: 0.000007 (0.000007)  time: 0.509598  data: 0.000127  max mem: 4023\n",
      "Epoch: [58/100]  [188/189]  eta: 0:00:00  loss: 6.461471 (6.461471)  lr: 0.000358 (0.000360)  wd: 0.000007 (0.000007)  time: 0.503515  data: 0.000083  max mem: 4023\n",
      "Epoch: [58/100] Total time: 0:01:42 (0.540057 s / it)\n",
      "Averaged stats: loss: 6.461471 (6.461471)  lr: 0.000358 (0.000360)  wd: 0.000007 (0.000007)\n",
      "Epoch: [59/100]  [  0/189]  eta: 0:11:23  loss: 6.461473 (6.461473)  lr: 0.000358 (0.000358)  wd: 0.000007 (0.000007)  time: 3.614679  data: 3.003688  max mem: 4023\n",
      "Epoch: [59/100]  [ 20/189]  eta: 0:01:55  loss: 6.461472 (6.461473)  lr: 0.000357 (0.000357)  wd: 0.000007 (0.000007)  time: 0.538089  data: 0.016586  max mem: 4023\n",
      "Epoch: [59/100]  [ 40/189]  eta: 0:01:29  loss: 6.461474 (6.461473)  lr: 0.000357 (0.000357)  wd: 0.000007 (0.000007)  time: 0.512173  data: 0.000413  max mem: 4023\n",
      "Epoch: [59/100]  [ 60/189]  eta: 0:01:13  loss: 6.461472 (6.461473)  lr: 0.000356 (0.000357)  wd: 0.000007 (0.000007)  time: 0.513739  data: 0.000398  max mem: 4023\n",
      "Epoch: [59/100]  [ 80/189]  eta: 0:01:00  loss: 6.461472 (6.461473)  lr: 0.000356 (0.000357)  wd: 0.000007 (0.000007)  time: 0.516284  data: 0.000208  max mem: 4023\n",
      "Epoch: [59/100]  [100/189]  eta: 0:00:48  loss: 6.461473 (6.461473)  lr: 0.000356 (0.000356)  wd: 0.000007 (0.000007)  time: 0.517915  data: 0.000187  max mem: 4023\n",
      "Epoch: [59/100]  [120/189]  eta: 0:00:37  loss: 6.461473 (6.461473)  lr: 0.000355 (0.000356)  wd: 0.000007 (0.000007)  time: 0.510988  data: 0.000540  max mem: 4023\n",
      "Epoch: [59/100]  [140/189]  eta: 0:00:26  loss: 6.461473 (6.461473)  lr: 0.000355 (0.000356)  wd: 0.000007 (0.000007)  time: 0.542901  data: 0.000303  max mem: 4023\n",
      "Epoch: [59/100]  [160/189]  eta: 0:00:15  loss: 6.461475 (6.461473)  lr: 0.000354 (0.000356)  wd: 0.000007 (0.000007)  time: 0.517038  data: 0.000133  max mem: 4023\n",
      "Epoch: [59/100]  [180/189]  eta: 0:00:04  loss: 6.461473 (6.461473)  lr: 0.000354 (0.000356)  wd: 0.000007 (0.000007)  time: 0.507794  data: 0.000178  max mem: 4023\n",
      "Epoch: [59/100]  [188/189]  eta: 0:00:00  loss: 6.461472 (6.461473)  lr: 0.000354 (0.000355)  wd: 0.000007 (0.000007)  time: 0.503925  data: 0.000135  max mem: 4023\n",
      "Epoch: [59/100] Total time: 0:01:41 (0.536015 s / it)\n",
      "Averaged stats: loss: 6.461472 (6.461473)  lr: 0.000354 (0.000355)  wd: 0.000007 (0.000007)\n",
      "Epoch: [60/100]  [  0/189]  eta: 0:15:12  loss: 6.461471 (6.461471)  lr: 0.000353 (0.000353)  wd: 0.000007 (0.000007)  time: 4.828066  data: 4.243426  max mem: 4023\n",
      "Epoch: [60/100]  [ 20/189]  eta: 0:02:01  loss: 6.461472 (6.461472)  lr: 0.000353 (0.000353)  wd: 0.000007 (0.000007)  time: 0.516119  data: 0.000941  max mem: 4023\n",
      "Epoch: [60/100]  [ 40/189]  eta: 0:01:31  loss: 6.461472 (6.461473)  lr: 0.000353 (0.000353)  wd: 0.000007 (0.000007)  time: 0.507556  data: 0.000277  max mem: 4023\n",
      "Epoch: [60/100]  [ 60/189]  eta: 0:01:15  loss: 6.461473 (6.461473)  lr: 0.000352 (0.000353)  wd: 0.000007 (0.000007)  time: 0.518584  data: 0.000337  max mem: 4023\n",
      "Epoch: [60/100]  [ 80/189]  eta: 0:01:01  loss: 6.461472 (6.461472)  lr: 0.000352 (0.000352)  wd: 0.000007 (0.000007)  time: 0.519911  data: 0.000426  max mem: 4023\n",
      "Epoch: [60/100]  [100/189]  eta: 0:00:49  loss: 6.461473 (6.461473)  lr: 0.000351 (0.000352)  wd: 0.000007 (0.000007)  time: 0.520644  data: 0.000156  max mem: 4023\n",
      "Epoch: [60/100]  [120/189]  eta: 0:00:38  loss: 6.461474 (6.461473)  lr: 0.000351 (0.000352)  wd: 0.000007 (0.000007)  time: 0.534335  data: 0.000171  max mem: 4023\n",
      "Epoch: [60/100]  [140/189]  eta: 0:00:26  loss: 6.461473 (6.461473)  lr: 0.000350 (0.000352)  wd: 0.000007 (0.000007)  time: 0.515149  data: 0.000291  max mem: 4023\n",
      "Epoch: [60/100]  [160/189]  eta: 0:00:15  loss: 6.461474 (6.461473)  lr: 0.000350 (0.000351)  wd: 0.000007 (0.000007)  time: 0.529059  data: 0.000224  max mem: 4023\n",
      "Epoch: [60/100]  [180/189]  eta: 0:00:04  loss: 6.461473 (6.461473)  lr: 0.000349 (0.000351)  wd: 0.000007 (0.000007)  time: 0.509376  data: 0.000126  max mem: 4023\n",
      "Epoch: [60/100]  [188/189]  eta: 0:00:00  loss: 6.461472 (6.461473)  lr: 0.000349 (0.000351)  wd: 0.000007 (0.000007)  time: 0.505333  data: 0.000078  max mem: 4023\n",
      "Epoch: [60/100] Total time: 0:01:42 (0.541574 s / it)\n",
      "Averaged stats: loss: 6.461472 (6.461473)  lr: 0.000349 (0.000351)  wd: 0.000007 (0.000007)\n",
      "Epoch: [61/100]  [  0/189]  eta: 0:14:39  loss: 6.461475 (6.461475)  lr: 0.000349 (0.000349)  wd: 0.000007 (0.000007)  time: 4.653265  data: 4.080268  max mem: 4023\n",
      "Epoch: [61/100]  [ 20/189]  eta: 0:02:00  loss: 6.461475 (6.461476)  lr: 0.000349 (0.000349)  wd: 0.000007 (0.000007)  time: 0.515849  data: 0.000800  max mem: 4023\n",
      "Epoch: [61/100]  [ 40/189]  eta: 0:01:31  loss: 6.461478 (6.461477)  lr: 0.000348 (0.000349)  wd: 0.000007 (0.000007)  time: 0.511458  data: 0.000171  max mem: 4023\n",
      "Epoch: [61/100]  [ 60/189]  eta: 0:01:14  loss: 6.461474 (6.461476)  lr: 0.000348 (0.000348)  wd: 0.000007 (0.000007)  time: 0.509428  data: 0.000422  max mem: 4023\n",
      "Epoch: [61/100]  [ 80/189]  eta: 0:01:01  loss: 6.461476 (6.461477)  lr: 0.000347 (0.000348)  wd: 0.000007 (0.000007)  time: 0.519085  data: 0.000716  max mem: 4023\n",
      "Epoch: [61/100]  [100/189]  eta: 0:00:49  loss: 6.461473 (6.461476)  lr: 0.000347 (0.000348)  wd: 0.000007 (0.000007)  time: 0.519575  data: 0.000151  max mem: 4023\n",
      "Epoch: [61/100]  [120/189]  eta: 0:00:38  loss: 6.461478 (6.461477)  lr: 0.000347 (0.000348)  wd: 0.000007 (0.000007)  time: 0.524174  data: 0.000797  max mem: 4023\n",
      "Epoch: [61/100]  [140/189]  eta: 0:00:26  loss: 6.461478 (6.461477)  lr: 0.000346 (0.000347)  wd: 0.000007 (0.000007)  time: 0.516447  data: 0.000158  max mem: 4023\n",
      "Epoch: [61/100]  [160/189]  eta: 0:00:15  loss: 6.461481 (6.461477)  lr: 0.000346 (0.000347)  wd: 0.000007 (0.000007)  time: 0.510950  data: 0.000152  max mem: 4023\n",
      "Epoch: [61/100]  [180/189]  eta: 0:00:04  loss: 6.461475 (6.461477)  lr: 0.000345 (0.000347)  wd: 0.000007 (0.000007)  time: 0.512791  data: 0.000114  max mem: 4023\n",
      "Epoch: [61/100]  [188/189]  eta: 0:00:00  loss: 6.461476 (6.461477)  lr: 0.000345 (0.000347)  wd: 0.000007 (0.000007)  time: 0.512336  data: 0.000073  max mem: 4023\n",
      "Epoch: [61/100] Total time: 0:01:41 (0.537799 s / it)\n",
      "Averaged stats: loss: 6.461476 (6.461477)  lr: 0.000345 (0.000347)  wd: 0.000007 (0.000007)\n",
      "Epoch: [62/100]  [  0/189]  eta: 0:12:33  loss: 6.461476 (6.461476)  lr: 0.000345 (0.000345)  wd: 0.000007 (0.000007)  time: 3.984952  data: 3.407625  max mem: 4023\n",
      "Epoch: [62/100]  [ 20/189]  eta: 0:01:55  loss: 6.461474 (6.461475)  lr: 0.000345 (0.000345)  wd: 0.000007 (0.000007)  time: 0.519100  data: 0.000852  max mem: 4023\n",
      "Epoch: [62/100]  [ 40/189]  eta: 0:01:30  loss: 6.461475 (6.461475)  lr: 0.000344 (0.000344)  wd: 0.000007 (0.000007)  time: 0.522097  data: 0.000919  max mem: 4023\n",
      "Epoch: [62/100]  [ 60/189]  eta: 0:01:14  loss: 6.461475 (6.461475)  lr: 0.000344 (0.000344)  wd: 0.000007 (0.000007)  time: 0.519104  data: 0.000570  max mem: 4023\n",
      "Epoch: [62/100]  [ 80/189]  eta: 0:01:01  loss: 6.461477 (6.461476)  lr: 0.000343 (0.000344)  wd: 0.000007 (0.000007)  time: 0.512325  data: 0.000148  max mem: 4023\n",
      "Epoch: [62/100]  [100/189]  eta: 0:00:49  loss: 6.461475 (6.461476)  lr: 0.000343 (0.000344)  wd: 0.000007 (0.000007)  time: 0.513620  data: 0.000538  max mem: 4023\n",
      "Epoch: [62/100]  [120/189]  eta: 0:00:37  loss: 6.461474 (6.461476)  lr: 0.000342 (0.000343)  wd: 0.000007 (0.000007)  time: 0.526130  data: 0.000203  max mem: 4023\n",
      "Epoch: [62/100]  [140/189]  eta: 0:00:26  loss: 6.461475 (6.461476)  lr: 0.000342 (0.000343)  wd: 0.000007 (0.000007)  time: 0.514948  data: 0.000438  max mem: 4023\n",
      "Epoch: [62/100]  [160/189]  eta: 0:00:15  loss: 6.461475 (6.461476)  lr: 0.000341 (0.000343)  wd: 0.000007 (0.000007)  time: 0.520151  data: 0.000163  max mem: 4023\n",
      "Epoch: [62/100]  [180/189]  eta: 0:00:04  loss: 6.461473 (6.461476)  lr: 0.000341 (0.000343)  wd: 0.000007 (0.000007)  time: 0.522613  data: 0.000132  max mem: 4023\n",
      "Epoch: [62/100]  [188/189]  eta: 0:00:00  loss: 6.461474 (6.461476)  lr: 0.000341 (0.000343)  wd: 0.000007 (0.000007)  time: 0.512882  data: 0.000087  max mem: 4023\n",
      "Epoch: [62/100] Total time: 0:01:41 (0.536681 s / it)\n",
      "Averaged stats: loss: 6.461474 (6.461476)  lr: 0.000341 (0.000343)  wd: 0.000007 (0.000007)\n",
      "Epoch: [63/100]  [  0/189]  eta: 0:11:08  loss: 6.461477 (6.461477)  lr: 0.000341 (0.000341)  wd: 0.000007 (0.000007)  time: 3.537970  data: 2.949463  max mem: 4023\n",
      "Epoch: [63/100]  [ 20/189]  eta: 0:01:53  loss: 6.461477 (6.461477)  lr: 0.000340 (0.000340)  wd: 0.000007 (0.000007)  time: 0.530867  data: 0.000602  max mem: 4023\n",
      "Epoch: [63/100]  [ 40/189]  eta: 0:01:28  loss: 6.461476 (6.461478)  lr: 0.000340 (0.000340)  wd: 0.000007 (0.000007)  time: 0.511380  data: 0.001290  max mem: 4023\n",
      "Epoch: [63/100]  [ 60/189]  eta: 0:01:13  loss: 6.461480 (6.461480)  lr: 0.000339 (0.000340)  wd: 0.000007 (0.000007)  time: 0.511491  data: 0.000228  max mem: 4023\n",
      "Epoch: [63/100]  [ 80/189]  eta: 0:01:00  loss: 6.461475 (6.461479)  lr: 0.000339 (0.000340)  wd: 0.000007 (0.000007)  time: 0.512483  data: 0.000534  max mem: 4023\n",
      "Epoch: [63/100]  [100/189]  eta: 0:00:48  loss: 6.461481 (6.461480)  lr: 0.000339 (0.000339)  wd: 0.000007 (0.000007)  time: 0.521126  data: 0.001085  max mem: 4023\n",
      "Epoch: [63/100]  [120/189]  eta: 0:00:37  loss: 6.461478 (6.461480)  lr: 0.000338 (0.000339)  wd: 0.000007 (0.000007)  time: 0.518380  data: 0.000327  max mem: 4023\n",
      "Epoch: [63/100]  [140/189]  eta: 0:00:26  loss: 6.461479 (6.461480)  lr: 0.000338 (0.000339)  wd: 0.000007 (0.000007)  time: 0.520287  data: 0.000941  max mem: 4023\n",
      "Epoch: [63/100]  [160/189]  eta: 0:00:15  loss: 6.461478 (6.461479)  lr: 0.000337 (0.000339)  wd: 0.000007 (0.000007)  time: 0.523142  data: 0.000150  max mem: 4023\n",
      "Epoch: [63/100]  [180/189]  eta: 0:00:04  loss: 6.461478 (6.461480)  lr: 0.000337 (0.000339)  wd: 0.000007 (0.000007)  time: 0.514182  data: 0.000452  max mem: 4023\n",
      "Epoch: [63/100]  [188/189]  eta: 0:00:00  loss: 6.461478 (6.461480)  lr: 0.000337 (0.000338)  wd: 0.000007 (0.000007)  time: 0.508503  data: 0.000081  max mem: 4023\n",
      "Epoch: [63/100] Total time: 0:01:40 (0.533899 s / it)\n",
      "Averaged stats: loss: 6.461478 (6.461480)  lr: 0.000337 (0.000338)  wd: 0.000007 (0.000007)\n",
      "Epoch: [64/100]  [  0/189]  eta: 0:11:31  loss: 6.461479 (6.461479)  lr: 0.000336 (0.000336)  wd: 0.000007 (0.000007)  time: 3.659566  data: 3.079390  max mem: 4023\n",
      "Epoch: [64/100]  [ 20/189]  eta: 0:01:56  loss: 6.461478 (6.461478)  lr: 0.000336 (0.000336)  wd: 0.000007 (0.000007)  time: 0.539790  data: 0.002311  max mem: 4023\n",
      "Epoch: [64/100]  [ 40/189]  eta: 0:01:30  loss: 6.461486 (6.461484)  lr: 0.000336 (0.000336)  wd: 0.000007 (0.000007)  time: 0.517012  data: 0.001051  max mem: 4023\n",
      "Epoch: [64/100]  [ 60/189]  eta: 0:01:13  loss: 6.461481 (6.461482)  lr: 0.000335 (0.000336)  wd: 0.000007 (0.000007)  time: 0.507653  data: 0.000733  max mem: 4023\n",
      "Epoch: [64/100]  [ 80/189]  eta: 0:01:00  loss: 6.461480 (6.461482)  lr: 0.000335 (0.000335)  wd: 0.000007 (0.000007)  time: 0.512635  data: 0.000175  max mem: 4023\n",
      "Epoch: [64/100]  [100/189]  eta: 0:00:49  loss: 6.461477 (6.461481)  lr: 0.000334 (0.000335)  wd: 0.000007 (0.000007)  time: 0.527187  data: 0.000849  max mem: 4023\n",
      "Epoch: [64/100]  [120/189]  eta: 0:00:37  loss: 6.461477 (6.461481)  lr: 0.000334 (0.000335)  wd: 0.000007 (0.000007)  time: 0.515655  data: 0.000460  max mem: 4023\n",
      "Epoch: [64/100]  [140/189]  eta: 0:00:26  loss: 6.461487 (6.461482)  lr: 0.000334 (0.000335)  wd: 0.000008 (0.000007)  time: 0.516617  data: 0.000766  max mem: 4023\n",
      "Epoch: [64/100]  [160/189]  eta: 0:00:15  loss: 6.461479 (6.461482)  lr: 0.000333 (0.000335)  wd: 0.000008 (0.000007)  time: 0.512580  data: 0.000133  max mem: 4023\n",
      "Epoch: [64/100]  [180/189]  eta: 0:00:04  loss: 6.461478 (6.461482)  lr: 0.000333 (0.000334)  wd: 0.000008 (0.000007)  time: 0.523293  data: 0.000219  max mem: 4023\n",
      "Epoch: [64/100]  [188/189]  eta: 0:00:00  loss: 6.461479 (6.461482)  lr: 0.000332 (0.000334)  wd: 0.000008 (0.000007)  time: 0.501999  data: 0.000073  max mem: 4023\n",
      "Epoch: [64/100] Total time: 0:01:41 (0.534978 s / it)\n",
      "Averaged stats: loss: 6.461479 (6.461482)  lr: 0.000332 (0.000334)  wd: 0.000008 (0.000007)\n",
      "Epoch: [65/100]  [  0/189]  eta: 0:11:45  loss: 6.461481 (6.461481)  lr: 0.000332 (0.000332)  wd: 0.000008 (0.000008)  time: 3.731000  data: 3.076230  max mem: 4023\n",
      "Epoch: [65/100]  [ 20/189]  eta: 0:01:55  loss: 6.461480 (6.461481)  lr: 0.000332 (0.000332)  wd: 0.000008 (0.000008)  time: 0.533035  data: 0.000184  max mem: 4023\n",
      "Epoch: [65/100]  [ 40/189]  eta: 0:01:29  loss: 6.461479 (6.461480)  lr: 0.000332 (0.000332)  wd: 0.000008 (0.000008)  time: 0.512822  data: 0.001052  max mem: 4023\n",
      "Epoch: [65/100]  [ 60/189]  eta: 0:01:13  loss: 6.461478 (6.461480)  lr: 0.000331 (0.000332)  wd: 0.000008 (0.000008)  time: 0.510536  data: 0.000234  max mem: 4023\n",
      "Epoch: [65/100]  [ 80/189]  eta: 0:01:00  loss: 6.461481 (6.461480)  lr: 0.000331 (0.000331)  wd: 0.000008 (0.000008)  time: 0.522159  data: 0.000869  max mem: 4023\n",
      "Epoch: [65/100]  [100/189]  eta: 0:00:48  loss: 6.461479 (6.461480)  lr: 0.000330 (0.000331)  wd: 0.000008 (0.000008)  time: 0.514587  data: 0.000171  max mem: 4023\n",
      "Epoch: [65/100]  [120/189]  eta: 0:00:37  loss: 6.461479 (6.461480)  lr: 0.000330 (0.000331)  wd: 0.000008 (0.000008)  time: 0.518972  data: 0.000245  max mem: 4023\n",
      "Epoch: [65/100]  [140/189]  eta: 0:00:26  loss: 6.461479 (6.461480)  lr: 0.000329 (0.000331)  wd: 0.000008 (0.000008)  time: 0.516796  data: 0.000156  max mem: 4023\n",
      "Epoch: [65/100]  [160/189]  eta: 0:00:15  loss: 6.461476 (6.461479)  lr: 0.000329 (0.000331)  wd: 0.000008 (0.000008)  time: 0.526323  data: 0.000150  max mem: 4023\n",
      "Epoch: [65/100]  [180/189]  eta: 0:00:04  loss: 6.461478 (6.461479)  lr: 0.000329 (0.000330)  wd: 0.000008 (0.000008)  time: 0.504572  data: 0.000147  max mem: 4023\n",
      "Epoch: [65/100]  [188/189]  eta: 0:00:00  loss: 6.461478 (6.461479)  lr: 0.000328 (0.000330)  wd: 0.000008 (0.000008)  time: 0.496721  data: 0.000079  max mem: 4023\n",
      "Epoch: [65/100] Total time: 0:01:40 (0.534036 s / it)\n",
      "Averaged stats: loss: 6.461478 (6.461479)  lr: 0.000328 (0.000330)  wd: 0.000008 (0.000008)\n",
      "Epoch: [66/100]  [  0/189]  eta: 0:11:29  loss: 6.461480 (6.461480)  lr: 0.000328 (0.000328)  wd: 0.000008 (0.000008)  time: 3.647274  data: 3.072403  max mem: 4023\n",
      "Epoch: [66/100]  [ 20/189]  eta: 0:01:55  loss: 6.461483 (6.461484)  lr: 0.000328 (0.000328)  wd: 0.000008 (0.000008)  time: 0.535067  data: 0.019140  max mem: 4023\n",
      "Epoch: [66/100]  [ 40/189]  eta: 0:01:29  loss: 6.461482 (6.461484)  lr: 0.000328 (0.000328)  wd: 0.000008 (0.000008)  time: 0.517796  data: 0.000865  max mem: 4023\n",
      "Epoch: [66/100]  [ 60/189]  eta: 0:01:14  loss: 6.461477 (6.461482)  lr: 0.000327 (0.000328)  wd: 0.000008 (0.000008)  time: 0.517666  data: 0.000161  max mem: 4023\n",
      "Epoch: [66/100]  [ 80/189]  eta: 0:01:00  loss: 6.461477 (6.461481)  lr: 0.000327 (0.000327)  wd: 0.000008 (0.000008)  time: 0.510975  data: 0.000476  max mem: 4023\n",
      "Epoch: [66/100]  [100/189]  eta: 0:00:49  loss: 6.461477 (6.461480)  lr: 0.000326 (0.000327)  wd: 0.000008 (0.000008)  time: 0.525431  data: 0.000167  max mem: 4023\n",
      "Epoch: [66/100]  [120/189]  eta: 0:00:37  loss: 6.461478 (6.461480)  lr: 0.000326 (0.000327)  wd: 0.000008 (0.000008)  time: 0.515822  data: 0.000272  max mem: 4023\n",
      "Epoch: [66/100]  [140/189]  eta: 0:00:26  loss: 6.461482 (6.461480)  lr: 0.000325 (0.000327)  wd: 0.000008 (0.000008)  time: 0.514093  data: 0.000247  max mem: 4023\n",
      "Epoch: [66/100]  [160/189]  eta: 0:00:15  loss: 6.461483 (6.461481)  lr: 0.000325 (0.000326)  wd: 0.000008 (0.000008)  time: 0.518104  data: 0.000284  max mem: 4023\n",
      "Epoch: [66/100]  [180/189]  eta: 0:00:04  loss: 6.461479 (6.461481)  lr: 0.000325 (0.000326)  wd: 0.000008 (0.000008)  time: 0.506251  data: 0.000153  max mem: 4023\n",
      "Epoch: [66/100]  [188/189]  eta: 0:00:00  loss: 6.461480 (6.461481)  lr: 0.000324 (0.000326)  wd: 0.000008 (0.000008)  time: 0.500783  data: 0.000083  max mem: 4023\n",
      "Epoch: [66/100] Total time: 0:01:40 (0.533745 s / it)\n",
      "Averaged stats: loss: 6.461480 (6.461481)  lr: 0.000324 (0.000326)  wd: 0.000008 (0.000008)\n",
      "Epoch: [67/100]  [  0/189]  eta: 0:12:26  loss: 6.461478 (6.461478)  lr: 0.000324 (0.000324)  wd: 0.000008 (0.000008)  time: 3.949810  data: 3.345920  max mem: 4023\n",
      "Epoch: [67/100]  [ 20/189]  eta: 0:01:57  loss: 6.461482 (6.461483)  lr: 0.000324 (0.000324)  wd: 0.000008 (0.000008)  time: 0.529699  data: 0.013247  max mem: 4023\n",
      "Epoch: [67/100]  [ 40/189]  eta: 0:01:29  loss: 6.461486 (6.461486)  lr: 0.000324 (0.000324)  wd: 0.000008 (0.000008)  time: 0.506624  data: 0.000279  max mem: 4023\n",
      "Epoch: [67/100]  [ 60/189]  eta: 0:01:13  loss: 6.461483 (6.461485)  lr: 0.000323 (0.000324)  wd: 0.000008 (0.000008)  time: 0.515658  data: 0.001520  max mem: 4023\n",
      "Epoch: [67/100]  [ 80/189]  eta: 0:01:00  loss: 6.461483 (6.461485)  lr: 0.000323 (0.000323)  wd: 0.000008 (0.000008)  time: 0.512861  data: 0.000175  max mem: 4023\n",
      "Epoch: [67/100]  [100/189]  eta: 0:00:48  loss: 6.461479 (6.461484)  lr: 0.000322 (0.000323)  wd: 0.000008 (0.000008)  time: 0.516446  data: 0.000462  max mem: 4023\n",
      "Epoch: [67/100]  [120/189]  eta: 0:00:37  loss: 6.461479 (6.461484)  lr: 0.000322 (0.000323)  wd: 0.000008 (0.000008)  time: 0.519021  data: 0.000133  max mem: 4023\n",
      "Epoch: [67/100]  [140/189]  eta: 0:00:26  loss: 6.461487 (6.461485)  lr: 0.000321 (0.000323)  wd: 0.000008 (0.000008)  time: 0.522142  data: 0.000812  max mem: 4023\n",
      "Epoch: [67/100]  [160/189]  eta: 0:00:15  loss: 6.461487 (6.461486)  lr: 0.000321 (0.000322)  wd: 0.000008 (0.000008)  time: 0.524499  data: 0.000217  max mem: 4023\n",
      "Epoch: [67/100]  [180/189]  eta: 0:00:04  loss: 6.461497 (6.461487)  lr: 0.000321 (0.000322)  wd: 0.000008 (0.000008)  time: 0.513477  data: 0.000293  max mem: 4023\n",
      "Epoch: [67/100]  [188/189]  eta: 0:00:00  loss: 6.461499 (6.461488)  lr: 0.000320 (0.000322)  wd: 0.000008 (0.000008)  time: 0.501272  data: 0.000086  max mem: 4023\n",
      "Epoch: [67/100] Total time: 0:01:41 (0.535417 s / it)\n",
      "Averaged stats: loss: 6.461499 (6.461488)  lr: 0.000320 (0.000322)  wd: 0.000008 (0.000008)\n",
      "Epoch: [68/100]  [  0/189]  eta: 0:14:17  loss: 6.461487 (6.461487)  lr: 0.000320 (0.000320)  wd: 0.000008 (0.000008)  time: 4.538791  data: 3.943783  max mem: 4023\n",
      "Epoch: [68/100]  [ 20/189]  eta: 0:02:00  loss: 6.461489 (6.461495)  lr: 0.000320 (0.000320)  wd: 0.000008 (0.000008)  time: 0.523599  data: 0.001103  max mem: 4023\n",
      "Epoch: [68/100]  [ 40/189]  eta: 0:01:31  loss: 6.461485 (6.461491)  lr: 0.000320 (0.000320)  wd: 0.000008 (0.000008)  time: 0.509107  data: 0.000373  max mem: 4023\n",
      "Epoch: [68/100]  [ 60/189]  eta: 0:01:14  loss: 6.461482 (6.461488)  lr: 0.000319 (0.000320)  wd: 0.000008 (0.000008)  time: 0.510466  data: 0.001266  max mem: 4023\n",
      "Epoch: [68/100]  [ 80/189]  eta: 0:01:01  loss: 6.461481 (6.461487)  lr: 0.000319 (0.000319)  wd: 0.000008 (0.000008)  time: 0.523490  data: 0.000147  max mem: 4023\n",
      "Epoch: [68/100]  [100/189]  eta: 0:00:49  loss: 6.461485 (6.461486)  lr: 0.000318 (0.000319)  wd: 0.000008 (0.000008)  time: 0.517867  data: 0.000157  max mem: 4023\n",
      "Epoch: [68/100]  [120/189]  eta: 0:00:37  loss: 6.461483 (6.461486)  lr: 0.000318 (0.000319)  wd: 0.000008 (0.000008)  time: 0.519278  data: 0.000294  max mem: 4023\n",
      "Epoch: [68/100]  [140/189]  eta: 0:00:26  loss: 6.461482 (6.461485)  lr: 0.000318 (0.000319)  wd: 0.000008 (0.000008)  time: 0.519516  data: 0.000149  max mem: 4023\n",
      "Epoch: [68/100]  [160/189]  eta: 0:00:15  loss: 6.461482 (6.461485)  lr: 0.000317 (0.000319)  wd: 0.000008 (0.000008)  time: 0.511074  data: 0.000130  max mem: 4023\n",
      "Epoch: [68/100]  [180/189]  eta: 0:00:04  loss: 6.461483 (6.461485)  lr: 0.000317 (0.000318)  wd: 0.000008 (0.000008)  time: 0.514298  data: 0.000104  max mem: 4023\n",
      "Epoch: [68/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461485)  lr: 0.000317 (0.000318)  wd: 0.000008 (0.000008)  time: 0.507932  data: 0.000081  max mem: 4023\n",
      "Epoch: [68/100] Total time: 0:01:41 (0.537515 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461485)  lr: 0.000317 (0.000318)  wd: 0.000008 (0.000008)\n",
      "Epoch: [69/100]  [  0/189]  eta: 0:13:46  loss: 6.461483 (6.461483)  lr: 0.000316 (0.000316)  wd: 0.000008 (0.000008)  time: 4.371427  data: 3.761296  max mem: 4023\n",
      "Epoch: [69/100]  [ 20/189]  eta: 0:01:58  loss: 6.461478 (6.461478)  lr: 0.000316 (0.000316)  wd: 0.000008 (0.000008)  time: 0.519336  data: 0.000564  max mem: 4023\n",
      "Epoch: [69/100]  [ 40/189]  eta: 0:01:30  loss: 6.461482 (6.461481)  lr: 0.000316 (0.000316)  wd: 0.000008 (0.000008)  time: 0.513302  data: 0.002475  max mem: 4023\n",
      "Epoch: [69/100]  [ 60/189]  eta: 0:01:14  loss: 6.461483 (6.461482)  lr: 0.000315 (0.000316)  wd: 0.000008 (0.000008)  time: 0.516991  data: 0.000292  max mem: 4023\n",
      "Epoch: [69/100]  [ 80/189]  eta: 0:01:01  loss: 6.461483 (6.461482)  lr: 0.000315 (0.000316)  wd: 0.000008 (0.000008)  time: 0.519289  data: 0.000154  max mem: 4023\n",
      "Epoch: [69/100]  [100/189]  eta: 0:00:49  loss: 6.461483 (6.461483)  lr: 0.000314 (0.000315)  wd: 0.000008 (0.000008)  time: 0.515276  data: 0.000146  max mem: 4023\n",
      "Epoch: [69/100]  [120/189]  eta: 0:00:37  loss: 6.461483 (6.461483)  lr: 0.000314 (0.000315)  wd: 0.000008 (0.000008)  time: 0.510774  data: 0.000162  max mem: 4023\n",
      "Epoch: [69/100]  [140/189]  eta: 0:00:26  loss: 6.461483 (6.461483)  lr: 0.000314 (0.000315)  wd: 0.000008 (0.000008)  time: 0.520261  data: 0.001063  max mem: 4023\n",
      "Epoch: [69/100]  [160/189]  eta: 0:00:15  loss: 6.461487 (6.461484)  lr: 0.000313 (0.000315)  wd: 0.000008 (0.000008)  time: 0.520502  data: 0.000142  max mem: 4023\n",
      "Epoch: [69/100]  [180/189]  eta: 0:00:04  loss: 6.461485 (6.461484)  lr: 0.000313 (0.000314)  wd: 0.000008 (0.000008)  time: 0.512917  data: 0.000213  max mem: 4023\n",
      "Epoch: [69/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461484)  lr: 0.000313 (0.000314)  wd: 0.000008 (0.000008)  time: 0.509183  data: 0.000077  max mem: 4023\n",
      "Epoch: [69/100] Total time: 0:01:41 (0.536480 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461484)  lr: 0.000313 (0.000314)  wd: 0.000008 (0.000008)\n",
      "Epoch: [70/100]  [  0/189]  eta: 0:13:35  loss: 6.461490 (6.461490)  lr: 0.000313 (0.000313)  wd: 0.000008 (0.000008)  time: 4.315078  data: 3.668770  max mem: 4023\n",
      "Epoch: [70/100]  [ 20/189]  eta: 0:01:58  loss: 6.461490 (6.461491)  lr: 0.000312 (0.000312)  wd: 0.000008 (0.000008)  time: 0.521356  data: 0.003066  max mem: 4023\n",
      "Epoch: [70/100]  [ 40/189]  eta: 0:01:30  loss: 6.461485 (6.461489)  lr: 0.000312 (0.000312)  wd: 0.000008 (0.000008)  time: 0.506359  data: 0.000222  max mem: 4023\n",
      "Epoch: [70/100]  [ 60/189]  eta: 0:01:14  loss: 6.461480 (6.461487)  lr: 0.000311 (0.000312)  wd: 0.000008 (0.000008)  time: 0.509616  data: 0.000555  max mem: 4023\n",
      "Epoch: [70/100]  [ 80/189]  eta: 0:01:00  loss: 6.461485 (6.461487)  lr: 0.000311 (0.000312)  wd: 0.000008 (0.000008)  time: 0.510336  data: 0.000152  max mem: 4023\n",
      "Epoch: [70/100]  [100/189]  eta: 0:00:49  loss: 6.461487 (6.461487)  lr: 0.000311 (0.000312)  wd: 0.000008 (0.000008)  time: 0.519082  data: 0.000390  max mem: 4023\n",
      "Epoch: [70/100]  [120/189]  eta: 0:00:37  loss: 6.461485 (6.461487)  lr: 0.000310 (0.000311)  wd: 0.000008 (0.000008)  time: 0.512108  data: 0.000508  max mem: 4023\n",
      "Epoch: [70/100]  [140/189]  eta: 0:00:26  loss: 6.461484 (6.461486)  lr: 0.000310 (0.000311)  wd: 0.000008 (0.000008)  time: 0.510177  data: 0.000166  max mem: 4023\n",
      "Epoch: [70/100]  [160/189]  eta: 0:00:15  loss: 6.461486 (6.461486)  lr: 0.000310 (0.000311)  wd: 0.000008 (0.000008)  time: 0.523596  data: 0.001019  max mem: 4023\n",
      "Epoch: [70/100]  [180/189]  eta: 0:00:04  loss: 6.461480 (6.461486)  lr: 0.000309 (0.000311)  wd: 0.000008 (0.000008)  time: 0.510053  data: 0.000130  max mem: 4023\n",
      "Epoch: [70/100]  [188/189]  eta: 0:00:00  loss: 6.461479 (6.461485)  lr: 0.000309 (0.000311)  wd: 0.000008 (0.000008)  time: 0.502525  data: 0.000078  max mem: 4023\n",
      "Epoch: [70/100] Total time: 0:01:40 (0.533346 s / it)\n",
      "Averaged stats: loss: 6.461479 (6.461485)  lr: 0.000309 (0.000311)  wd: 0.000008 (0.000008)\n",
      "Epoch: [71/100]  [  0/189]  eta: 0:11:27  loss: 6.461478 (6.461478)  lr: 0.000309 (0.000309)  wd: 0.000008 (0.000008)  time: 3.639188  data: 3.043727  max mem: 4023\n",
      "Epoch: [71/100]  [ 20/189]  eta: 0:01:54  loss: 6.461482 (6.461484)  lr: 0.000309 (0.000309)  wd: 0.000008 (0.000008)  time: 0.530053  data: 0.001039  max mem: 4023\n",
      "Epoch: [71/100]  [ 40/189]  eta: 0:01:29  loss: 6.461484 (6.461485)  lr: 0.000308 (0.000308)  wd: 0.000008 (0.000008)  time: 0.513892  data: 0.001608  max mem: 4023\n",
      "Epoch: [71/100]  [ 60/189]  eta: 0:01:13  loss: 6.461482 (6.461484)  lr: 0.000308 (0.000308)  wd: 0.000008 (0.000008)  time: 0.511354  data: 0.000417  max mem: 4023\n",
      "Epoch: [71/100]  [ 80/189]  eta: 0:01:00  loss: 6.461480 (6.461483)  lr: 0.000307 (0.000308)  wd: 0.000008 (0.000008)  time: 0.512798  data: 0.000247  max mem: 4023\n",
      "Epoch: [71/100]  [100/189]  eta: 0:00:48  loss: 6.461483 (6.461483)  lr: 0.000307 (0.000308)  wd: 0.000008 (0.000008)  time: 0.516579  data: 0.000342  max mem: 4023\n",
      "Epoch: [71/100]  [120/189]  eta: 0:00:37  loss: 6.461487 (6.461484)  lr: 0.000307 (0.000308)  wd: 0.000008 (0.000008)  time: 0.528270  data: 0.000655  max mem: 4023\n",
      "Epoch: [71/100]  [140/189]  eta: 0:00:26  loss: 6.461483 (6.461484)  lr: 0.000306 (0.000307)  wd: 0.000008 (0.000008)  time: 0.513006  data: 0.000203  max mem: 4023\n",
      "Epoch: [71/100]  [160/189]  eta: 0:00:15  loss: 6.461485 (6.461485)  lr: 0.000306 (0.000307)  wd: 0.000008 (0.000008)  time: 0.525759  data: 0.000538  max mem: 4023\n",
      "Epoch: [71/100]  [180/189]  eta: 0:00:04  loss: 6.461489 (6.461485)  lr: 0.000305 (0.000307)  wd: 0.000008 (0.000008)  time: 0.506148  data: 0.000133  max mem: 4023\n",
      "Epoch: [71/100]  [188/189]  eta: 0:00:00  loss: 6.461489 (6.461486)  lr: 0.000305 (0.000307)  wd: 0.000008 (0.000008)  time: 0.499316  data: 0.000085  max mem: 4023\n",
      "Epoch: [71/100] Total time: 0:01:40 (0.533442 s / it)\n",
      "Averaged stats: loss: 6.461489 (6.461486)  lr: 0.000305 (0.000307)  wd: 0.000008 (0.000008)\n",
      "Epoch: [72/100]  [  0/189]  eta: 0:13:09  loss: 6.461493 (6.461493)  lr: 0.000305 (0.000305)  wd: 0.000008 (0.000008)  time: 4.177294  data: 3.562921  max mem: 4023\n",
      "Epoch: [72/100]  [ 20/189]  eta: 0:01:56  loss: 6.461486 (6.461486)  lr: 0.000305 (0.000305)  wd: 0.000008 (0.000008)  time: 0.517859  data: 0.000788  max mem: 4023\n",
      "Epoch: [72/100]  [ 40/189]  eta: 0:01:29  loss: 6.461487 (6.461487)  lr: 0.000305 (0.000305)  wd: 0.000008 (0.000008)  time: 0.507693  data: 0.000444  max mem: 4023\n",
      "Epoch: [72/100]  [ 60/189]  eta: 0:01:14  loss: 6.461483 (6.461486)  lr: 0.000304 (0.000305)  wd: 0.000008 (0.000008)  time: 0.515649  data: 0.000127  max mem: 4023\n",
      "Epoch: [72/100]  [ 80/189]  eta: 0:01:01  loss: 6.461487 (6.461486)  lr: 0.000304 (0.000304)  wd: 0.000008 (0.000008)  time: 0.517826  data: 0.000140  max mem: 4023\n",
      "Epoch: [72/100]  [100/189]  eta: 0:00:48  loss: 6.461495 (6.461489)  lr: 0.000303 (0.000304)  wd: 0.000008 (0.000008)  time: 0.508517  data: 0.000134  max mem: 4023\n",
      "Epoch: [72/100]  [120/189]  eta: 0:00:37  loss: 6.461489 (6.461490)  lr: 0.000303 (0.000304)  wd: 0.000008 (0.000008)  time: 0.529458  data: 0.000219  max mem: 4023\n",
      "Epoch: [72/100]  [140/189]  eta: 0:00:26  loss: 6.461491 (6.461490)  lr: 0.000303 (0.000304)  wd: 0.000008 (0.000008)  time: 0.512450  data: 0.000138  max mem: 4023\n",
      "Epoch: [72/100]  [160/189]  eta: 0:00:15  loss: 6.461484 (6.461489)  lr: 0.000302 (0.000304)  wd: 0.000008 (0.000008)  time: 0.515966  data: 0.000131  max mem: 4023\n",
      "Epoch: [72/100]  [180/189]  eta: 0:00:04  loss: 6.461485 (6.461489)  lr: 0.000302 (0.000303)  wd: 0.000008 (0.000008)  time: 0.516322  data: 0.000115  max mem: 4023\n",
      "Epoch: [72/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461489)  lr: 0.000302 (0.000303)  wd: 0.000008 (0.000008)  time: 0.513561  data: 0.000072  max mem: 4023\n",
      "Epoch: [72/100] Total time: 0:01:41 (0.534957 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461489)  lr: 0.000302 (0.000303)  wd: 0.000008 (0.000008)\n",
      "Epoch: [73/100]  [  0/189]  eta: 0:14:48  loss: 6.461485 (6.461485)  lr: 0.000302 (0.000302)  wd: 0.000008 (0.000008)  time: 4.700146  data: 4.100229  max mem: 4023\n",
      "Epoch: [73/100]  [ 20/189]  eta: 0:02:00  loss: 6.461486 (6.461488)  lr: 0.000301 (0.000301)  wd: 0.000008 (0.000008)  time: 0.516173  data: 0.000194  max mem: 4023\n",
      "Epoch: [73/100]  [ 40/189]  eta: 0:01:31  loss: 6.461493 (6.461492)  lr: 0.000301 (0.000301)  wd: 0.000008 (0.000008)  time: 0.508756  data: 0.001361  max mem: 4023\n",
      "Epoch: [73/100]  [ 60/189]  eta: 0:01:14  loss: 6.461482 (6.461489)  lr: 0.000301 (0.000301)  wd: 0.000009 (0.000008)  time: 0.511563  data: 0.000931  max mem: 4023\n",
      "Epoch: [73/100]  [ 80/189]  eta: 0:01:01  loss: 6.461483 (6.461488)  lr: 0.000300 (0.000301)  wd: 0.000009 (0.000008)  time: 0.519994  data: 0.000182  max mem: 4023\n",
      "Epoch: [73/100]  [100/189]  eta: 0:00:49  loss: 6.461483 (6.461487)  lr: 0.000300 (0.000301)  wd: 0.000009 (0.000009)  time: 0.511390  data: 0.000162  max mem: 4023\n",
      "Epoch: [73/100]  [120/189]  eta: 0:00:37  loss: 6.461483 (6.461486)  lr: 0.000299 (0.000300)  wd: 0.000009 (0.000009)  time: 0.516004  data: 0.000145  max mem: 4023\n",
      "Epoch: [73/100]  [140/189]  eta: 0:00:26  loss: 6.461485 (6.461486)  lr: 0.000299 (0.000300)  wd: 0.000009 (0.000009)  time: 0.523663  data: 0.000274  max mem: 4023\n",
      "Epoch: [73/100]  [160/189]  eta: 0:00:15  loss: 6.461493 (6.461487)  lr: 0.000299 (0.000300)  wd: 0.000009 (0.000009)  time: 0.521778  data: 0.000143  max mem: 4023\n",
      "Epoch: [73/100]  [180/189]  eta: 0:00:04  loss: 6.461492 (6.461488)  lr: 0.000298 (0.000300)  wd: 0.000009 (0.000009)  time: 0.519356  data: 0.000176  max mem: 4023\n",
      "Epoch: [73/100]  [188/189]  eta: 0:00:00  loss: 6.461490 (6.461488)  lr: 0.000298 (0.000300)  wd: 0.000009 (0.000009)  time: 0.504422  data: 0.000083  max mem: 4023\n",
      "Epoch: [73/100] Total time: 0:01:41 (0.537854 s / it)\n",
      "Averaged stats: loss: 6.461490 (6.461488)  lr: 0.000298 (0.000300)  wd: 0.000009 (0.000009)\n",
      "Epoch: [74/100]  [  0/189]  eta: 0:12:24  loss: 6.461491 (6.461491)  lr: 0.000298 (0.000298)  wd: 0.000009 (0.000009)  time: 3.940146  data: 3.306393  max mem: 4023\n",
      "Epoch: [74/100]  [ 20/189]  eta: 0:01:54  loss: 6.461483 (6.461485)  lr: 0.000298 (0.000298)  wd: 0.000009 (0.000009)  time: 0.516950  data: 0.000854  max mem: 4023\n",
      "Epoch: [74/100]  [ 40/189]  eta: 0:01:29  loss: 6.461483 (6.461484)  lr: 0.000297 (0.000298)  wd: 0.000009 (0.000009)  time: 0.514406  data: 0.001685  max mem: 4023\n",
      "Epoch: [74/100]  [ 60/189]  eta: 0:01:13  loss: 6.461485 (6.461484)  lr: 0.000297 (0.000297)  wd: 0.000009 (0.000009)  time: 0.511789  data: 0.000431  max mem: 4023\n",
      "Epoch: [74/100]  [ 80/189]  eta: 0:01:01  loss: 6.461485 (6.461485)  lr: 0.000297 (0.000297)  wd: 0.000009 (0.000009)  time: 0.528125  data: 0.000289  max mem: 4023\n",
      "Epoch: [74/100]  [100/189]  eta: 0:00:49  loss: 6.461483 (6.461485)  lr: 0.000296 (0.000297)  wd: 0.000009 (0.000009)  time: 0.521099  data: 0.000421  max mem: 4023\n",
      "Epoch: [74/100]  [120/189]  eta: 0:00:37  loss: 6.461485 (6.461485)  lr: 0.000296 (0.000297)  wd: 0.000009 (0.000009)  time: 0.512426  data: 0.000172  max mem: 4023\n",
      "Epoch: [74/100]  [140/189]  eta: 0:00:26  loss: 6.461485 (6.461485)  lr: 0.000296 (0.000297)  wd: 0.000009 (0.000009)  time: 0.512175  data: 0.000288  max mem: 4023\n",
      "Epoch: [74/100]  [160/189]  eta: 0:00:15  loss: 6.461482 (6.461485)  lr: 0.000295 (0.000297)  wd: 0.000009 (0.000009)  time: 0.518428  data: 0.000162  max mem: 4023\n",
      "Epoch: [74/100]  [180/189]  eta: 0:00:04  loss: 6.461487 (6.461485)  lr: 0.000295 (0.000296)  wd: 0.000009 (0.000009)  time: 0.505664  data: 0.000217  max mem: 4023\n",
      "Epoch: [74/100]  [188/189]  eta: 0:00:00  loss: 6.461484 (6.461485)  lr: 0.000295 (0.000296)  wd: 0.000009 (0.000009)  time: 0.499696  data: 0.000085  max mem: 4023\n",
      "Epoch: [74/100] Total time: 0:01:40 (0.533310 s / it)\n",
      "Averaged stats: loss: 6.461484 (6.461485)  lr: 0.000295 (0.000296)  wd: 0.000009 (0.000009)\n",
      "Epoch: [75/100]  [  0/189]  eta: 0:12:24  loss: 6.461482 (6.461482)  lr: 0.000295 (0.000295)  wd: 0.000009 (0.000009)  time: 3.937088  data: 3.355437  max mem: 4023\n",
      "Epoch: [75/100]  [ 20/189]  eta: 0:01:55  loss: 6.461485 (6.461486)  lr: 0.000294 (0.000294)  wd: 0.000009 (0.000009)  time: 0.523753  data: 0.000963  max mem: 4023\n",
      "Epoch: [75/100]  [ 40/189]  eta: 0:01:30  loss: 6.461484 (6.461485)  lr: 0.000294 (0.000294)  wd: 0.000009 (0.000009)  time: 0.521621  data: 0.001686  max mem: 4023\n",
      "Epoch: [75/100]  [ 60/189]  eta: 0:01:14  loss: 6.461484 (6.461485)  lr: 0.000294 (0.000294)  wd: 0.000009 (0.000009)  time: 0.513621  data: 0.000912  max mem: 4023\n",
      "Epoch: [75/100]  [ 80/189]  eta: 0:01:01  loss: 6.461488 (6.461488)  lr: 0.000293 (0.000294)  wd: 0.000009 (0.000009)  time: 0.521856  data: 0.000154  max mem: 4023\n",
      "Epoch: [75/100]  [100/189]  eta: 0:00:49  loss: 6.461503 (6.461491)  lr: 0.000293 (0.000294)  wd: 0.000009 (0.000009)  time: 0.510898  data: 0.000980  max mem: 4023\n",
      "Epoch: [75/100]  [120/189]  eta: 0:00:37  loss: 6.461491 (6.461491)  lr: 0.000293 (0.000294)  wd: 0.000009 (0.000009)  time: 0.511231  data: 0.000339  max mem: 4023\n",
      "Epoch: [75/100]  [140/189]  eta: 0:00:26  loss: 6.461483 (6.461490)  lr: 0.000292 (0.000293)  wd: 0.000009 (0.000009)  time: 0.512889  data: 0.000148  max mem: 4023\n",
      "Epoch: [75/100]  [160/189]  eta: 0:00:15  loss: 6.461485 (6.461490)  lr: 0.000292 (0.000293)  wd: 0.000009 (0.000009)  time: 0.514463  data: 0.000147  max mem: 4023\n",
      "Epoch: [75/100]  [180/189]  eta: 0:00:04  loss: 6.461485 (6.461490)  lr: 0.000292 (0.000293)  wd: 0.000009 (0.000009)  time: 0.522765  data: 0.000175  max mem: 4023\n",
      "Epoch: [75/100]  [188/189]  eta: 0:00:00  loss: 6.461487 (6.461490)  lr: 0.000292 (0.000293)  wd: 0.000009 (0.000009)  time: 0.510736  data: 0.000126  max mem: 4023\n",
      "Epoch: [75/100] Total time: 0:01:41 (0.534492 s / it)\n",
      "Averaged stats: loss: 6.461487 (6.461490)  lr: 0.000292 (0.000293)  wd: 0.000009 (0.000009)\n",
      "Epoch: [76/100]  [  0/189]  eta: 0:11:05  loss: 6.461489 (6.461489)  lr: 0.000291 (0.000291)  wd: 0.000009 (0.000009)  time: 3.519500  data: 2.901475  max mem: 4023\n",
      "Epoch: [76/100]  [ 20/189]  eta: 0:01:51  loss: 6.461486 (6.461487)  lr: 0.000291 (0.000291)  wd: 0.000009 (0.000009)  time: 0.519596  data: 0.007249  max mem: 4023\n",
      "Epoch: [76/100]  [ 40/189]  eta: 0:01:28  loss: 6.461483 (6.461487)  lr: 0.000291 (0.000291)  wd: 0.000009 (0.000009)  time: 0.522978  data: 0.000403  max mem: 4023\n",
      "Epoch: [76/100]  [ 60/189]  eta: 0:01:13  loss: 6.461484 (6.461486)  lr: 0.000290 (0.000291)  wd: 0.000009 (0.000009)  time: 0.511568  data: 0.000361  max mem: 4023\n",
      "Epoch: [76/100]  [ 80/189]  eta: 0:01:00  loss: 6.461479 (6.461485)  lr: 0.000290 (0.000291)  wd: 0.000009 (0.000009)  time: 0.513562  data: 0.000374  max mem: 4023\n",
      "Epoch: [76/100]  [100/189]  eta: 0:00:48  loss: 6.461483 (6.461485)  lr: 0.000290 (0.000291)  wd: 0.000009 (0.000009)  time: 0.516189  data: 0.000436  max mem: 4023\n",
      "Epoch: [76/100]  [120/189]  eta: 0:00:37  loss: 6.461497 (6.461488)  lr: 0.000289 (0.000290)  wd: 0.000009 (0.000009)  time: 0.513854  data: 0.000146  max mem: 4023\n",
      "Epoch: [76/100]  [140/189]  eta: 0:00:26  loss: 6.461490 (6.461488)  lr: 0.000289 (0.000290)  wd: 0.000009 (0.000009)  time: 0.518644  data: 0.000127  max mem: 4023\n",
      "Epoch: [76/100]  [160/189]  eta: 0:00:15  loss: 6.461484 (6.461488)  lr: 0.000289 (0.000290)  wd: 0.000009 (0.000009)  time: 0.521881  data: 0.000822  max mem: 4023\n",
      "Epoch: [76/100]  [180/189]  eta: 0:00:04  loss: 6.461482 (6.461487)  lr: 0.000288 (0.000290)  wd: 0.000009 (0.000009)  time: 0.514300  data: 0.000109  max mem: 4023\n",
      "Epoch: [76/100]  [188/189]  eta: 0:00:00  loss: 6.461483 (6.461488)  lr: 0.000288 (0.000290)  wd: 0.000009 (0.000009)  time: 0.500375  data: 0.000076  max mem: 4023\n",
      "Epoch: [76/100] Total time: 0:01:40 (0.532242 s / it)\n",
      "Averaged stats: loss: 6.461483 (6.461488)  lr: 0.000288 (0.000290)  wd: 0.000009 (0.000009)\n",
      "Epoch: [77/100]  [  0/189]  eta: 0:13:42  loss: 6.461497 (6.461497)  lr: 0.000288 (0.000288)  wd: 0.000009 (0.000009)  time: 4.352923  data: 3.768381  max mem: 4023\n",
      "Epoch: [77/100]  [ 20/189]  eta: 0:01:59  loss: 6.461486 (6.461488)  lr: 0.000288 (0.000288)  wd: 0.000009 (0.000009)  time: 0.523600  data: 0.001869  max mem: 4023\n",
      "Epoch: [77/100]  [ 40/189]  eta: 0:01:30  loss: 6.461485 (6.461488)  lr: 0.000288 (0.000288)  wd: 0.000009 (0.000009)  time: 0.509022  data: 0.000233  max mem: 4023\n",
      "Epoch: [77/100]  [ 60/189]  eta: 0:01:15  loss: 6.461487 (6.461489)  lr: 0.000287 (0.000288)  wd: 0.000009 (0.000009)  time: 0.526002  data: 0.000222  max mem: 4023\n",
      "Epoch: [77/100]  [ 80/189]  eta: 0:01:01  loss: 6.461491 (6.461489)  lr: 0.000287 (0.000288)  wd: 0.000009 (0.000009)  time: 0.517532  data: 0.000259  max mem: 4023\n",
      "Epoch: [77/100]  [100/189]  eta: 0:00:49  loss: 6.461482 (6.461488)  lr: 0.000287 (0.000287)  wd: 0.000009 (0.000009)  time: 0.525022  data: 0.000141  max mem: 4023\n",
      "Epoch: [77/100]  [120/189]  eta: 0:00:38  loss: 6.461482 (6.461487)  lr: 0.000286 (0.000287)  wd: 0.000009 (0.000009)  time: 0.514977  data: 0.000223  max mem: 4023\n",
      "Epoch: [77/100]  [140/189]  eta: 0:00:26  loss: 6.461522 (6.461492)  lr: 0.000286 (0.000287)  wd: 0.000009 (0.000009)  time: 0.529811  data: 0.000145  max mem: 4023\n",
      "Epoch: [77/100]  [160/189]  eta: 0:00:15  loss: 6.461493 (6.461492)  lr: 0.000286 (0.000287)  wd: 0.000009 (0.000009)  time: 0.519084  data: 0.000158  max mem: 4023\n",
      "Epoch: [77/100]  [180/189]  eta: 0:00:04  loss: 6.461483 (6.461491)  lr: 0.000285 (0.000287)  wd: 0.000009 (0.000009)  time: 0.517027  data: 0.000258  max mem: 4023\n",
      "Epoch: [77/100]  [188/189]  eta: 0:00:00  loss: 6.461484 (6.461491)  lr: 0.000285 (0.000287)  wd: 0.000009 (0.000009)  time: 0.507430  data: 0.000216  max mem: 4023\n",
      "Epoch: [77/100] Total time: 0:01:42 (0.540464 s / it)\n",
      "Averaged stats: loss: 6.461484 (6.461491)  lr: 0.000285 (0.000287)  wd: 0.000009 (0.000009)\n",
      "Epoch: [78/100]  [  0/189]  eta: 0:13:46  loss: 6.461486 (6.461486)  lr: 0.000285 (0.000285)  wd: 0.000009 (0.000009)  time: 4.370871  data: 3.791406  max mem: 4023\n",
      "Epoch: [78/100]  [ 20/189]  eta: 0:01:58  loss: 6.461483 (6.461484)  lr: 0.000285 (0.000285)  wd: 0.000009 (0.000009)  time: 0.516309  data: 0.000188  max mem: 4023\n",
      "Epoch: [78/100]  [ 40/189]  eta: 0:01:30  loss: 6.461506 (6.461496)  lr: 0.000285 (0.000285)  wd: 0.000009 (0.000009)  time: 0.513513  data: 0.001548  max mem: 4023\n",
      "Epoch: [78/100]  [ 60/189]  eta: 0:01:14  loss: 6.461489 (6.461493)  lr: 0.000284 (0.000285)  wd: 0.000009 (0.000009)  time: 0.511013  data: 0.000139  max mem: 4023\n",
      "Epoch: [78/100]  [ 80/189]  eta: 0:01:01  loss: 6.461482 (6.461491)  lr: 0.000284 (0.000284)  wd: 0.000009 (0.000009)  time: 0.520719  data: 0.000425  max mem: 4023\n",
      "Epoch: [78/100]  [100/189]  eta: 0:00:49  loss: 6.461486 (6.461491)  lr: 0.000284 (0.000284)  wd: 0.000009 (0.000009)  time: 0.514351  data: 0.000284  max mem: 4023\n",
      "Epoch: [78/100]  [120/189]  eta: 0:00:37  loss: 6.461493 (6.461492)  lr: 0.000283 (0.000284)  wd: 0.000009 (0.000009)  time: 0.516646  data: 0.000206  max mem: 4023\n",
      "Epoch: [78/100]  [140/189]  eta: 0:00:26  loss: 6.461484 (6.461491)  lr: 0.000283 (0.000284)  wd: 0.000009 (0.000009)  time: 0.511455  data: 0.000120  max mem: 4023\n",
      "Epoch: [78/100]  [160/189]  eta: 0:00:15  loss: 6.461489 (6.461491)  lr: 0.000283 (0.000284)  wd: 0.000009 (0.000009)  time: 0.532461  data: 0.000165  max mem: 4023\n",
      "Epoch: [78/100]  [180/189]  eta: 0:00:04  loss: 6.461483 (6.461491)  lr: 0.000282 (0.000284)  wd: 0.000009 (0.000009)  time: 0.507129  data: 0.000117  max mem: 4023\n",
      "Epoch: [78/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461490)  lr: 0.000282 (0.000284)  wd: 0.000009 (0.000009)  time: 0.497826  data: 0.000075  max mem: 4023\n",
      "Epoch: [78/100] Total time: 0:01:41 (0.535806 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461490)  lr: 0.000282 (0.000284)  wd: 0.000009 (0.000009)\n",
      "Epoch: [79/100]  [  0/189]  eta: 0:12:26  loss: 6.461478 (6.461478)  lr: 0.000282 (0.000282)  wd: 0.000009 (0.000009)  time: 3.948141  data: 3.297600  max mem: 4023\n",
      "Epoch: [79/100]  [ 20/189]  eta: 0:01:54  loss: 6.461496 (6.461500)  lr: 0.000282 (0.000282)  wd: 0.000009 (0.000009)  time: 0.514446  data: 0.001357  max mem: 4023\n",
      "Epoch: [79/100]  [ 40/189]  eta: 0:01:29  loss: 6.461484 (6.461492)  lr: 0.000282 (0.000282)  wd: 0.000009 (0.000009)  time: 0.515356  data: 0.000845  max mem: 4023\n",
      "Epoch: [79/100]  [ 60/189]  eta: 0:01:14  loss: 6.461480 (6.461489)  lr: 0.000281 (0.000282)  wd: 0.000009 (0.000009)  time: 0.528976  data: 0.000744  max mem: 4023\n",
      "Epoch: [79/100]  [ 80/189]  eta: 0:01:00  loss: 6.461485 (6.461488)  lr: 0.000281 (0.000281)  wd: 0.000009 (0.000009)  time: 0.508972  data: 0.000156  max mem: 4023\n",
      "Epoch: [79/100]  [100/189]  eta: 0:00:49  loss: 6.461483 (6.461488)  lr: 0.000281 (0.000281)  wd: 0.000009 (0.000009)  time: 0.515457  data: 0.000135  max mem: 4023\n",
      "Epoch: [79/100]  [120/189]  eta: 0:00:37  loss: 6.461481 (6.461486)  lr: 0.000280 (0.000281)  wd: 0.000009 (0.000009)  time: 0.519998  data: 0.000443  max mem: 4023\n",
      "Epoch: [79/100]  [140/189]  eta: 0:00:26  loss: 6.461492 (6.461487)  lr: 0.000280 (0.000281)  wd: 0.000009 (0.000009)  time: 0.529701  data: 0.000216  max mem: 4023\n",
      "Epoch: [79/100]  [160/189]  eta: 0:00:15  loss: 6.461493 (6.461488)  lr: 0.000280 (0.000281)  wd: 0.000009 (0.000009)  time: 0.514943  data: 0.000393  max mem: 4023\n",
      "Epoch: [79/100]  [180/189]  eta: 0:00:04  loss: 6.461483 (6.461487)  lr: 0.000280 (0.000281)  wd: 0.000009 (0.000009)  time: 0.512249  data: 0.000215  max mem: 4023\n",
      "Epoch: [79/100]  [188/189]  eta: 0:00:00  loss: 6.461481 (6.461487)  lr: 0.000279 (0.000281)  wd: 0.000009 (0.000009)  time: 0.504402  data: 0.000176  max mem: 4023\n",
      "Epoch: [79/100] Total time: 0:01:41 (0.535377 s / it)\n",
      "Averaged stats: loss: 6.461481 (6.461487)  lr: 0.000279 (0.000281)  wd: 0.000009 (0.000009)\n",
      "Epoch: [80/100]  [  0/189]  eta: 0:11:56  loss: 6.461477 (6.461477)  lr: 0.000279 (0.000279)  wd: 0.000009 (0.000009)  time: 3.793177  data: 3.189784  max mem: 4023\n",
      "Epoch: [80/100]  [ 20/189]  eta: 0:01:55  loss: 6.461482 (6.461486)  lr: 0.000279 (0.000279)  wd: 0.000009 (0.000009)  time: 0.525136  data: 0.000199  max mem: 4023\n",
      "Epoch: [80/100]  [ 40/189]  eta: 0:01:29  loss: 6.461489 (6.461488)  lr: 0.000279 (0.000279)  wd: 0.000009 (0.000009)  time: 0.509977  data: 0.000285  max mem: 4023\n",
      "Epoch: [80/100]  [ 60/189]  eta: 0:01:13  loss: 6.461483 (6.461487)  lr: 0.000278 (0.000279)  wd: 0.000009 (0.000009)  time: 0.517659  data: 0.000431  max mem: 4023\n",
      "Epoch: [80/100]  [ 80/189]  eta: 0:01:00  loss: 6.461483 (6.461486)  lr: 0.000278 (0.000279)  wd: 0.000009 (0.000009)  time: 0.515495  data: 0.000873  max mem: 4023\n",
      "Epoch: [80/100]  [100/189]  eta: 0:00:48  loss: 6.461487 (6.461486)  lr: 0.000278 (0.000279)  wd: 0.000009 (0.000009)  time: 0.516917  data: 0.000340  max mem: 4023\n",
      "Epoch: [80/100]  [120/189]  eta: 0:00:37  loss: 6.461491 (6.461488)  lr: 0.000278 (0.000278)  wd: 0.000009 (0.000009)  time: 0.522022  data: 0.000380  max mem: 4023\n",
      "Epoch: [80/100]  [140/189]  eta: 0:00:26  loss: 6.461498 (6.461490)  lr: 0.000277 (0.000278)  wd: 0.000009 (0.000009)  time: 0.512111  data: 0.000973  max mem: 4023\n",
      "Epoch: [80/100]  [160/189]  eta: 0:00:15  loss: 6.461486 (6.461490)  lr: 0.000277 (0.000278)  wd: 0.000009 (0.000009)  time: 0.513521  data: 0.000130  max mem: 4023\n",
      "Epoch: [80/100]  [180/189]  eta: 0:00:04  loss: 6.461483 (6.461489)  lr: 0.000277 (0.000278)  wd: 0.000009 (0.000009)  time: 0.503345  data: 0.000104  max mem: 4023\n",
      "Epoch: [80/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461489)  lr: 0.000277 (0.000278)  wd: 0.000009 (0.000009)  time: 0.503155  data: 0.000075  max mem: 4023\n",
      "Epoch: [80/100] Total time: 0:01:40 (0.532677 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461489)  lr: 0.000277 (0.000278)  wd: 0.000009 (0.000009)\n",
      "Epoch: [81/100]  [  0/189]  eta: 0:11:47  loss: 6.461479 (6.461479)  lr: 0.000276 (0.000276)  wd: 0.000009 (0.000009)  time: 3.744645  data: 3.145435  max mem: 4023\n",
      "Epoch: [81/100]  [ 20/189]  eta: 0:01:54  loss: 6.461481 (6.461482)  lr: 0.000276 (0.000276)  wd: 0.000009 (0.000009)  time: 0.523568  data: 0.001521  max mem: 4023\n",
      "Epoch: [81/100]  [ 40/189]  eta: 0:01:28  loss: 6.461479 (6.461482)  lr: 0.000276 (0.000276)  wd: 0.000009 (0.000009)  time: 0.510299  data: 0.000195  max mem: 4023\n",
      "Epoch: [81/100]  [ 60/189]  eta: 0:01:13  loss: 6.461482 (6.461482)  lr: 0.000276 (0.000276)  wd: 0.000009 (0.000009)  time: 0.511508  data: 0.000369  max mem: 4023\n",
      "Epoch: [81/100]  [ 80/189]  eta: 0:01:00  loss: 6.461483 (6.461482)  lr: 0.000275 (0.000276)  wd: 0.000009 (0.000009)  time: 0.514554  data: 0.000156  max mem: 4023\n",
      "Epoch: [81/100]  [100/189]  eta: 0:00:48  loss: 6.461484 (6.461483)  lr: 0.000275 (0.000276)  wd: 0.000009 (0.000009)  time: 0.531046  data: 0.000141  max mem: 4023\n",
      "Epoch: [81/100]  [120/189]  eta: 0:00:37  loss: 6.461481 (6.461483)  lr: 0.000275 (0.000276)  wd: 0.000009 (0.000009)  time: 0.514806  data: 0.000127  max mem: 4023\n",
      "Epoch: [81/100]  [140/189]  eta: 0:00:26  loss: 6.461485 (6.461483)  lr: 0.000275 (0.000276)  wd: 0.000009 (0.000009)  time: 0.523478  data: 0.000209  max mem: 4023\n",
      "Epoch: [81/100]  [160/189]  eta: 0:00:15  loss: 6.461491 (6.461485)  lr: 0.000274 (0.000275)  wd: 0.000009 (0.000009)  time: 0.511219  data: 0.000194  max mem: 4023\n",
      "Epoch: [81/100]  [180/189]  eta: 0:00:04  loss: 6.461485 (6.461485)  lr: 0.000274 (0.000275)  wd: 0.000009 (0.000009)  time: 0.509925  data: 0.001171  max mem: 4023\n",
      "Epoch: [81/100]  [188/189]  eta: 0:00:00  loss: 6.461483 (6.461485)  lr: 0.000274 (0.000275)  wd: 0.000009 (0.000009)  time: 0.511885  data: 0.000079  max mem: 4023\n",
      "Epoch: [81/100] Total time: 0:01:40 (0.534246 s / it)\n",
      "Averaged stats: loss: 6.461483 (6.461485)  lr: 0.000274 (0.000275)  wd: 0.000009 (0.000009)\n",
      "Epoch: [82/100]  [  0/189]  eta: 0:12:07  loss: 6.461481 (6.461481)  lr: 0.000274 (0.000274)  wd: 0.000009 (0.000009)  time: 3.848241  data: 3.258863  max mem: 4023\n",
      "Epoch: [82/100]  [ 20/189]  eta: 0:01:55  loss: 6.461483 (6.461483)  lr: 0.000274 (0.000274)  wd: 0.000009 (0.000009)  time: 0.526411  data: 0.004057  max mem: 4023\n",
      "Epoch: [82/100]  [ 40/189]  eta: 0:01:29  loss: 6.461488 (6.461488)  lr: 0.000273 (0.000274)  wd: 0.000009 (0.000009)  time: 0.512338  data: 0.001701  max mem: 4023\n",
      "Epoch: [82/100]  [ 60/189]  eta: 0:01:14  loss: 6.461487 (6.461488)  lr: 0.000273 (0.000273)  wd: 0.000009 (0.000009)  time: 0.520601  data: 0.000358  max mem: 4023\n",
      "Epoch: [82/100]  [ 80/189]  eta: 0:01:01  loss: 6.461487 (6.461488)  lr: 0.000273 (0.000273)  wd: 0.000009 (0.000009)  time: 0.519661  data: 0.000607  max mem: 4023\n",
      "Epoch: [82/100]  [100/189]  eta: 0:00:49  loss: 6.461489 (6.461489)  lr: 0.000273 (0.000273)  wd: 0.000009 (0.000009)  time: 0.511548  data: 0.000172  max mem: 4023\n",
      "Epoch: [82/100]  [120/189]  eta: 0:00:37  loss: 6.461489 (6.461490)  lr: 0.000272 (0.000273)  wd: 0.000009 (0.000009)  time: 0.517567  data: 0.000166  max mem: 4023\n",
      "Epoch: [82/100]  [140/189]  eta: 0:00:26  loss: 6.461483 (6.461489)  lr: 0.000272 (0.000273)  wd: 0.000009 (0.000009)  time: 0.521881  data: 0.000177  max mem: 4023\n",
      "Epoch: [82/100]  [160/189]  eta: 0:00:15  loss: 6.461483 (6.461489)  lr: 0.000272 (0.000273)  wd: 0.000009 (0.000009)  time: 0.511467  data: 0.000248  max mem: 4023\n",
      "Epoch: [82/100]  [180/189]  eta: 0:00:04  loss: 6.461482 (6.461488)  lr: 0.000272 (0.000273)  wd: 0.000009 (0.000009)  time: 0.510267  data: 0.000116  max mem: 4023\n",
      "Epoch: [82/100]  [188/189]  eta: 0:00:00  loss: 6.461485 (6.461488)  lr: 0.000271 (0.000273)  wd: 0.000009 (0.000009)  time: 0.507688  data: 0.000074  max mem: 4023\n",
      "Epoch: [82/100] Total time: 0:01:40 (0.534350 s / it)\n",
      "Averaged stats: loss: 6.461485 (6.461488)  lr: 0.000271 (0.000273)  wd: 0.000009 (0.000009)\n",
      "Epoch: [83/100]  [  0/189]  eta: 0:13:05  loss: 6.461498 (6.461498)  lr: 0.000271 (0.000271)  wd: 0.000009 (0.000009)  time: 4.153664  data: 3.552619  max mem: 4023\n",
      "Epoch: [83/100]  [ 20/189]  eta: 0:01:56  loss: 6.461489 (6.461492)  lr: 0.000271 (0.000271)  wd: 0.000009 (0.000009)  time: 0.515244  data: 0.000398  max mem: 4023\n",
      "Epoch: [83/100]  [ 40/189]  eta: 0:01:29  loss: 6.461494 (6.461494)  lr: 0.000271 (0.000271)  wd: 0.000009 (0.000009)  time: 0.510459  data: 0.000892  max mem: 4023\n",
      "Epoch: [83/100]  [ 60/189]  eta: 0:01:13  loss: 6.461491 (6.461495)  lr: 0.000271 (0.000271)  wd: 0.000009 (0.000009)  time: 0.511734  data: 0.000173  max mem: 4023\n",
      "Epoch: [83/100]  [ 80/189]  eta: 0:01:01  loss: 6.461492 (6.461496)  lr: 0.000270 (0.000271)  wd: 0.000009 (0.000009)  time: 0.526054  data: 0.000860  max mem: 4023\n",
      "Epoch: [83/100]  [100/189]  eta: 0:00:49  loss: 6.461485 (6.461494)  lr: 0.000270 (0.000271)  wd: 0.000009 (0.000009)  time: 0.512519  data: 0.000157  max mem: 4023\n",
      "Epoch: [83/100]  [120/189]  eta: 0:00:37  loss: 6.461486 (6.461493)  lr: 0.000270 (0.000271)  wd: 0.000009 (0.000009)  time: 0.513216  data: 0.000256  max mem: 4023\n",
      "Epoch: [83/100]  [140/189]  eta: 0:00:26  loss: 6.461485 (6.461492)  lr: 0.000270 (0.000270)  wd: 0.000009 (0.000009)  time: 0.511217  data: 0.000135  max mem: 4023\n",
      "Epoch: [83/100]  [160/189]  eta: 0:00:15  loss: 6.461485 (6.461491)  lr: 0.000269 (0.000270)  wd: 0.000009 (0.000009)  time: 0.526734  data: 0.000980  max mem: 4023\n",
      "Epoch: [83/100]  [180/189]  eta: 0:00:04  loss: 6.461488 (6.461491)  lr: 0.000269 (0.000270)  wd: 0.000009 (0.000009)  time: 0.511468  data: 0.000110  max mem: 4023\n",
      "Epoch: [83/100]  [188/189]  eta: 0:00:00  loss: 6.461490 (6.461491)  lr: 0.000269 (0.000270)  wd: 0.000009 (0.000009)  time: 0.507512  data: 0.000080  max mem: 4023\n",
      "Epoch: [83/100] Total time: 0:01:41 (0.534543 s / it)\n",
      "Averaged stats: loss: 6.461490 (6.461491)  lr: 0.000269 (0.000270)  wd: 0.000009 (0.000009)\n",
      "Epoch: [84/100]  [  0/189]  eta: 0:15:10  loss: 6.461484 (6.461484)  lr: 0.000269 (0.000269)  wd: 0.000009 (0.000009)  time: 4.817692  data: 4.212122  max mem: 4023\n",
      "Epoch: [84/100]  [ 20/189]  eta: 0:02:01  loss: 6.461489 (6.461491)  lr: 0.000269 (0.000269)  wd: 0.000009 (0.000009)  time: 0.512736  data: 0.000898  max mem: 4023\n",
      "Epoch: [84/100]  [ 40/189]  eta: 0:01:31  loss: 6.461483 (6.461488)  lr: 0.000269 (0.000269)  wd: 0.000009 (0.000009)  time: 0.509605  data: 0.000146  max mem: 4023\n",
      "Epoch: [84/100]  [ 60/189]  eta: 0:01:15  loss: 6.461487 (6.461488)  lr: 0.000268 (0.000269)  wd: 0.000009 (0.000009)  time: 0.523166  data: 0.000261  max mem: 4023\n",
      "Epoch: [84/100]  [ 80/189]  eta: 0:01:02  loss: 6.461486 (6.461488)  lr: 0.000268 (0.000269)  wd: 0.000009 (0.000009)  time: 0.529160  data: 0.000595  max mem: 4023\n",
      "Epoch: [84/100]  [100/189]  eta: 0:00:49  loss: 6.461482 (6.461487)  lr: 0.000268 (0.000268)  wd: 0.000009 (0.000009)  time: 0.518218  data: 0.000177  max mem: 4023\n",
      "Epoch: [84/100]  [120/189]  eta: 0:00:38  loss: 6.461483 (6.461487)  lr: 0.000268 (0.000268)  wd: 0.000009 (0.000009)  time: 0.513783  data: 0.000127  max mem: 4023\n",
      "Epoch: [84/100]  [140/189]  eta: 0:00:26  loss: 6.461488 (6.461487)  lr: 0.000267 (0.000268)  wd: 0.000009 (0.000009)  time: 0.525129  data: 0.000145  max mem: 4023\n",
      "Epoch: [84/100]  [160/189]  eta: 0:00:15  loss: 6.461490 (6.461488)  lr: 0.000267 (0.000268)  wd: 0.000009 (0.000009)  time: 0.511928  data: 0.000244  max mem: 4023\n",
      "Epoch: [84/100]  [180/189]  eta: 0:00:04  loss: 6.461483 (6.461487)  lr: 0.000267 (0.000268)  wd: 0.000010 (0.000009)  time: 0.511113  data: 0.000129  max mem: 4023\n",
      "Epoch: [84/100]  [188/189]  eta: 0:00:00  loss: 6.461482 (6.461487)  lr: 0.000267 (0.000268)  wd: 0.000010 (0.000009)  time: 0.503945  data: 0.000080  max mem: 4023\n",
      "Epoch: [84/100] Total time: 0:01:41 (0.539460 s / it)\n",
      "Averaged stats: loss: 6.461482 (6.461487)  lr: 0.000267 (0.000268)  wd: 0.000010 (0.000009)\n",
      "Epoch: [85/100]  [  0/189]  eta: 0:13:25  loss: 6.461489 (6.461489)  lr: 0.000267 (0.000267)  wd: 0.000010 (0.000010)  time: 4.262731  data: 3.656722  max mem: 4023\n",
      "Epoch: [85/100]  [ 20/189]  eta: 0:01:58  loss: 6.461483 (6.461487)  lr: 0.000267 (0.000267)  wd: 0.000010 (0.000010)  time: 0.523322  data: 0.002558  max mem: 4023\n",
      "Epoch: [85/100]  [ 40/189]  eta: 0:01:31  loss: 6.461480 (6.461484)  lr: 0.000266 (0.000267)  wd: 0.000010 (0.000010)  time: 0.515755  data: 0.001458  max mem: 4023\n",
      "Epoch: [85/100]  [ 60/189]  eta: 0:01:14  loss: 6.461492 (6.461487)  lr: 0.000266 (0.000266)  wd: 0.000010 (0.000010)  time: 0.511798  data: 0.001170  max mem: 4023\n",
      "Epoch: [85/100]  [ 80/189]  eta: 0:01:01  loss: 6.461493 (6.461489)  lr: 0.000266 (0.000266)  wd: 0.000010 (0.000010)  time: 0.516174  data: 0.000113  max mem: 4023\n",
      "Epoch: [85/100]  [100/189]  eta: 0:00:49  loss: 6.461490 (6.461490)  lr: 0.000266 (0.000266)  wd: 0.000010 (0.000010)  time: 0.516772  data: 0.000165  max mem: 4023\n",
      "Epoch: [85/100]  [120/189]  eta: 0:00:38  loss: 6.461501 (6.461492)  lr: 0.000265 (0.000266)  wd: 0.000010 (0.000010)  time: 0.536545  data: 0.000137  max mem: 4023\n",
      "Epoch: [85/100]  [140/189]  eta: 0:00:26  loss: 6.461583 (6.462281)  lr: 0.000265 (0.000266)  wd: 0.000010 (0.000010)  time: 0.519673  data: 0.000134  max mem: 4023\n",
      "Epoch: [85/100]  [160/189]  eta: 0:00:15  loss: 6.462550 (6.462374)  lr: 0.000265 (0.000266)  wd: 0.000010 (0.000010)  time: 0.516974  data: 0.000305  max mem: 4023\n",
      "Epoch: [85/100]  [180/189]  eta: 0:00:04  loss: 6.461596 (6.462292)  lr: 0.000265 (0.000266)  wd: 0.000010 (0.000010)  time: 0.521944  data: 0.000864  max mem: 4023\n",
      "Epoch: [85/100]  [188/189]  eta: 0:00:00  loss: 6.461534 (6.462259)  lr: 0.000265 (0.000266)  wd: 0.000010 (0.000010)  time: 0.505919  data: 0.000077  max mem: 4023\n",
      "Epoch: [85/100] Total time: 0:01:41 (0.538880 s / it)\n",
      "Averaged stats: loss: 6.461534 (6.462259)  lr: 0.000265 (0.000266)  wd: 0.000010 (0.000010)\n",
      "Epoch: [86/100]  [  0/189]  eta: 0:12:36  loss: 6.461498 (6.461498)  lr: 0.000265 (0.000265)  wd: 0.000010 (0.000010)  time: 4.000839  data: 3.416431  max mem: 4023\n",
      "Epoch: [86/100]  [ 20/189]  eta: 0:01:55  loss: 6.461490 (6.461494)  lr: 0.000265 (0.000265)  wd: 0.000010 (0.000010)  time: 0.519173  data: 0.000193  max mem: 4023\n",
      "Epoch: [86/100]  [ 40/189]  eta: 0:01:30  loss: 6.461485 (6.461493)  lr: 0.000264 (0.000264)  wd: 0.000010 (0.000010)  time: 0.525743  data: 0.000181  max mem: 4023\n",
      "Epoch: [86/100]  [ 60/189]  eta: 0:01:14  loss: 6.461479 (6.461489)  lr: 0.000264 (0.000264)  wd: 0.000010 (0.000010)  time: 0.521037  data: 0.000327  max mem: 4023\n",
      "Epoch: [86/100]  [ 80/189]  eta: 0:01:01  loss: 6.461478 (6.461487)  lr: 0.000264 (0.000264)  wd: 0.000010 (0.000010)  time: 0.517099  data: 0.000135  max mem: 4023\n",
      "Epoch: [86/100]  [100/189]  eta: 0:00:49  loss: 6.461501 (6.461491)  lr: 0.000264 (0.000264)  wd: 0.000010 (0.000010)  time: 0.516117  data: 0.001090  max mem: 4023\n",
      "Epoch: [86/100]  [120/189]  eta: 0:00:37  loss: 6.461504 (6.461495)  lr: 0.000263 (0.000264)  wd: 0.000010 (0.000010)  time: 0.513202  data: 0.000915  max mem: 4023\n",
      "Epoch: [86/100]  [140/189]  eta: 0:00:26  loss: 6.461495 (6.461496)  lr: 0.000263 (0.000264)  wd: 0.000010 (0.000010)  time: 0.514099  data: 0.000694  max mem: 4023\n",
      "Epoch: [86/100]  [160/189]  eta: 0:00:15  loss: 6.461483 (6.461495)  lr: 0.000263 (0.000264)  wd: 0.000010 (0.000010)  time: 0.513655  data: 0.000139  max mem: 4023\n",
      "Epoch: [86/100]  [180/189]  eta: 0:00:04  loss: 6.461506 (6.461500)  lr: 0.000263 (0.000264)  wd: 0.000010 (0.000010)  time: 0.524117  data: 0.000275  max mem: 4023\n",
      "Epoch: [86/100]  [188/189]  eta: 0:00:00  loss: 6.461556 (6.461503)  lr: 0.000263 (0.000264)  wd: 0.000010 (0.000010)  time: 0.519304  data: 0.000074  max mem: 4023\n",
      "Epoch: [86/100] Total time: 0:01:41 (0.536960 s / it)\n",
      "Averaged stats: loss: 6.461556 (6.461503)  lr: 0.000263 (0.000264)  wd: 0.000010 (0.000010)\n",
      "Epoch: [87/100]  [  0/189]  eta: 0:10:28  loss: 6.461498 (6.461498)  lr: 0.000263 (0.000263)  wd: 0.000010 (0.000010)  time: 3.323339  data: 2.677592  max mem: 4023\n",
      "Epoch: [87/100]  [ 20/189]  eta: 0:01:52  loss: 6.461504 (6.461511)  lr: 0.000263 (0.000263)  wd: 0.000010 (0.000010)  time: 0.530441  data: 0.000442  max mem: 4023\n",
      "Epoch: [87/100]  [ 40/189]  eta: 0:01:29  loss: 6.461504 (6.461508)  lr: 0.000262 (0.000262)  wd: 0.000010 (0.000010)  time: 0.530325  data: 0.001580  max mem: 4023\n",
      "Epoch: [87/100]  [ 60/189]  eta: 0:01:13  loss: 6.461524 (6.461512)  lr: 0.000262 (0.000262)  wd: 0.000010 (0.000010)  time: 0.521062  data: 0.000154  max mem: 4023\n",
      "Epoch: [87/100]  [ 80/189]  eta: 0:01:00  loss: 6.461553 (6.461526)  lr: 0.000262 (0.000262)  wd: 0.000010 (0.000010)  time: 0.515230  data: 0.000173  max mem: 4023\n",
      "Epoch: [87/100]  [100/189]  eta: 0:00:49  loss: 6.461501 (6.461522)  lr: 0.000262 (0.000262)  wd: 0.000010 (0.000010)  time: 0.518957  data: 0.000129  max mem: 4023\n",
      "Epoch: [87/100]  [120/189]  eta: 0:00:37  loss: 6.461536 (6.461527)  lr: 0.000262 (0.000262)  wd: 0.000010 (0.000010)  time: 0.524040  data: 0.000150  max mem: 4023\n",
      "Epoch: [87/100]  [140/189]  eta: 0:00:26  loss: 6.461520 (6.461526)  lr: 0.000261 (0.000262)  wd: 0.000010 (0.000010)  time: 0.519661  data: 0.000461  max mem: 4023\n",
      "Epoch: [87/100]  [160/189]  eta: 0:00:15  loss: 6.461510 (6.461525)  lr: 0.000261 (0.000262)  wd: 0.000010 (0.000010)  time: 0.516042  data: 0.000404  max mem: 4023\n",
      "Epoch: [87/100]  [180/189]  eta: 0:00:04  loss: 6.461512 (6.461524)  lr: 0.000261 (0.000262)  wd: 0.000010 (0.000010)  time: 0.524134  data: 0.000292  max mem: 4023\n",
      "Epoch: [87/100]  [188/189]  eta: 0:00:00  loss: 6.461502 (6.461523)  lr: 0.000261 (0.000262)  wd: 0.000010 (0.000010)  time: 0.516044  data: 0.000085  max mem: 4023\n",
      "Epoch: [87/100] Total time: 0:01:41 (0.536735 s / it)\n",
      "Averaged stats: loss: 6.461502 (6.461523)  lr: 0.000261 (0.000262)  wd: 0.000010 (0.000010)\n",
      "Epoch: [88/100]  [  0/189]  eta: 0:13:02  loss: 6.461493 (6.461493)  lr: 0.000261 (0.000261)  wd: 0.000010 (0.000010)  time: 4.137711  data: 3.536048  max mem: 4023\n",
      "Epoch: [88/100]  [ 20/189]  eta: 0:01:58  loss: 6.461491 (6.461499)  lr: 0.000261 (0.000261)  wd: 0.000010 (0.000010)  time: 0.530780  data: 0.013711  max mem: 4023\n",
      "Epoch: [88/100]  [ 40/189]  eta: 0:01:30  loss: 6.461512 (6.461508)  lr: 0.000261 (0.000261)  wd: 0.000010 (0.000010)  time: 0.510949  data: 0.000810  max mem: 4023\n",
      "Epoch: [88/100]  [ 60/189]  eta: 0:01:14  loss: 6.461550 (6.461535)  lr: 0.000260 (0.000261)  wd: 0.000010 (0.000010)  time: 0.513340  data: 0.000395  max mem: 4023\n",
      "Epoch: [88/100]  [ 80/189]  eta: 0:01:01  loss: 6.461517 (6.461536)  lr: 0.000260 (0.000260)  wd: 0.000010 (0.000010)  time: 0.535605  data: 0.000160  max mem: 4023\n",
      "Epoch: [88/100]  [100/189]  eta: 0:00:49  loss: 6.461519 (6.461536)  lr: 0.000260 (0.000260)  wd: 0.000010 (0.000010)  time: 0.525143  data: 0.000155  max mem: 4023\n",
      "Epoch: [88/100]  [120/189]  eta: 0:00:38  loss: 6.461545 (6.461542)  lr: 0.000260 (0.000260)  wd: 0.000010 (0.000010)  time: 0.529498  data: 0.000213  max mem: 4023\n",
      "Epoch: [88/100]  [140/189]  eta: 0:00:26  loss: 6.461657 (6.461569)  lr: 0.000260 (0.000260)  wd: 0.000010 (0.000010)  time: 0.511783  data: 0.000224  max mem: 4023\n",
      "Epoch: [88/100]  [160/189]  eta: 0:00:15  loss: 6.461611 (6.461583)  lr: 0.000259 (0.000260)  wd: 0.000010 (0.000010)  time: 0.522703  data: 0.000367  max mem: 4023\n",
      "Epoch: [88/100]  [180/189]  eta: 0:00:04  loss: 6.461558 (6.461583)  lr: 0.000259 (0.000260)  wd: 0.000010 (0.000010)  time: 0.516260  data: 0.000153  max mem: 4023\n",
      "Epoch: [88/100]  [188/189]  eta: 0:00:00  loss: 6.461559 (6.461586)  lr: 0.000259 (0.000260)  wd: 0.000010 (0.000010)  time: 0.521217  data: 0.000081  max mem: 4023\n",
      "Epoch: [88/100] Total time: 0:01:42 (0.541440 s / it)\n",
      "Averaged stats: loss: 6.461559 (6.461586)  lr: 0.000259 (0.000260)  wd: 0.000010 (0.000010)\n",
      "Epoch: [89/100]  [  0/189]  eta: 0:11:28  loss: 6.461511 (6.461511)  lr: 0.000259 (0.000259)  wd: 0.000010 (0.000010)  time: 3.643268  data: 2.988481  max mem: 4023\n",
      "Epoch: [89/100]  [ 20/189]  eta: 0:01:53  loss: 6.461514 (6.461557)  lr: 0.000259 (0.000259)  wd: 0.000010 (0.000010)  time: 0.521215  data: 0.001694  max mem: 4023\n",
      "Epoch: [89/100]  [ 40/189]  eta: 0:01:29  loss: 6.461500 (6.461541)  lr: 0.000259 (0.000259)  wd: 0.000010 (0.000010)  time: 0.525030  data: 0.000815  max mem: 4023\n",
      "Epoch: [89/100]  [ 60/189]  eta: 0:01:13  loss: 6.461562 (6.461554)  lr: 0.000259 (0.000259)  wd: 0.000010 (0.000010)  time: 0.512528  data: 0.000409  max mem: 4023\n",
      "Epoch: [89/100]  [ 80/189]  eta: 0:01:00  loss: 6.461554 (6.461560)  lr: 0.000258 (0.000259)  wd: 0.000010 (0.000010)  time: 0.519450  data: 0.000195  max mem: 4023\n",
      "Epoch: [89/100]  [100/189]  eta: 0:00:49  loss: 6.461527 (6.461561)  lr: 0.000258 (0.000259)  wd: 0.000010 (0.000010)  time: 0.532201  data: 0.001220  max mem: 4023\n",
      "Epoch: [89/100]  [120/189]  eta: 0:00:37  loss: 6.461553 (6.461559)  lr: 0.000258 (0.000259)  wd: 0.000010 (0.000010)  time: 0.512334  data: 0.000157  max mem: 4023\n",
      "Epoch: [89/100]  [140/189]  eta: 0:00:26  loss: 6.461574 (6.461563)  lr: 0.000258 (0.000259)  wd: 0.000010 (0.000010)  time: 0.511247  data: 0.000469  max mem: 4023\n",
      "Epoch: [89/100]  [160/189]  eta: 0:00:15  loss: 6.461628 (6.461580)  lr: 0.000258 (0.000258)  wd: 0.000010 (0.000010)  time: 0.510171  data: 0.000155  max mem: 4023\n",
      "Epoch: [89/100]  [180/189]  eta: 0:00:04  loss: 6.461910 (6.461654)  lr: 0.000258 (0.000258)  wd: 0.000010 (0.000010)  time: 0.530143  data: 0.000122  max mem: 4023\n",
      "Epoch: [89/100]  [188/189]  eta: 0:00:00  loss: 6.462062 (6.461661)  lr: 0.000258 (0.000258)  wd: 0.000010 (0.000010)  time: 0.522022  data: 0.000077  max mem: 4023\n",
      "Epoch: [89/100] Total time: 0:01:41 (0.535645 s / it)\n",
      "Averaged stats: loss: 6.462062 (6.461661)  lr: 0.000258 (0.000258)  wd: 0.000010 (0.000010)\n",
      "Epoch: [90/100]  [  0/189]  eta: 0:12:07  loss: 6.461605 (6.461605)  lr: 0.000258 (0.000258)  wd: 0.000010 (0.000010)  time: 3.849717  data: 3.267021  max mem: 4023\n",
      "Epoch: [90/100]  [ 20/189]  eta: 0:01:55  loss: 6.461644 (6.461674)  lr: 0.000257 (0.000257)  wd: 0.000010 (0.000010)  time: 0.522246  data: 0.000186  max mem: 4023\n",
      "Epoch: [90/100]  [ 40/189]  eta: 0:01:30  loss: 6.461526 (6.461612)  lr: 0.000257 (0.000257)  wd: 0.000010 (0.000010)  time: 0.527682  data: 0.001443  max mem: 4023\n",
      "Epoch: [90/100]  [ 60/189]  eta: 0:01:14  loss: 6.461577 (6.461611)  lr: 0.000257 (0.000257)  wd: 0.000010 (0.000010)  time: 0.525238  data: 0.000151  max mem: 4023\n",
      "Epoch: [90/100]  [ 80/189]  eta: 0:01:01  loss: 6.461510 (6.461595)  lr: 0.000257 (0.000257)  wd: 0.000010 (0.000010)  time: 0.511297  data: 0.000158  max mem: 4023\n",
      "Epoch: [90/100]  [100/189]  eta: 0:00:49  loss: 6.461678 (6.461619)  lr: 0.000257 (0.000257)  wd: 0.000010 (0.000010)  time: 0.518323  data: 0.000762  max mem: 4023\n",
      "Epoch: [90/100]  [120/189]  eta: 0:00:37  loss: 6.461573 (6.461612)  lr: 0.000257 (0.000257)  wd: 0.000010 (0.000010)  time: 0.514800  data: 0.000148  max mem: 4023\n",
      "Epoch: [90/100]  [140/189]  eta: 0:00:26  loss: 6.461533 (6.461607)  lr: 0.000257 (0.000257)  wd: 0.000010 (0.000010)  time: 0.540594  data: 0.000293  max mem: 4023\n",
      "Epoch: [90/100]  [160/189]  eta: 0:00:15  loss: 6.461562 (6.461608)  lr: 0.000256 (0.000257)  wd: 0.000010 (0.000010)  time: 0.522036  data: 0.000276  max mem: 4023\n",
      "Epoch: [90/100]  [180/189]  eta: 0:00:04  loss: 6.461577 (6.461614)  lr: 0.000256 (0.000257)  wd: 0.000010 (0.000010)  time: 0.524107  data: 0.000106  max mem: 4023\n",
      "Epoch: [90/100]  [188/189]  eta: 0:00:00  loss: 6.461535 (6.461611)  lr: 0.000256 (0.000257)  wd: 0.000010 (0.000010)  time: 0.500936  data: 0.000072  max mem: 4023\n",
      "Epoch: [90/100] Total time: 0:01:42 (0.539721 s / it)\n",
      "Averaged stats: loss: 6.461535 (6.461611)  lr: 0.000256 (0.000257)  wd: 0.000010 (0.000010)\n",
      "Epoch: [91/100]  [  0/189]  eta: 0:13:57  loss: 6.461658 (6.461658)  lr: 0.000256 (0.000256)  wd: 0.000010 (0.000010)  time: 4.430082  data: 3.828176  max mem: 4023\n",
      "Epoch: [91/100]  [ 20/189]  eta: 0:01:58  loss: 6.461592 (6.461627)  lr: 0.000256 (0.000256)  wd: 0.000010 (0.000010)  time: 0.514247  data: 0.000735  max mem: 4023\n",
      "Epoch: [91/100]  [ 40/189]  eta: 0:01:31  loss: 6.461554 (6.461602)  lr: 0.000256 (0.000256)  wd: 0.000010 (0.000010)  time: 0.516283  data: 0.002552  max mem: 4023\n",
      "Epoch: [91/100]  [ 60/189]  eta: 0:01:14  loss: 6.461504 (6.461580)  lr: 0.000256 (0.000256)  wd: 0.000010 (0.000010)  time: 0.520940  data: 0.000292  max mem: 4023\n",
      "Epoch: [91/100]  [ 80/189]  eta: 0:01:01  loss: 6.461604 (6.461599)  lr: 0.000256 (0.000256)  wd: 0.000010 (0.000010)  time: 0.523246  data: 0.000221  max mem: 4023\n",
      "Epoch: [91/100]  [100/189]  eta: 0:00:49  loss: 6.462027 (6.461694)  lr: 0.000255 (0.000256)  wd: 0.000010 (0.000010)  time: 0.526206  data: 0.000276  max mem: 4023\n",
      "Epoch: [91/100]  [120/189]  eta: 0:00:38  loss: 6.461718 (6.461704)  lr: 0.000255 (0.000256)  wd: 0.000010 (0.000010)  time: 0.519347  data: 0.000137  max mem: 4023\n",
      "Epoch: [91/100]  [140/189]  eta: 0:00:26  loss: 6.461659 (6.461698)  lr: 0.000255 (0.000256)  wd: 0.000010 (0.000010)  time: 0.520668  data: 0.000146  max mem: 4023\n",
      "Epoch: [91/100]  [160/189]  eta: 0:00:15  loss: 6.461531 (6.461678)  lr: 0.000255 (0.000256)  wd: 0.000010 (0.000010)  time: 0.511168  data: 0.000321  max mem: 4023\n",
      "Epoch: [91/100]  [180/189]  eta: 0:00:04  loss: 6.461563 (6.461671)  lr: 0.000255 (0.000255)  wd: 0.000010 (0.000010)  time: 0.510424  data: 0.000162  max mem: 4023\n",
      "Epoch: [91/100]  [188/189]  eta: 0:00:00  loss: 6.461563 (6.461666)  lr: 0.000255 (0.000255)  wd: 0.000010 (0.000010)  time: 0.513953  data: 0.000078  max mem: 4023\n",
      "Epoch: [91/100] Total time: 0:01:41 (0.539415 s / it)\n",
      "Averaged stats: loss: 6.461563 (6.461666)  lr: 0.000255 (0.000255)  wd: 0.000010 (0.000010)\n",
      "Epoch: [92/100]  [  0/189]  eta: 0:12:03  loss: 6.462267 (6.462267)  lr: 0.000255 (0.000255)  wd: 0.000010 (0.000010)  time: 3.830339  data: 3.268686  max mem: 4023\n",
      "Epoch: [92/100]  [ 20/189]  eta: 0:01:54  loss: 6.465131 (6.467124)  lr: 0.000255 (0.000255)  wd: 0.000010 (0.000010)  time: 0.521508  data: 0.002229  max mem: 4023\n",
      "Epoch: [92/100]  [ 40/189]  eta: 0:01:29  loss: 6.461753 (6.464662)  lr: 0.000255 (0.000255)  wd: 0.000010 (0.000010)  time: 0.525027  data: 0.000357  max mem: 4023\n",
      "Epoch: [92/100]  [ 60/189]  eta: 0:01:14  loss: 6.461648 (6.463681)  lr: 0.000255 (0.000255)  wd: 0.000010 (0.000010)  time: 0.521755  data: 0.000218  max mem: 4023\n",
      "Epoch: [92/100]  [ 80/189]  eta: 0:01:01  loss: 6.461596 (6.463174)  lr: 0.000254 (0.000255)  wd: 0.000010 (0.000010)  time: 0.512939  data: 0.000162  max mem: 4023\n",
      "Epoch: [92/100]  [100/189]  eta: 0:00:49  loss: 6.461533 (6.462853)  lr: 0.000254 (0.000255)  wd: 0.000010 (0.000010)  time: 0.518660  data: 0.000136  max mem: 4023\n",
      "Epoch: [92/100]  [120/189]  eta: 0:00:37  loss: 6.461532 (6.462640)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 0.520012  data: 0.000810  max mem: 4023\n",
      "Epoch: [92/100]  [140/189]  eta: 0:00:26  loss: 6.461586 (6.462504)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 0.522280  data: 0.000132  max mem: 4023\n",
      "Epoch: [92/100]  [160/189]  eta: 0:00:15  loss: 6.461646 (6.462409)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 0.517255  data: 0.000321  max mem: 4023\n",
      "Epoch: [92/100]  [180/189]  eta: 0:00:04  loss: 6.461577 (6.462317)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 0.524932  data: 0.000124  max mem: 4023\n",
      "Epoch: [92/100]  [188/189]  eta: 0:00:00  loss: 6.461526 (6.462290)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 0.516023  data: 0.000080  max mem: 4023\n",
      "Epoch: [92/100] Total time: 0:01:41 (0.538633 s / it)\n",
      "Averaged stats: loss: 6.461526 (6.462290)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)\n",
      "Epoch: [93/100]  [  0/189]  eta: 0:12:37  loss: 6.461630 (6.461630)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 4.006243  data: 3.417264  max mem: 4023\n",
      "Epoch: [93/100]  [ 20/189]  eta: 0:01:56  loss: 6.461534 (6.461573)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 0.523681  data: 0.002770  max mem: 4023\n",
      "Epoch: [93/100]  [ 40/189]  eta: 0:01:30  loss: 6.461527 (6.461564)  lr: 0.000254 (0.000254)  wd: 0.000010 (0.000010)  time: 0.521894  data: 0.001860  max mem: 4023\n",
      "Epoch: [93/100]  [ 60/189]  eta: 0:01:14  loss: 6.461504 (6.461555)  lr: 0.000253 (0.000254)  wd: 0.000010 (0.000010)  time: 0.510037  data: 0.001045  max mem: 4023\n",
      "Epoch: [93/100]  [ 80/189]  eta: 0:01:01  loss: 6.461568 (6.461593)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.512097  data: 0.000306  max mem: 4023\n",
      "Epoch: [93/100]  [100/189]  eta: 0:00:49  loss: 6.461666 (6.461610)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.517166  data: 0.000404  max mem: 4023\n",
      "Epoch: [93/100]  [120/189]  eta: 0:00:37  loss: 6.461552 (6.461607)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.524990  data: 0.000364  max mem: 4023\n",
      "Epoch: [93/100]  [140/189]  eta: 0:00:26  loss: 6.461529 (6.461601)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.530953  data: 0.000130  max mem: 4023\n",
      "Epoch: [93/100]  [160/189]  eta: 0:00:15  loss: 6.461491 (6.461594)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.516229  data: 0.000228  max mem: 4023\n",
      "Epoch: [93/100]  [180/189]  eta: 0:00:04  loss: 6.461598 (6.461598)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.517090  data: 0.000138  max mem: 4023\n",
      "Epoch: [93/100]  [188/189]  eta: 0:00:00  loss: 6.461572 (6.461599)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.516914  data: 0.000086  max mem: 4023\n",
      "Epoch: [93/100] Total time: 0:01:41 (0.537661 s / it)\n",
      "Averaged stats: loss: 6.461572 (6.461599)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)\n",
      "Epoch: [94/100]  [  0/189]  eta: 0:13:55  loss: 6.461550 (6.461550)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 4.420358  data: 3.827271  max mem: 4023\n",
      "Epoch: [94/100]  [ 20/189]  eta: 0:02:00  loss: 6.461491 (6.461497)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.525203  data: 0.002173  max mem: 4023\n",
      "Epoch: [94/100]  [ 40/189]  eta: 0:01:32  loss: 6.461496 (6.461502)  lr: 0.000253 (0.000253)  wd: 0.000010 (0.000010)  time: 0.519804  data: 0.001342  max mem: 4023\n",
      "Epoch: [94/100]  [ 60/189]  eta: 0:01:15  loss: 6.461490 (6.461507)  lr: 0.000252 (0.000253)  wd: 0.000010 (0.000010)  time: 0.526882  data: 0.000364  max mem: 4023\n",
      "Epoch: [94/100]  [ 80/189]  eta: 0:01:01  loss: 6.461486 (6.461507)  lr: 0.000252 (0.000253)  wd: 0.000010 (0.000010)  time: 0.510755  data: 0.000157  max mem: 4023\n",
      "Epoch: [94/100]  [100/189]  eta: 0:00:50  loss: 6.461512 (6.461517)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.534753  data: 0.000829  max mem: 4023\n",
      "Epoch: [94/100]  [120/189]  eta: 0:00:38  loss: 6.461503 (6.461518)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.515498  data: 0.000153  max mem: 4023\n",
      "Epoch: [94/100]  [140/189]  eta: 0:00:26  loss: 6.461493 (6.461518)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.522452  data: 0.000478  max mem: 4023\n",
      "Epoch: [94/100]  [160/189]  eta: 0:00:15  loss: 6.461480 (6.461519)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.521666  data: 0.000932  max mem: 4023\n",
      "Epoch: [94/100]  [180/189]  eta: 0:00:04  loss: 6.461545 (6.461523)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.510078  data: 0.000133  max mem: 4023\n",
      "Epoch: [94/100]  [188/189]  eta: 0:00:00  loss: 6.461545 (6.461534)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.508200  data: 0.000088  max mem: 4023\n",
      "Epoch: [94/100] Total time: 0:01:42 (0.541284 s / it)\n",
      "Averaged stats: loss: 6.461545 (6.461534)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)\n",
      "Epoch: [95/100]  [  0/189]  eta: 0:12:00  loss: 6.461566 (6.461566)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 3.812254  data: 3.202270  max mem: 4023\n",
      "Epoch: [95/100]  [ 20/189]  eta: 0:02:02  loss: 6.461539 (6.461572)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.571564  data: 0.051180  max mem: 4023\n",
      "Epoch: [95/100]  [ 40/189]  eta: 0:01:32  loss: 6.461487 (6.461537)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.512420  data: 0.000811  max mem: 4023\n",
      "Epoch: [95/100]  [ 60/189]  eta: 0:01:15  loss: 6.461523 (6.461639)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.515556  data: 0.000348  max mem: 4023\n",
      "Epoch: [95/100]  [ 80/189]  eta: 0:01:01  loss: 6.461822 (6.461717)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.511977  data: 0.000318  max mem: 4023\n",
      "Epoch: [95/100]  [100/189]  eta: 0:00:49  loss: 6.461577 (6.461692)  lr: 0.000252 (0.000252)  wd: 0.000010 (0.000010)  time: 0.519585  data: 0.000152  max mem: 4023\n",
      "Epoch: [95/100]  [120/189]  eta: 0:00:38  loss: 6.461534 (6.461675)  lr: 0.000251 (0.000252)  wd: 0.000010 (0.000010)  time: 0.522212  data: 0.000942  max mem: 4023\n",
      "Epoch: [95/100]  [140/189]  eta: 0:00:26  loss: 6.461545 (6.461658)  lr: 0.000251 (0.000252)  wd: 0.000010 (0.000010)  time: 0.513194  data: 0.000332  max mem: 4023\n",
      "Epoch: [95/100]  [160/189]  eta: 0:00:15  loss: 6.461518 (6.461645)  lr: 0.000251 (0.000252)  wd: 0.000010 (0.000010)  time: 0.512390  data: 0.000179  max mem: 4023\n",
      "Epoch: [95/100]  [180/189]  eta: 0:00:04  loss: 6.461508 (6.461631)  lr: 0.000251 (0.000252)  wd: 0.000010 (0.000010)  time: 0.517413  data: 0.000121  max mem: 4023\n",
      "Epoch: [95/100]  [188/189]  eta: 0:00:00  loss: 6.461499 (6.461626)  lr: 0.000251 (0.000252)  wd: 0.000010 (0.000010)  time: 0.523305  data: 0.000085  max mem: 4023\n",
      "Epoch: [95/100] Total time: 0:01:42 (0.539838 s / it)\n",
      "Averaged stats: loss: 6.461499 (6.461626)  lr: 0.000251 (0.000252)  wd: 0.000010 (0.000010)\n",
      "Epoch: [96/100]  [  0/189]  eta: 0:13:52  loss: 6.461707 (6.461707)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 4.406939  data: 3.835735  max mem: 4023\n",
      "Epoch: [96/100]  [ 20/189]  eta: 0:01:58  loss: 6.461490 (6.461535)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.515009  data: 0.000189  max mem: 4023\n",
      "Epoch: [96/100]  [ 40/189]  eta: 0:01:30  loss: 6.461511 (6.461549)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.513424  data: 0.000994  max mem: 4023\n",
      "Epoch: [96/100]  [ 60/189]  eta: 0:01:14  loss: 6.461488 (6.461541)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.514340  data: 0.000340  max mem: 4023\n",
      "Epoch: [96/100]  [ 80/189]  eta: 0:01:01  loss: 6.461557 (6.461566)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.510929  data: 0.000207  max mem: 4023\n",
      "Epoch: [96/100]  [100/189]  eta: 0:00:49  loss: 6.461543 (6.461569)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.532132  data: 0.000295  max mem: 4023\n",
      "Epoch: [96/100]  [120/189]  eta: 0:00:37  loss: 6.461509 (6.461563)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.519541  data: 0.000178  max mem: 4023\n",
      "Epoch: [96/100]  [140/189]  eta: 0:00:26  loss: 6.461505 (6.461559)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.520229  data: 0.000269  max mem: 4023\n",
      "Epoch: [96/100]  [160/189]  eta: 0:00:15  loss: 6.461493 (6.461554)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.513034  data: 0.000538  max mem: 4023\n",
      "Epoch: [96/100]  [180/189]  eta: 0:00:04  loss: 6.461504 (6.461549)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.517621  data: 0.000125  max mem: 4023\n",
      "Epoch: [96/100]  [188/189]  eta: 0:00:00  loss: 6.461518 (6.461548)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.503930  data: 0.000077  max mem: 4023\n",
      "Epoch: [96/100] Total time: 0:01:41 (0.537558 s / it)\n",
      "Averaged stats: loss: 6.461518 (6.461548)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)\n",
      "Epoch: [97/100]  [  0/189]  eta: 0:13:55  loss: 6.461894 (6.461894)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 4.421098  data: 3.787608  max mem: 4023\n",
      "Epoch: [97/100]  [ 20/189]  eta: 0:01:58  loss: 6.461555 (6.461591)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.515272  data: 0.000248  max mem: 4023\n",
      "Epoch: [97/100]  [ 40/189]  eta: 0:01:30  loss: 6.461581 (6.461601)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.510183  data: 0.000954  max mem: 4023\n",
      "Epoch: [97/100]  [ 60/189]  eta: 0:01:14  loss: 6.461613 (6.461620)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.511686  data: 0.000712  max mem: 4023\n",
      "Epoch: [97/100]  [ 80/189]  eta: 0:01:01  loss: 6.461505 (6.461593)  lr: 0.000251 (0.000251)  wd: 0.000010 (0.000010)  time: 0.522774  data: 0.000626  max mem: 4023\n",
      "Epoch: [97/100]  [100/189]  eta: 0:00:49  loss: 6.461498 (6.461578)  lr: 0.000250 (0.000251)  wd: 0.000010 (0.000010)  time: 0.512636  data: 0.000233  max mem: 4023\n",
      "Epoch: [97/100]  [120/189]  eta: 0:00:37  loss: 6.461527 (6.461572)  lr: 0.000250 (0.000251)  wd: 0.000010 (0.000010)  time: 0.526870  data: 0.000353  max mem: 4023\n",
      "Epoch: [97/100]  [140/189]  eta: 0:00:26  loss: 6.461535 (6.461569)  lr: 0.000250 (0.000251)  wd: 0.000010 (0.000010)  time: 0.511040  data: 0.000165  max mem: 4023\n",
      "Epoch: [97/100]  [160/189]  eta: 0:00:15  loss: 6.461561 (6.461570)  lr: 0.000250 (0.000251)  wd: 0.000010 (0.000010)  time: 0.512892  data: 0.000372  max mem: 4023\n",
      "Epoch: [97/100]  [180/189]  eta: 0:00:04  loss: 6.461580 (6.461578)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.514852  data: 0.000122  max mem: 4023\n",
      "Epoch: [97/100]  [188/189]  eta: 0:00:00  loss: 6.461535 (6.461575)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.507975  data: 0.000075  max mem: 4023\n",
      "Epoch: [97/100] Total time: 0:01:41 (0.535534 s / it)\n",
      "Averaged stats: loss: 6.461535 (6.461575)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)\n",
      "Epoch: [98/100]  [  0/189]  eta: 0:12:23  loss: 6.461563 (6.461563)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 3.932767  data: 3.312759  max mem: 4023\n",
      "Epoch: [98/100]  [ 20/189]  eta: 0:01:54  loss: 6.461519 (6.461538)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.514539  data: 0.000905  max mem: 4023\n",
      "Epoch: [98/100]  [ 40/189]  eta: 0:01:28  loss: 6.461607 (6.461568)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.507120  data: 0.000292  max mem: 4023\n",
      "Epoch: [98/100]  [ 60/189]  eta: 0:01:13  loss: 6.461554 (6.461577)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.522824  data: 0.000261  max mem: 4023\n",
      "Epoch: [98/100]  [ 80/189]  eta: 0:01:00  loss: 6.461530 (6.461565)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.515094  data: 0.000156  max mem: 4023\n",
      "Epoch: [98/100]  [100/189]  eta: 0:00:48  loss: 6.461493 (6.461559)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.520669  data: 0.000368  max mem: 4023\n",
      "Epoch: [98/100]  [120/189]  eta: 0:00:37  loss: 6.461512 (6.461558)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.516864  data: 0.000373  max mem: 4023\n",
      "Epoch: [98/100]  [140/189]  eta: 0:00:26  loss: 6.461574 (6.461562)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.522223  data: 0.000227  max mem: 4023\n",
      "Epoch: [98/100]  [160/189]  eta: 0:00:15  loss: 6.461543 (6.461563)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.520000  data: 0.000159  max mem: 4023\n",
      "Epoch: [98/100]  [180/189]  eta: 0:00:04  loss: 6.461548 (6.461568)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.510264  data: 0.000152  max mem: 4023\n",
      "Epoch: [98/100]  [188/189]  eta: 0:00:00  loss: 6.461555 (6.461569)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.507124  data: 0.000102  max mem: 4023\n",
      "Epoch: [98/100] Total time: 0:01:41 (0.534429 s / it)\n",
      "Averaged stats: loss: 6.461555 (6.461569)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)\n",
      "Epoch: [99/100]  [  0/189]  eta: 0:12:47  loss: 6.461519 (6.461519)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 4.058659  data: 3.440080  max mem: 4023\n",
      "Epoch: [99/100]  [ 20/189]  eta: 0:01:56  loss: 6.461559 (6.461588)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.523042  data: 0.001001  max mem: 4023\n",
      "Epoch: [99/100]  [ 40/189]  eta: 0:01:29  loss: 6.461529 (6.461572)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.506578  data: 0.000173  max mem: 4023\n",
      "Epoch: [99/100]  [ 60/189]  eta: 0:01:13  loss: 6.461527 (6.461567)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.510420  data: 0.000421  max mem: 4023\n",
      "Epoch: [99/100]  [ 80/189]  eta: 0:01:00  loss: 6.461506 (6.461560)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.511499  data: 0.000482  max mem: 4023\n",
      "Epoch: [99/100]  [100/189]  eta: 0:00:48  loss: 6.461495 (6.461547)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.519957  data: 0.000159  max mem: 4023\n",
      "Epoch: [99/100]  [120/189]  eta: 0:00:37  loss: 6.461489 (6.461540)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.520981  data: 0.000224  max mem: 4023\n",
      "Epoch: [99/100]  [140/189]  eta: 0:00:26  loss: 6.461479 (6.461533)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.510361  data: 0.000319  max mem: 4023\n",
      "Epoch: [99/100]  [160/189]  eta: 0:00:15  loss: 6.461751 (6.461567)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.523265  data: 0.000149  max mem: 4023\n",
      "Epoch: [99/100]  [180/189]  eta: 0:00:04  loss: 6.461657 (6.461578)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.514614  data: 0.000640  max mem: 4023\n",
      "Epoch: [99/100]  [188/189]  eta: 0:00:00  loss: 6.461597 (6.461578)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)  time: 0.509885  data: 0.000606  max mem: 4023\n",
      "Epoch: [99/100] Total time: 0:01:40 (0.534264 s / it)\n",
      "Averaged stats: loss: 6.461597 (6.461578)  lr: 0.000250 (0.000250)  wd: 0.000010 (0.000010)\n",
      "Training time 2:52:27\n"
     ]
    }
   ],
   "source": [
    "# Treinar EfficientNet pré-treinada com o  DINO\n",
    "! python -m torch.distributed.launch --nproc_per_node=1 main_dino.py --arch efficientnet_b0 --pretrained True --optimizer adam --batch_size_per_gpu 64 --min_lr  0.00025 --weight_decay 1e-6 --weight_decay_end 1e-5 --num_workers 8 --out_dim 640 --data_path ./datasets/chestx-ray14-v3/train/ --output_dir ./output/DINOXray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c6da4-a547-4cca-a7e8-0a47b69229ca",
   "metadata": {},
   "source": [
    "## Salvar pesos do backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0881466e-7504-43e4-bd32-9adaaf3a2110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint ./output/DINOXray/checkpoint.pth was loaded\n",
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): ConvNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): ConvNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): ConvNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "def weights_update(model, checkpoint_path, key):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in checkpoint[key].items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f'Checkpoint {checkpoint_path} was loaded')\n",
    "    return model\n",
    "\n",
    "save_model_path = './output/DINOXray'\n",
    "model = efficientnet_b0()\n",
    "model = weights_update(model, f\"{save_model_path}/checkpoint.pth\", 'teacher')\n",
    "\n",
    "model_backbone_weights = model\n",
    "print(model_backbone_weights)\n",
    "torch.save({\n",
    "            'model_state_dict': model_backbone_weights.state_dict(),\n",
    "            }, f'{save_model_path}/efficientnet_b0_backbone_weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8543e-8b8b-4682-a2bf-8a9e73d45b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
