{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad9ad46-a972-4963-8fc0-9bd3a31d2c0a",
   "metadata": {},
   "source": [
    "# VICREG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532303f0-b10b-4077-803f-e2e544ecd301",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e49ae9-86d3-49b8-bbf2-16784868e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 27 12:53:43 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN Xp     Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 28%   53C    P0    81W / 250W |      2MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74bd7a04-0630-4197-acc5-37bf4a5f2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import transforms\n",
    "import torch.distributed as dist\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def get_byol_transforms(size, mean, std):\n",
    "    transformT = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(size=size, scale=(0.08,1), ratio=(3 / 4, 4 / 3)),\n",
    "        transforms.RandomRotation((-90, 90)),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.GaussianBlur(kernel_size=(23,23), sigma=(0.1, 2.0)),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "    transformT1 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(size=size, scale=(0.08,1), ratio=(3 / 4, 4 / 3)),\n",
    "        transforms.RandomRotation((-90, 90)),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=(23,23), sigma=(0.1, 2.0)),\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "    transformEvalT = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(size=size),\n",
    "        transforms.Normalize(mean, std),        \n",
    "    ])\n",
    "\n",
    "    return transformT, transformT1, transformEvalT\n",
    "\n",
    "def get_cxr_transforms(size=224, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "        transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    transform_eval = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(size=size),\n",
    "        transforms.Normalize(mean, std),        \n",
    "    ])\n",
    "\n",
    "    return transform, transform, transform_eval\n",
    "\n",
    "\n",
    "class MultiViewDataInjector(object):\n",
    "    def __init__(self, *args):\n",
    "        self.transforms = args[0]\n",
    "        self.random_flip = transforms.RandomHorizontalFlip()\n",
    "\n",
    "    def __call__(self, sample, *with_consistent_flipping):\n",
    "        if with_consistent_flipping:\n",
    "            sample = self.random_flip(sample)\n",
    "        output = [transform(sample) for transform in self.transforms]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1580723f-9b2d-44d4-9e62-a7dcafa6a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "    input_size = 2048,\n",
    "    output_size = 8192,\n",
    "    depth = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        inp = input_size\n",
    "        for d in range(depth):\n",
    "            if d == depth - 1:\n",
    "                layers.append(nn.Linear(inp, output_size))\n",
    "            else:\n",
    "                layers.extend([nn.Linear(inp, output_size), nn.BatchNorm1d(output_size), nn.ReLU(inplace=True)])\n",
    "                inp = output_size\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class VicReg(nn.Module):\n",
    "    def __init__(self,\n",
    "    backend = 'resnet50',\n",
    "    input_size = 2048,\n",
    "    output_size = 8192,\n",
    "    depth_projector = 3,\n",
    "    lmbd = 5e-3, u = 1, v= 1, epsilon = 1e-3):\n",
    "\n",
    "        super().__init__()\n",
    "        self.backend = backend\n",
    "        self.projector = MLP(input_size=input_size, output_size=output_size, depth=depth_projector)\n",
    "        self.output_size = output_size\n",
    "        self.epsilon = epsilon\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        self.lmbd = lmbd\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = self.backend(x)\n",
    "        y = self.backend(y)\n",
    "        x = self.projector(x)\n",
    "        y = self.projector(y)\n",
    "\n",
    "        bs = x.size(0)\n",
    "    \n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "    \n",
    "        # x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
    "        # y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "    \n",
    "        std_x = torch.sqrt(x.var(dim=0) + self.epsilon)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + self.epsilon)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "    \n",
    "        cov_x = (x.T @ x) / (bs - 1)\n",
    "        cov_y = (y.T @ y) / (bs - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(self.output_size) + off_diagonal(cov_y).pow_(2).sum().div(self.output_size)\n",
    "    \n",
    "        loss = (\n",
    "            self.u * repr_loss\n",
    "            + self.v * std_loss\n",
    "            + self.lmbd * cov_loss\n",
    "        )\n",
    "        \n",
    "        # Free tensors\n",
    "        del x, y, cov_x, cov_y, std_x, std_y\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725f8ccc-429b-4394-b5e8-67fa1eded88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/noahgolmant/pytorch-lars\n",
    "\"\"\" Layer-wise adaptive rate scaling for SGD in PyTorch! \"\"\"\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    r\"\"\"Implements layer-wise adaptive rate scaling for SGD.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): base learning rate (\\gamma_0)\n",
    "        momentum (float, optional): momentum factor (default: 0) (\"m\")\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "            (\"\\beta\")\n",
    "        eta (float, optional): LARS coefficient\n",
    "        max_epoch: maximum training epoch to determine polynomial LR decay.\n",
    "\n",
    "    Based on Algorithm 1 of the following paper by You, Gitman, and Ginsburg.\n",
    "    Large Batch Training of Convolutional Networks:\n",
    "        https://arxiv.org/abs/1708.03888\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = LARS(model.parameters(), lr=0.1, eta=1e-3)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, momentum=.9,\n",
    "                 weight_decay=.0005, eta=0.001, max_epoch=200):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\"\n",
    "                             .format(weight_decay))\n",
    "        if eta < 0.0:\n",
    "            raise ValueError(\"Invalid LARS coefficient value: {}\".format(eta))\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(lr=lr, momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        eta=eta, max_epoch=max_epoch)\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "            epoch: current epoch to calculate polynomial LR decay schedule.\n",
    "                   if None, uses self.epoch and increments it.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            eta = group['eta']\n",
    "            lr = group['lr']\n",
    "            max_epoch = group['max_epoch']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                weight_norm = torch.norm(p.data)\n",
    "                grad_norm = torch.norm(d_p)\n",
    "\n",
    "                # Global LR computed on polynomial decay schedule\n",
    "                decay = (1 - float(epoch) / max_epoch) ** 2\n",
    "                global_lr = lr * decay\n",
    "\n",
    "                # Compute local learning rate for this layer\n",
    "                local_lr = eta * weight_norm / \\\n",
    "                    (grad_norm + weight_decay * weight_norm)\n",
    "\n",
    "                # Update the momentum term\n",
    "                actual_lr = local_lr * global_lr\n",
    "\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = \\\n",
    "                            torch.zeros_like(p.data)\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
    "                p.data.add_(-buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47657b62-7560-42a1-88fa-1bbf9f8ce111",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7605d35-ce76-4793-8004-dbf1b7c06cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2116e62-16fa-4b0f-be96-56172e346317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "base_lr = 0.00025\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79adc513-ff86-40c1-ba0c-9a8328ba1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "transform_x1, transform_x2, transform_test = get_cxr_transforms()\n",
    "\n",
    "dataset_path = './datasets/chestx-ray14-v3'\n",
    "train_dataset = datasets.ImageFolder(root=f'{dataset_path}/train', transform=MultiViewDataInjector([transform_x1, transform_x2]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=f'{dataset_path}/test', transform=MultiViewDataInjector([transform_x1, transform_x2]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed95bcf-71c5-441d-a356-724546f61d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "def train_loop(model, epoch, optimizer, train_loader, device):\n",
    "    tk0 = tqdm(train_loader)\n",
    "    train_loss = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for (x, x1), _ in tk0:\n",
    "        x = x.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        loss = model.forward(x, x1)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step(epoch)\n",
    "\n",
    "        # Free tensors\n",
    "        del x, x1, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return train_loss, time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "946de44a-ec7a-428f-80b0-85736e0067c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VicReg(\n",
       "  (backend): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): ConvNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): ConvNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): ConvNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=True)\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (projector): MLP(\n",
       "    (layer): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (1): BatchNorm1d(5120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "      (4): BatchNorm1d(5120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# Load model\n",
    "model = efficientnet_b0(pretrained=True)\n",
    "embed_dim = model.classifier[1].in_features\n",
    "output_features = embed_dim*4\n",
    "model.classifier[1] = nn.Identity()\n",
    "model = VicReg(input_size=embed_dim, output_size=output_features, backend=model, depth_projector=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b516ff9-e4bd-4466-8474-ffddf7e0e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:11<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]  loss: 0.8958430943034944, time: 191.5934774875641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:13<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/100]  loss: 0.8951006262075334, time: 193.77315425872803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:16<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/100]  loss: 0.8942718471168841, time: 196.10740327835083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:16<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/100]  loss: 0.8931942925882087, time: 196.9852659702301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/100]  loss: 0.8926089172640805, time: 198.52095651626587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/100]  loss: 0.8917242310665272, time: 199.83911752700806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/100]  loss: 0.8909359654736897, time: 199.70517992973328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/100]  loss: 0.8904558727665554, time: 199.34570908546448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/100]  loss: 0.8898204147184967, time: 199.2631320953369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/100]  loss: 0.8892012079241415, time: 198.9500434398651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/100]  loss: 0.8883832801902105, time: 198.2771406173706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/100]  loss: 0.8874900831431939, time: 199.02566385269165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/100]  loss: 0.8870017533264463, time: 198.25108432769775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/100]  loss: 0.8862234368210747, time: 198.12313508987427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/100]  loss: 0.8857569232504204, time: 199.0055365562439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/100]  loss: 0.8852145397473895, time: 198.97399878501892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17/100]  loss: 0.8842151568680213, time: 199.3594663143158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18/100]  loss: 0.8833925225747326, time: 200.05258202552795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19/100]  loss: 0.8828140889841413, time: 200.24986600875854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/100]  loss: 0.8823403766546299, time: 200.4049482345581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21/100]  loss: 0.8814554316972298, time: 200.52200150489807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22/100]  loss: 0.8811780852930886, time: 200.34282636642456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23/100]  loss: 0.8799124681760394, time: 200.70960998535156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24/100]  loss: 0.8795196123854824, time: 201.70652151107788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25/100]  loss: 0.8788522567067828, time: 200.92377829551697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26/100]  loss: 0.8783283039690957, time: 201.77794289588928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27/100]  loss: 0.8779435110470605, time: 201.71964120864868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28/100]  loss: 0.8769346992805521, time: 201.80504512786865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29/100]  loss: 0.87634478170405, time: 202.1645007133484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30/100]  loss: 0.875686980586834, time: 202.52016401290894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31/100]  loss: 0.8749421869636212, time: 202.31433939933777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32/100]  loss: 0.8745006625614469, time: 202.35027360916138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33/100]  loss: 0.8739768157875727, time: 201.39208817481995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34/100]  loss: 0.8732498553064134, time: 201.25915026664734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35/100]  loss: 0.872496997710889, time: 201.46399927139282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36/100]  loss: 0.8720572362185786, time: 200.41816115379333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37/100]  loss: 0.8716130792779266, time: 201.05916547775269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38/100]  loss: 0.8708564798352579, time: 201.07016324996948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39/100]  loss: 0.870528535868125, time: 200.9608817100525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40/100]  loss: 0.8694204587154287, time: 200.4762098789215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41/100]  loss: 0.8692345655469037, time: 200.2870659828186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [42/100]  loss: 0.868613305861357, time: 199.73852133750916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43/100]  loss: 0.8679208785453171, time: 199.14261054992676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [44/100]  loss: 0.8675287189307036, time: 199.47419142723083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [45/100]  loss: 0.8671657659073986, time: 199.8659405708313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46/100]  loss: 0.8663436243773768, time: 199.29248809814453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47/100]  loss: 0.8663930856676959, time: 199.54005551338196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [48/100]  loss: 0.8658899885636789, time: 199.157719373703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [49/100]  loss: 0.8649519472210495, time: 199.18470811843872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50/100]  loss: 0.8645683438689621, time: 198.76797580718994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51/100]  loss: 0.8640688949476474, time: 198.34739756584167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [52/100]  loss: 0.8636427668351976, time: 198.33520221710205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [53/100]  loss: 0.8630457443219645, time: 198.50255823135376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [54/100]  loss: 0.862727849414109, time: 199.1151692867279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [55/100]  loss: 0.8623525495567019, time: 199.2866427898407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [56/100]  loss: 0.8615595532788171, time: 199.70905876159668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [57/100]  loss: 0.8610990509469673, time: 200.01586318016052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [58/100]  loss: 0.8607371151447296, time: 200.50033020973206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [59/100]  loss: 0.8604432908946245, time: 200.73246145248413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [60/100]  loss: 0.8598744796697425, time: 201.20284962654114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [61/100]  loss: 0.8593640849388465, time: 200.2835955619812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [62/100]  loss: 0.8589191942934006, time: 200.39198446273804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [63/100]  loss: 0.8584460892059185, time: 201.1406705379486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [64/100]  loss: 0.8582978448855183, time: 200.95063710212708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [65/100]  loss: 0.8574250863973426, time: 200.8277280330658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [66/100]  loss: 0.857158369330502, time: 201.01833319664001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [67/100]  loss: 0.8569873353791615, time: 201.88142275810242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [68/100]  loss: 0.8564745517319472, time: 201.72744035720825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69/100]  loss: 0.8557784342891955, time: 201.26887893676758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [70/100]  loss: 0.855580722055738, time: 200.61423015594482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71/100]  loss: 0.855286129567989, time: 200.240172624588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [72/100]  loss: 0.8549626994700659, time: 200.0749969482422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [73/100]  loss: 0.8542175026482375, time: 200.54785680770874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [74/100]  loss: 0.8539603317856158, time: 200.37876963615417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [75/100]  loss: 0.853442347711987, time: 200.63526844978333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [76/100]  loss: 0.8531838505671768, time: 200.52086758613586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [77/100]  loss: 0.8529345571364044, time: 201.26562404632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [78/100]  loss: 0.8523046287594649, time: 200.5060522556305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [79/100]  loss: 0.8520244104521615, time: 200.55772423744202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [80/100]  loss: 0.8516901127560429, time: 199.3030264377594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [81/100]  loss: 0.8512557910548316, time: 199.12538361549377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [82/100]  loss: 0.8513614018127401, time: 198.67746591567993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83/100]  loss: 0.8508493838802217, time: 198.24900770187378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [84/100]  loss: 0.8507139660693981, time: 197.4820384979248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [85/100]  loss: 0.850045617453005, time: 197.4932792186737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [86/100]  loss: 0.8501250923626007, time: 197.11886715888977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [87/100]  loss: 0.8494645013380303, time: 197.05017757415771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [88/100]  loss: 0.8490141381031622, time: 197.53760170936584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [89/100]  loss: 0.8486938610594109, time: 197.12932896614075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [90/100]  loss: 0.8488604926243031, time: 197.44296765327454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [91/100]  loss: 0.8479638485996811, time: 197.1139805316925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [92/100]  loss: 0.8479974427551189, time: 197.5750970840454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [93/100]  loss: 0.8473331035760344, time: 198.1093454360962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [94/100]  loss: 0.8471233076834804, time: 198.45048785209656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [95/100]  loss: 0.8472175902475125, time: 198.49177360534668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [96/100]  loss: 0.8468498557648331, time: 199.0325276851654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [97/100]  loss: 0.8463553101928146, time: 199.3070101737976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [98/100]  loss: 0.846441183140669, time: 199.840589761734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [99/100]  loss: 0.8461569287474193, time: 199.6111056804657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]  loss: 0.845874012778045, time: 199.61502313613892\n",
      "Training time 19961.72344970703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train script\n",
    "start_time = time.time()\n",
    "\n",
    "model = model.to(device)\n",
    "params = model.parameters()\n",
    "optimizer = LARS(params, lr=base_lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, run_time = train_loop(model, epoch, optimizer, train_loader, device)\n",
    "    print(f'Epoch: [{epoch+1}/{epochs}]  loss: {np.mean(train_loss)}, time: {run_time}')\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(f'Training time {train_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c728081f-6c0f-4f81-b64a-15f71af7fb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): ConvNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): ConvNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): ConvNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): ConvNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): ConvNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "model_backbone_weights = model.backend\n",
    "print(model_backbone_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1eaa9f-ca7b-4f50-be0b-de4e439b1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./output/VICReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1522def-0fdf-4fe2-b05c-628d8106a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './output/VICReg'\n",
    "torch.save({ 'model_state_dict': model_backbone_weights.state_dict() }, f'{save_model_path}/efficientnet_b0_backbone_weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6e76-b144-422c-a3ce-017964fb2fd9",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce8b6658-f7cc-4128-9012-cb6f5c1d618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = './datasets/COVIDGR_1.0'\n",
    "positive_path = os.path.join(dataset_path, 'P')\n",
    "negative_path = os.path.join(dataset_path, 'N')\n",
    "\n",
    "# Data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomApply(\n",
    "            [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "            p=0.8\n",
    "        ),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96e7d5c7-fca2-48a0-8795-0cb1cab00a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 852\n",
       "    Root location: ./datasets/COVIDGR_1.0\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(224, 224), scale=(0.3, 0.9), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
       "               RandomApply(\n",
       "               p=0.8\n",
       "               ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.8, 1.2], hue=[-0.1, 0.1])\n",
       "           )\n",
       "               RandomGrayscale(p=0.2)\n",
       "               RandomApply(\n",
       "               p=0.5\n",
       "               GaussianBlur(kernel_size=(25, 25), sigma=(0.1, 2.0))\n",
       "           )\n",
       "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "           )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Map labels\n",
    "dataset.class_to_idx = {'N': 0, 'P': 1}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8984a78-804b-492d-8f96-7a4292ea8b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766 256\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "batch_size = 256\n",
    "print(train_size, batch_size)\n",
    "\n",
    "# Split off the test set\n",
    "train_val_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for the test set (held out)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f89a992-6939-4c08-8633-9e705c6ba5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./output/VICReg/efficientnet_b0_backbone_weights.ckpt\"\n",
    "best_params = {\"learning_rate\": 0.01, \"weight_decay\": 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "808c7a90-63d0-4127-83a0-a244db80487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_model():\n",
    "    # Load the EfficientNet model\n",
    "    model = efficientnet_b0()\n",
    "    \n",
    "    # Modify the final classification head for your dataset\n",
    "    embed_dim = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Identity()\n",
    "    \n",
    "    # Load the pre-trained weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    state_dict = checkpoint[\"model_state_dict\"]  # Adjust the key if needed\n",
    "    \n",
    "    # Load the weights into the model\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Pretrained weights found at {} and loaded with msg: {}'.format(checkpoint_path, msg))\n",
    "\n",
    "    # Freeze model params\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7933c82e-52a0-4bbd-bbc9-c709efe6fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def train_model(model, classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "\n",
    "        # Validation phase\n",
    "        classifier.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                outputs = classifier(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "\n",
    "        print(f\"[{device}] Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss / len(val_loader):.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return best_val_f1\n",
    "\n",
    "# Evaluate model on the test set\n",
    "def evaluate_model(model, classifier, test_loader):\n",
    "    classifier.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def grid_search(train_loader, val_loader, learning_rates, weight_decays, num_epochs):\n",
    "    best_model = None\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decays:\n",
    "            model = get_model()\n",
    "            optimizer = optim.SGD(model.classifier[1].parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
    "            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            print(f\"\\nTraining with lr={lr}, weight_decay={wd}\")\n",
    "            f1_score = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "            if f1_score > best_f1:\n",
    "                best_f1 = f1_score\n",
    "                best_model = model\n",
    "                best_params = {\"learning_rate\": lr, \"weight_decay\": wd}\n",
    "    print(f\"\\nBest Model F1: {best_f1} with params {best_params}\")\n",
    "    return best_model, best_params\n",
    "\n",
    "# Step 9: 5-Fold Cross-Validation\n",
    "def cross_validation(best_params, dataset, test_loader, num_epochs=50, folds=5):\n",
    "    fold_metrics = []\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=100)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\nStarting fold {fold + 1}/{folds}\")\n",
    "\n",
    "        # Split dataset indices for training and validation\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Create DataLoaders for this fold\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model, embed_dim = get_model()\n",
    "        linear_classifier = nn.Linear(embed_dim, 2) # 2 is the number of features\n",
    "        optimizer = optim.SGD(linear_classifier.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'], momentum=0.9)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = train_model(model, linear_classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        accuracy, precision, recall, f1 = evaluate_model(model, linear_classifier, test_loader)\n",
    "        fold_metrics.append((accuracy, precision, recall, f1, end_time - start_time))\n",
    "\n",
    "    return np.array(fold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "776f7512-68b0-4aea-99bd-126eccf7d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6803, Val Loss: 0.7020, F1 Score: 0.4427\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6768, Val Loss: 0.6813, F1 Score: 0.4806\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6522, Val Loss: 0.6593, F1 Score: 0.6197\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6258, Val Loss: 0.6395, F1 Score: 0.6711\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6094, Val Loss: 0.6044, F1 Score: 0.7006\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5779, Val Loss: 0.5893, F1 Score: 0.7425\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5604, Val Loss: 0.6093, F1 Score: 0.6456\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5432, Val Loss: 0.5691, F1 Score: 0.7284\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5616, Val Loss: 0.6195, F1 Score: 0.7329\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5659, Val Loss: 0.6165, F1 Score: 0.7237\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5312, Val Loss: 0.6187, F1 Score: 0.6708\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5193, Val Loss: 0.5318, F1 Score: 0.7531\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5238, Val Loss: 0.5817, F1 Score: 0.7081\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5246, Val Loss: 0.5443, F1 Score: 0.7417\n",
      "[cuda] Epoch 15/50, Train Loss: 0.5090, Val Loss: 0.6293, F1 Score: 0.6577\n",
      "[cuda] Epoch 16/50, Train Loss: 0.5306, Val Loss: 0.5647, F1 Score: 0.7239\n",
      "[cuda] Epoch 17/50, Train Loss: 0.5404, Val Loss: 0.5259, F1 Score: 0.7719\n",
      "[cuda] Epoch 18/50, Train Loss: 0.5387, Val Loss: 0.5401, F1 Score: 0.7296\n",
      "[cuda] Epoch 19/50, Train Loss: 0.5277, Val Loss: 0.5735, F1 Score: 0.7355\n",
      "[cuda] Epoch 20/50, Train Loss: 0.5094, Val Loss: 0.5964, F1 Score: 0.6986\n",
      "[cuda] Epoch 21/50, Train Loss: 0.5006, Val Loss: 0.5507, F1 Score: 0.7532\n",
      "[cuda] Epoch 22/50, Train Loss: 0.5061, Val Loss: 0.5818, F1 Score: 0.6914\n",
      "[cuda] Epoch 23/50, Train Loss: 0.5144, Val Loss: 0.6095, F1 Score: 0.6625\n",
      "[cuda] Epoch 24/50, Train Loss: 0.5066, Val Loss: 0.5474, F1 Score: 0.7333\n",
      "[cuda] Epoch 25/50, Train Loss: 0.5106, Val Loss: 0.5976, F1 Score: 0.6755\n",
      "[cuda] Epoch 26/50, Train Loss: 0.5094, Val Loss: 0.5973, F1 Score: 0.6849\n",
      "[cuda] Epoch 27/50, Train Loss: 0.5042, Val Loss: 0.5574, F1 Score: 0.7215\n",
      "[cuda] Epoch 28/50, Train Loss: 0.5116, Val Loss: 0.5800, F1 Score: 0.6951\n",
      "[cuda] Epoch 29/50, Train Loss: 0.5257, Val Loss: 0.5675, F1 Score: 0.6923\n",
      "[cuda] Epoch 30/50, Train Loss: 0.4971, Val Loss: 0.5971, F1 Score: 0.7205\n",
      "[cuda] Epoch 31/50, Train Loss: 0.5010, Val Loss: 0.6078, F1 Score: 0.6974\n",
      "[cuda] Epoch 32/50, Train Loss: 0.5199, Val Loss: 0.5537, F1 Score: 0.7375\n",
      "[cuda] Epoch 33/50, Train Loss: 0.4936, Val Loss: 0.5663, F1 Score: 0.7296\n",
      "[cuda] Epoch 34/50, Train Loss: 0.5009, Val Loss: 0.5637, F1 Score: 0.7215\n",
      "[cuda] Epoch 35/50, Train Loss: 0.5031, Val Loss: 0.5622, F1 Score: 0.6800\n",
      "[cuda] Epoch 36/50, Train Loss: 0.4761, Val Loss: 0.5569, F1 Score: 0.7134\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4852, Val Loss: 0.5998, F1 Score: 0.6957\n",
      "[cuda] Epoch 38/50, Train Loss: 0.4940, Val Loss: 0.5637, F1 Score: 0.6962\n",
      "[cuda] Epoch 39/50, Train Loss: 0.5328, Val Loss: 0.5384, F1 Score: 0.7643\n",
      "[cuda] Epoch 40/50, Train Loss: 0.4755, Val Loss: 0.5544, F1 Score: 0.7595\n",
      "[cuda] Epoch 41/50, Train Loss: 0.4930, Val Loss: 0.5282, F1 Score: 0.7848\n",
      "[cuda] Epoch 42/50, Train Loss: 0.4834, Val Loss: 0.5733, F1 Score: 0.7226\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4602, Val Loss: 0.5324, F1 Score: 0.7547\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4917, Val Loss: 0.5662, F1 Score: 0.7044\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4831, Val Loss: 0.5804, F1 Score: 0.7500\n",
      "[cuda] Epoch 46/50, Train Loss: 0.5161, Val Loss: 0.5709, F1 Score: 0.7607\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4884, Val Loss: 0.6142, F1 Score: 0.6939\n",
      "[cuda] Epoch 48/50, Train Loss: 0.5071, Val Loss: 0.5305, F1 Score: 0.7517\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4855, Val Loss: 0.5761, F1 Score: 0.7342\n",
      "[cuda] Epoch 50/50, Train Loss: 0.4903, Val Loss: 0.5655, F1 Score: 0.7073\n",
      "\n",
      "Starting fold 2/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6850, Val Loss: 0.7070, F1 Score: 0.4286\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6771, Val Loss: 0.6781, F1 Score: 0.4615\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6582, Val Loss: 0.6478, F1 Score: 0.6525\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6213, Val Loss: 0.6087, F1 Score: 0.6533\n",
      "[cuda] Epoch 5/50, Train Loss: 0.5908, Val Loss: 0.5828, F1 Score: 0.6711\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5692, Val Loss: 0.5605, F1 Score: 0.6962\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5995, Val Loss: 0.5617, F1 Score: 0.7500\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5529, Val Loss: 0.5743, F1 Score: 0.7273\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5604, Val Loss: 0.5661, F1 Score: 0.7442\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5330, Val Loss: 0.5698, F1 Score: 0.6914\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5550, Val Loss: 0.5733, F1 Score: 0.7125\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5335, Val Loss: 0.5970, F1 Score: 0.6871\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5348, Val Loss: 0.5979, F1 Score: 0.6667\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5376, Val Loss: 0.6077, F1 Score: 0.7000\n",
      "[cuda] Epoch 15/50, Train Loss: 0.5246, Val Loss: 0.6179, F1 Score: 0.7006\n",
      "[cuda] Epoch 16/50, Train Loss: 0.5528, Val Loss: 0.6329, F1 Score: 0.6928\n",
      "[cuda] Epoch 17/50, Train Loss: 0.5284, Val Loss: 0.6008, F1 Score: 0.7066\n",
      "[cuda] Epoch 18/50, Train Loss: 0.5456, Val Loss: 0.6056, F1 Score: 0.6584\n",
      "[cuda] Epoch 19/50, Train Loss: 0.5137, Val Loss: 0.6675, F1 Score: 0.6234\n",
      "[cuda] Epoch 20/50, Train Loss: 0.4680, Val Loss: 0.5904, F1 Score: 0.6389\n",
      "[cuda] Epoch 21/50, Train Loss: 0.4776, Val Loss: 0.5841, F1 Score: 0.7143\n",
      "[cuda] Epoch 22/50, Train Loss: 0.5187, Val Loss: 0.5779, F1 Score: 0.7134\n",
      "[cuda] Epoch 23/50, Train Loss: 0.5042, Val Loss: 0.5744, F1 Score: 0.7531\n",
      "[cuda] Epoch 24/50, Train Loss: 0.5044, Val Loss: 0.5617, F1 Score: 0.7561\n",
      "[cuda] Epoch 25/50, Train Loss: 0.5074, Val Loss: 0.5785, F1 Score: 0.7143\n",
      "[cuda] Epoch 26/50, Train Loss: 0.5069, Val Loss: 0.5971, F1 Score: 0.6842\n",
      "[cuda] Epoch 27/50, Train Loss: 0.5165, Val Loss: 0.5833, F1 Score: 0.7250\n",
      "[cuda] Epoch 28/50, Train Loss: 0.4943, Val Loss: 0.5759, F1 Score: 0.7317\n",
      "[cuda] Epoch 29/50, Train Loss: 0.4902, Val Loss: 0.5690, F1 Score: 0.7273\n",
      "[cuda] Epoch 30/50, Train Loss: 0.4824, Val Loss: 0.6219, F1 Score: 0.6887\n",
      "[cuda] Epoch 31/50, Train Loss: 0.5173, Val Loss: 0.5698, F1 Score: 0.7547\n",
      "[cuda] Epoch 32/50, Train Loss: 0.4945, Val Loss: 0.5784, F1 Score: 0.7516\n",
      "[cuda] Epoch 33/50, Train Loss: 0.4967, Val Loss: 0.5792, F1 Score: 0.7134\n",
      "[cuda] Epoch 34/50, Train Loss: 0.4845, Val Loss: 0.6648, F1 Score: 0.6968\n",
      "[cuda] Epoch 35/50, Train Loss: 0.5092, Val Loss: 0.5853, F1 Score: 0.7237\n",
      "[cuda] Epoch 36/50, Train Loss: 0.4990, Val Loss: 0.5737, F1 Score: 0.7134\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4938, Val Loss: 0.5758, F1 Score: 0.7342\n",
      "[cuda] Epoch 38/50, Train Loss: 0.4794, Val Loss: 0.5885, F1 Score: 0.7051\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4951, Val Loss: 0.5812, F1 Score: 0.7013\n",
      "[cuda] Epoch 40/50, Train Loss: 0.5137, Val Loss: 0.5456, F1 Score: 0.7226\n",
      "[cuda] Epoch 41/50, Train Loss: 0.5123, Val Loss: 0.5546, F1 Score: 0.7500\n",
      "[cuda] Epoch 42/50, Train Loss: 0.5174, Val Loss: 0.6021, F1 Score: 0.7044\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4884, Val Loss: 0.5796, F1 Score: 0.7134\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4842, Val Loss: 0.6248, F1 Score: 0.7261\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4753, Val Loss: 0.5486, F1 Score: 0.7702\n",
      "[cuda] Epoch 46/50, Train Loss: 0.4792, Val Loss: 0.6132, F1 Score: 0.6918\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4784, Val Loss: 0.6252, F1 Score: 0.6994\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4822, Val Loss: 0.6214, F1 Score: 0.6923\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4870, Val Loss: 0.5969, F1 Score: 0.7179\n",
      "[cuda] Epoch 50/50, Train Loss: 0.5054, Val Loss: 0.5321, F1 Score: 0.7273\n",
      "\n",
      "Starting fold 3/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7355, Val Loss: 0.7228, F1 Score: 0.5116\n",
      "[cuda] Epoch 2/50, Train Loss: 0.7022, Val Loss: 0.6646, F1 Score: 0.5811\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6816, Val Loss: 0.6266, F1 Score: 0.5620\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6505, Val Loss: 0.5894, F1 Score: 0.6412\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6260, Val Loss: 0.5746, F1 Score: 0.7234\n",
      "[cuda] Epoch 6/50, Train Loss: 0.6000, Val Loss: 0.5830, F1 Score: 0.6974\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5946, Val Loss: 0.5558, F1 Score: 0.7564\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5844, Val Loss: 0.5366, F1 Score: 0.7453\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5564, Val Loss: 0.5139, F1 Score: 0.7763\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5653, Val Loss: 0.4913, F1 Score: 0.7338\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5935, Val Loss: 0.5414, F1 Score: 0.7183\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5367, Val Loss: 0.5655, F1 Score: 0.7190\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5508, Val Loss: 0.5328, F1 Score: 0.7248\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5447, Val Loss: 0.5520, F1 Score: 0.6944\n",
      "[cuda] Epoch 15/50, Train Loss: 0.5703, Val Loss: 0.4768, F1 Score: 0.7626\n",
      "[cuda] Epoch 16/50, Train Loss: 0.5494, Val Loss: 0.4948, F1 Score: 0.7671\n",
      "[cuda] Epoch 17/50, Train Loss: 0.5566, Val Loss: 0.5483, F1 Score: 0.7152\n",
      "[cuda] Epoch 18/50, Train Loss: 0.5522, Val Loss: 0.5102, F1 Score: 0.7606\n",
      "[cuda] Epoch 19/50, Train Loss: 0.5149, Val Loss: 0.5058, F1 Score: 0.7500\n",
      "[cuda] Epoch 20/50, Train Loss: 0.5244, Val Loss: 0.5099, F1 Score: 0.7101\n",
      "[cuda] Epoch 21/50, Train Loss: 0.5138, Val Loss: 0.5516, F1 Score: 0.7361\n",
      "[cuda] Epoch 22/50, Train Loss: 0.5139, Val Loss: 0.5137, F1 Score: 0.7451\n",
      "[cuda] Epoch 23/50, Train Loss: 0.5171, Val Loss: 0.4981, F1 Score: 0.7432\n",
      "[cuda] Epoch 24/50, Train Loss: 0.5245, Val Loss: 0.5030, F1 Score: 0.7591\n",
      "[cuda] Epoch 25/50, Train Loss: 0.5415, Val Loss: 0.5482, F1 Score: 0.7299\n",
      "[cuda] Epoch 26/50, Train Loss: 0.5247, Val Loss: 0.5309, F1 Score: 0.7194\n",
      "[cuda] Epoch 27/50, Train Loss: 0.5428, Val Loss: 0.5034, F1 Score: 0.7397\n",
      "[cuda] Epoch 28/50, Train Loss: 0.5250, Val Loss: 0.5038, F1 Score: 0.7483\n",
      "[cuda] Epoch 29/50, Train Loss: 0.5163, Val Loss: 0.5292, F1 Score: 0.7162\n",
      "[cuda] Epoch 30/50, Train Loss: 0.5017, Val Loss: 0.4941, F1 Score: 0.7417\n",
      "[cuda] Epoch 31/50, Train Loss: 0.5028, Val Loss: 0.5122, F1 Score: 0.7483\n",
      "[cuda] Epoch 32/50, Train Loss: 0.5309, Val Loss: 0.4803, F1 Score: 0.7857\n",
      "[cuda] Epoch 33/50, Train Loss: 0.5229, Val Loss: 0.5215, F1 Score: 0.7273\n",
      "[cuda] Epoch 34/50, Train Loss: 0.5153, Val Loss: 0.5221, F1 Score: 0.7733\n",
      "[cuda] Epoch 35/50, Train Loss: 0.5211, Val Loss: 0.5586, F1 Score: 0.7027\n",
      "[cuda] Epoch 36/50, Train Loss: 0.5121, Val Loss: 0.5122, F1 Score: 0.7338\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4955, Val Loss: 0.4909, F1 Score: 0.7647\n",
      "[cuda] Epoch 38/50, Train Loss: 0.5387, Val Loss: 0.4977, F1 Score: 0.7286\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4922, Val Loss: 0.5357, F1 Score: 0.7260\n",
      "[cuda] Epoch 40/50, Train Loss: 0.5004, Val Loss: 0.5113, F1 Score: 0.7606\n",
      "[cuda] Epoch 41/50, Train Loss: 0.5212, Val Loss: 0.4845, F1 Score: 0.7857\n",
      "[cuda] Epoch 42/50, Train Loss: 0.5152, Val Loss: 0.5280, F1 Score: 0.7042\n",
      "[cuda] Epoch 43/50, Train Loss: 0.5071, Val Loss: 0.5485, F1 Score: 0.6849\n",
      "[cuda] Epoch 44/50, Train Loss: 0.5319, Val Loss: 0.4990, F1 Score: 0.7200\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4981, Val Loss: 0.5012, F1 Score: 0.7808\n",
      "[cuda] Epoch 46/50, Train Loss: 0.5054, Val Loss: 0.4624, F1 Score: 0.7586\n",
      "[cuda] Epoch 47/50, Train Loss: 0.5504, Val Loss: 0.5093, F1 Score: 0.7347\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4911, Val Loss: 0.5057, F1 Score: 0.7083\n",
      "[cuda] Epoch 49/50, Train Loss: 0.5169, Val Loss: 0.5012, F1 Score: 0.7465\n",
      "[cuda] Epoch 50/50, Train Loss: 0.5073, Val Loss: 0.4975, F1 Score: 0.7536\n",
      "\n",
      "Starting fold 4/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6916, Val Loss: 0.6843, F1 Score: 0.4923\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6888, Val Loss: 0.6733, F1 Score: 0.4806\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6692, Val Loss: 0.6474, F1 Score: 0.5714\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6223, Val Loss: 0.6305, F1 Score: 0.6429\n",
      "[cuda] Epoch 5/50, Train Loss: 0.5827, Val Loss: 0.5908, F1 Score: 0.6621\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5712, Val Loss: 0.6059, F1 Score: 0.6887\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5901, Val Loss: 0.6265, F1 Score: 0.6835\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5659, Val Loss: 0.6417, F1 Score: 0.6536\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5321, Val Loss: 0.5903, F1 Score: 0.6944\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5355, Val Loss: 0.6186, F1 Score: 0.6974\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5286, Val Loss: 0.5648, F1 Score: 0.7114\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5306, Val Loss: 0.5790, F1 Score: 0.6528\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5649, Val Loss: 0.5581, F1 Score: 0.7027\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5551, Val Loss: 0.5433, F1 Score: 0.7482\n",
      "[cuda] Epoch 15/50, Train Loss: 0.5318, Val Loss: 0.5880, F1 Score: 0.6797\n",
      "[cuda] Epoch 16/50, Train Loss: 0.5034, Val Loss: 0.5681, F1 Score: 0.6853\n",
      "[cuda] Epoch 17/50, Train Loss: 0.5406, Val Loss: 0.5394, F1 Score: 0.7600\n",
      "[cuda] Epoch 18/50, Train Loss: 0.5231, Val Loss: 0.5701, F1 Score: 0.7285\n",
      "[cuda] Epoch 19/50, Train Loss: 0.5208, Val Loss: 0.6009, F1 Score: 0.6711\n",
      "[cuda] Epoch 20/50, Train Loss: 0.5394, Val Loss: 0.5446, F1 Score: 0.7619\n",
      "[cuda] Epoch 21/50, Train Loss: 0.5077, Val Loss: 0.5608, F1 Score: 0.7123\n",
      "[cuda] Epoch 22/50, Train Loss: 0.5163, Val Loss: 0.5620, F1 Score: 0.7260\n",
      "[cuda] Epoch 23/50, Train Loss: 0.4991, Val Loss: 0.5424, F1 Score: 0.6906\n",
      "[cuda] Epoch 24/50, Train Loss: 0.5008, Val Loss: 0.5656, F1 Score: 0.6715\n",
      "[cuda] Epoch 25/50, Train Loss: 0.5125, Val Loss: 0.5462, F1 Score: 0.7034\n",
      "[cuda] Epoch 26/50, Train Loss: 0.5202, Val Loss: 0.5801, F1 Score: 0.6918\n",
      "[cuda] Epoch 27/50, Train Loss: 0.5140, Val Loss: 0.5545, F1 Score: 0.6974\n",
      "[cuda] Epoch 28/50, Train Loss: 0.5356, Val Loss: 0.5143, F1 Score: 0.7361\n",
      "[cuda] Epoch 29/50, Train Loss: 0.5095, Val Loss: 0.5444, F1 Score: 0.7324\n",
      "[cuda] Epoch 30/50, Train Loss: 0.5039, Val Loss: 0.5813, F1 Score: 0.7042\n",
      "[cuda] Epoch 31/50, Train Loss: 0.5023, Val Loss: 0.6112, F1 Score: 0.6923\n",
      "[cuda] Epoch 32/50, Train Loss: 0.5231, Val Loss: 0.5539, F1 Score: 0.7516\n",
      "[cuda] Epoch 33/50, Train Loss: 0.5000, Val Loss: 0.5245, F1 Score: 0.7778\n",
      "[cuda] Epoch 34/50, Train Loss: 0.4933, Val Loss: 0.5696, F1 Score: 0.7413\n",
      "[cuda] Epoch 35/50, Train Loss: 0.4871, Val Loss: 0.5580, F1 Score: 0.7778\n",
      "[cuda] Epoch 36/50, Train Loss: 0.5009, Val Loss: 0.6108, F1 Score: 0.6438\n",
      "[cuda] Epoch 37/50, Train Loss: 0.5032, Val Loss: 0.5333, F1 Score: 0.7285\n",
      "[cuda] Epoch 38/50, Train Loss: 0.5005, Val Loss: 0.5682, F1 Score: 0.6944\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4947, Val Loss: 0.5754, F1 Score: 0.6923\n",
      "[cuda] Epoch 40/50, Train Loss: 0.5025, Val Loss: 0.5526, F1 Score: 0.7152\n",
      "[cuda] Epoch 41/50, Train Loss: 0.4815, Val Loss: 0.6117, F1 Score: 0.6842\n",
      "[cuda] Epoch 42/50, Train Loss: 0.5236, Val Loss: 0.5339, F1 Score: 0.7170\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4912, Val Loss: 0.5663, F1 Score: 0.7075\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4783, Val Loss: 0.5514, F1 Score: 0.6980\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4863, Val Loss: 0.5301, F1 Score: 0.7101\n",
      "[cuda] Epoch 46/50, Train Loss: 0.5069, Val Loss: 0.5398, F1 Score: 0.7517\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4768, Val Loss: 0.5160, F1 Score: 0.7532\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4938, Val Loss: 0.5440, F1 Score: 0.7226\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4849, Val Loss: 0.5536, F1 Score: 0.7190\n",
      "[cuda] Epoch 50/50, Train Loss: 0.5037, Val Loss: 0.5458, F1 Score: 0.6423\n",
      "\n",
      "Starting fold 5/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7011, Val Loss: 0.6867, F1 Score: 0.5584\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6909, Val Loss: 0.6924, F1 Score: 0.4211\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6536, Val Loss: 0.6447, F1 Score: 0.5625\n",
      "[cuda] Epoch 4/50, Train Loss: 0.6333, Val Loss: 0.6279, F1 Score: 0.6308\n",
      "[cuda] Epoch 5/50, Train Loss: 0.6078, Val Loss: 0.6241, F1 Score: 0.6986\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5835, Val Loss: 0.5858, F1 Score: 0.7226\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5765, Val Loss: 0.5849, F1 Score: 0.7456\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5854, Val Loss: 0.5533, F1 Score: 0.7515\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5976, Val Loss: 0.5179, F1 Score: 0.8000\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5447, Val Loss: 0.5542, F1 Score: 0.7297\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5521, Val Loss: 0.5740, F1 Score: 0.7034\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5652, Val Loss: 0.5559, F1 Score: 0.7383\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5285, Val Loss: 0.5487, F1 Score: 0.7284\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5221, Val Loss: 0.5287, F1 Score: 0.7595\n",
      "[cuda] Epoch 15/50, Train Loss: 0.5147, Val Loss: 0.5598, F1 Score: 0.7333\n",
      "[cuda] Epoch 16/50, Train Loss: 0.5338, Val Loss: 0.5624, F1 Score: 0.6846\n",
      "[cuda] Epoch 17/50, Train Loss: 0.5362, Val Loss: 0.5474, F1 Score: 0.7436\n",
      "[cuda] Epoch 18/50, Train Loss: 0.4937, Val Loss: 0.5272, F1 Score: 0.7375\n",
      "[cuda] Epoch 19/50, Train Loss: 0.5376, Val Loss: 0.5216, F1 Score: 0.7613\n",
      "[cuda] Epoch 20/50, Train Loss: 0.5048, Val Loss: 0.5406, F1 Score: 0.7651\n",
      "[cuda] Epoch 21/50, Train Loss: 0.5099, Val Loss: 0.5607, F1 Score: 0.7179\n",
      "[cuda] Epoch 22/50, Train Loss: 0.5104, Val Loss: 0.5530, F1 Score: 0.7308\n",
      "[cuda] Epoch 23/50, Train Loss: 0.5018, Val Loss: 0.5882, F1 Score: 0.6800\n",
      "[cuda] Epoch 24/50, Train Loss: 0.5231, Val Loss: 0.5676, F1 Score: 0.7484\n",
      "[cuda] Epoch 25/50, Train Loss: 0.5022, Val Loss: 0.5674, F1 Score: 0.6887\n",
      "[cuda] Epoch 26/50, Train Loss: 0.5091, Val Loss: 0.5795, F1 Score: 0.7123\n",
      "[cuda] Epoch 27/50, Train Loss: 0.5079, Val Loss: 0.5858, F1 Score: 0.6711\n",
      "[cuda] Epoch 28/50, Train Loss: 0.5246, Val Loss: 0.5344, F1 Score: 0.7417\n",
      "[cuda] Epoch 29/50, Train Loss: 0.4932, Val Loss: 0.5298, F1 Score: 0.7368\n",
      "[cuda] Epoch 30/50, Train Loss: 0.5138, Val Loss: 0.5170, F1 Score: 0.7517\n",
      "[cuda] Epoch 31/50, Train Loss: 0.5152, Val Loss: 0.5268, F1 Score: 0.6875\n",
      "[cuda] Epoch 32/50, Train Loss: 0.5000, Val Loss: 0.5480, F1 Score: 0.7483\n",
      "[cuda] Epoch 33/50, Train Loss: 0.5221, Val Loss: 0.5549, F1 Score: 0.7162\n",
      "[cuda] Epoch 34/50, Train Loss: 0.5325, Val Loss: 0.5901, F1 Score: 0.6759\n",
      "[cuda] Epoch 35/50, Train Loss: 0.5122, Val Loss: 0.5388, F1 Score: 0.7375\n",
      "[cuda] Epoch 36/50, Train Loss: 0.5043, Val Loss: 0.5590, F1 Score: 0.6957\n",
      "[cuda] Epoch 37/50, Train Loss: 0.5037, Val Loss: 0.5450, F1 Score: 0.7595\n",
      "[cuda] Epoch 38/50, Train Loss: 0.5005, Val Loss: 0.5395, F1 Score: 0.7534\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4961, Val Loss: 0.5719, F1 Score: 0.6957\n",
      "[cuda] Epoch 40/50, Train Loss: 0.5146, Val Loss: 0.5421, F1 Score: 0.7020\n",
      "[cuda] Epoch 41/50, Train Loss: 0.5149, Val Loss: 0.5410, F1 Score: 0.7871\n",
      "[cuda] Epoch 42/50, Train Loss: 0.4932, Val Loss: 0.5401, F1 Score: 0.7261\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4924, Val Loss: 0.5239, F1 Score: 0.7568\n",
      "[cuda] Epoch 44/50, Train Loss: 0.5148, Val Loss: 0.5284, F1 Score: 0.7483\n",
      "[cuda] Epoch 45/50, Train Loss: 0.5127, Val Loss: 0.5673, F1 Score: 0.7320\n",
      "[cuda] Epoch 46/50, Train Loss: 0.5031, Val Loss: 0.5309, F1 Score: 0.7821\n",
      "[cuda] Epoch 47/50, Train Loss: 0.5000, Val Loss: 0.5318, F1 Score: 0.7342\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4988, Val Loss: 0.4907, F1 Score: 0.7407\n",
      "[cuda] Epoch 49/50, Train Loss: 0.5164, Val Loss: 0.5200, F1 Score: 0.7226\n",
      "[cuda] Epoch 50/50, Train Loss: 0.4999, Val Loss: 0.5597, F1 Score: 0.7465\n",
      "\n",
      "Average metrics over 5 folds in test set:\n",
      "Accuracy: 0.7023 ± 0.0271\n",
      "Precision: 0.6520 ± 0.0257\n",
      "Recall: 0.7000 ± 0.0542\n",
      "F1 Score: 0.6744 ± 0.0346\n",
      "Training Time per Fold: 1861.10 ± 17.52 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold cross-validation\n",
    "metrics = cross_validation(best_params, train_val_dataset, test_loader, num_epochs=50, folds=5)\n",
    "\n",
    "# Calculate average and standard deviation of metrics across folds\n",
    "avg_metrics = metrics.mean(axis=0)\n",
    "std_metrics = metrics.std(axis=0)\n",
    "\n",
    "print(f\"\\nAverage metrics over 5 folds in test set:\\n\"\n",
    "      f\"Accuracy: {avg_metrics[0]:.4f} ± {std_metrics[0]:.4f}\\n\"\n",
    "      f\"Precision: {avg_metrics[1]:.4f} ± {std_metrics[1]:.4f}\\n\"\n",
    "      f\"Recall: {avg_metrics[2]:.4f} ± {std_metrics[2]:.4f}\\n\"\n",
    "      f\"F1 Score: {avg_metrics[3]:.4f} ± {std_metrics[3]:.4f}\\n\"\n",
    "      f\"Training Time per Fold: {avg_metrics[4]:.2f} ± {std_metrics[4]:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1033d199-b12f-4a7b-9c1f-ccfab7161163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.55813953e-01, 6.97674419e-01, 7.89473684e-01, 7.40740741e-01,\n",
       "        1.88389540e+03],\n",
       "       [6.97674419e-01, 6.57894737e-01, 6.57894737e-01, 6.57894737e-01,\n",
       "        1.86215698e+03],\n",
       "       [6.86046512e-01, 6.27906977e-01, 7.10526316e-01, 6.66666667e-01,\n",
       "        1.85964647e+03],\n",
       "       [6.86046512e-01, 6.27906977e-01, 7.10526316e-01, 6.66666667e-01,\n",
       "        1.83039524e+03],\n",
       "       [6.86046512e-01, 6.48648649e-01, 6.31578947e-01, 6.40000000e-01,\n",
       "        1.86942247e+03]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "558b478a-966b-4322-8d58-e90c25c4baaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>training time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.755814</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>1883.895396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>1862.156977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1859.646466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1830.395243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>1869.422467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall        f1  training time\n",
       "0  0.755814   0.697674  0.789474  0.740741    1883.895396\n",
       "1  0.697674   0.657895  0.657895  0.657895    1862.156977\n",
       "2  0.686047   0.627907  0.710526  0.666667    1859.646466\n",
       "3  0.686047   0.627907  0.710526  0.666667    1830.395243\n",
       "4  0.686047   0.648649  0.631579  0.640000    1869.422467"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(metrics, columns=['accuracy', 'precision', 'recall', 'f1', 'training time'])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e26de73f-0593-405e-851f-84a732843424",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/vicreg_fine_tuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a898a-b511-44f4-981b-a3a7b81b8388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
