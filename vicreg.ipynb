{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad9ad46-a972-4963-8fc0-9bd3a31d2c0a",
   "metadata": {},
   "source": [
    "# VICREG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532303f0-b10b-4077-803f-e2e544ecd301",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e49ae9-86d3-49b8-bbf2-16784868e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 28 10:00:21 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN Xp     Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 23%   28C    P8     8W / 250W |   1627MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1865586      C   ...c-project/.env/bin/python     1623MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74bd7a04-0630-4197-acc5-37bf4a5f2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import transforms\n",
    "import torch.distributed as dist\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def get_byol_transforms(size, mean, std):\n",
    "    transformT = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(size=size, scale=(0.08,1), ratio=(3 / 4, 4 / 3)),\n",
    "        transforms.RandomRotation((-90, 90)),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.GaussianBlur(kernel_size=(23,23), sigma=(0.1, 2.0)),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "    transformT1 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(size=size, scale=(0.08,1), ratio=(3 / 4, 4 / 3)),\n",
    "        transforms.RandomRotation((-90, 90)),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=(23,23), sigma=(0.1, 2.0)),\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "    transformEvalT = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(size=size),\n",
    "        transforms.Normalize(mean, std),        \n",
    "    ])\n",
    "\n",
    "    return transformT, transformT1, transformEvalT\n",
    "\n",
    "def get_cxr_transforms(size=224, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "        transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    transform_eval = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(size=size),\n",
    "        transforms.Normalize(mean, std),        \n",
    "    ])\n",
    "\n",
    "    return transform, transform, transform_eval\n",
    "\n",
    "\n",
    "class MultiViewDataInjector(object):\n",
    "    def __init__(self, *args):\n",
    "        self.transforms = args[0]\n",
    "        self.random_flip = transforms.RandomHorizontalFlip()\n",
    "\n",
    "    def __call__(self, sample, *with_consistent_flipping):\n",
    "        if with_consistent_flipping:\n",
    "            sample = self.random_flip(sample)\n",
    "        output = [transform(sample) for transform in self.transforms]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1580723f-9b2d-44d4-9e62-a7dcafa6a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "    input_size = 2048,\n",
    "    output_size = 8192,\n",
    "    depth = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        inp = input_size\n",
    "        for d in range(depth):\n",
    "            if d == depth - 1:\n",
    "                layers.append(nn.Linear(inp, output_size))\n",
    "            else:\n",
    "                layers.extend([nn.Linear(inp, output_size), nn.BatchNorm1d(output_size), nn.ReLU(inplace=True)])\n",
    "                inp = output_size\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class VicReg(nn.Module):\n",
    "    def __init__(self,\n",
    "    backend = 'resnet50',\n",
    "    input_size = 2048,\n",
    "    output_size = 8192,\n",
    "    depth_projector = 3,\n",
    "    lmbd = 5e-3, u = 1, v= 1, epsilon = 1e-3):\n",
    "\n",
    "        super().__init__()\n",
    "        self.backend = backend\n",
    "        self.projector = MLP(input_size=input_size, output_size=output_size, depth=depth_projector)\n",
    "        self.output_size = output_size\n",
    "        self.epsilon = epsilon\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        self.lmbd = lmbd\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = self.backend(x)\n",
    "        y = self.backend(y)\n",
    "        x = self.projector(x)\n",
    "        y = self.projector(y)\n",
    "\n",
    "        bs = x.size(0)\n",
    "    \n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "    \n",
    "        # x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
    "        # y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "    \n",
    "        std_x = torch.sqrt(x.var(dim=0) + self.epsilon)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + self.epsilon)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "    \n",
    "        cov_x = (x.T @ x) / (bs - 1)\n",
    "        cov_y = (y.T @ y) / (bs - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(self.output_size) + off_diagonal(cov_y).pow_(2).sum().div(self.output_size)\n",
    "    \n",
    "        loss = (\n",
    "            self.u * repr_loss\n",
    "            + self.v * std_loss\n",
    "            + self.lmbd * cov_loss\n",
    "        )\n",
    "        \n",
    "        # Free tensors\n",
    "        del x, y, cov_x, cov_y, std_x, std_y\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725f8ccc-429b-4394-b5e8-67fa1eded88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/noahgolmant/pytorch-lars\n",
    "\"\"\" Layer-wise adaptive rate scaling for SGD in PyTorch! \"\"\"\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    r\"\"\"Implements layer-wise adaptive rate scaling for SGD.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): base learning rate (\\gamma_0)\n",
    "        momentum (float, optional): momentum factor (default: 0) (\"m\")\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "            (\"\\beta\")\n",
    "        eta (float, optional): LARS coefficient\n",
    "        max_epoch: maximum training epoch to determine polynomial LR decay.\n",
    "\n",
    "    Based on Algorithm 1 of the following paper by You, Gitman, and Ginsburg.\n",
    "    Large Batch Training of Convolutional Networks:\n",
    "        https://arxiv.org/abs/1708.03888\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = LARS(model.parameters(), lr=0.1, eta=1e-3)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, momentum=.9,\n",
    "                 weight_decay=.0005, eta=0.001, max_epoch=200):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\"\n",
    "                             .format(weight_decay))\n",
    "        if eta < 0.0:\n",
    "            raise ValueError(\"Invalid LARS coefficient value: {}\".format(eta))\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(lr=lr, momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        eta=eta, max_epoch=max_epoch)\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "            epoch: current epoch to calculate polynomial LR decay schedule.\n",
    "                   if None, uses self.epoch and increments it.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            eta = group['eta']\n",
    "            lr = group['lr']\n",
    "            max_epoch = group['max_epoch']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                weight_norm = torch.norm(p.data)\n",
    "                grad_norm = torch.norm(d_p)\n",
    "\n",
    "                # Global LR computed on polynomial decay schedule\n",
    "                decay = (1 - float(epoch) / max_epoch) ** 2\n",
    "                global_lr = lr * decay\n",
    "\n",
    "                # Compute local learning rate for this layer\n",
    "                local_lr = eta * weight_norm / \\\n",
    "                    (grad_norm + weight_decay * weight_norm)\n",
    "\n",
    "                # Update the momentum term\n",
    "                actual_lr = local_lr * global_lr\n",
    "\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = \\\n",
    "                            torch.zeros_like(p.data)\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
    "                p.data.add_(-buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47657b62-7560-42a1-88fa-1bbf9f8ce111",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7605d35-ce76-4793-8004-dbf1b7c06cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2116e62-16fa-4b0f-be96-56172e346317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "base_lr = 0.25\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79adc513-ff86-40c1-ba0c-9a8328ba1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "transform_x1, transform_x2, transform_test = get_cxr_transforms()\n",
    "\n",
    "dataset_path = './datasets/chestx-ray14-v3'\n",
    "train_dataset = datasets.ImageFolder(root=f'{dataset_path}/train', transform=MultiViewDataInjector([transform_x1, transform_x2]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=f'{dataset_path}/test', transform=MultiViewDataInjector([transform_x1, transform_x2]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed95bcf-71c5-441d-a356-724546f61d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "def train_loop(model, epoch, optimizer, train_loader, device):\n",
    "    tk0 = tqdm(train_loader)\n",
    "    train_loss = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for (x, x1), _ in tk0:\n",
    "        x = x.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        loss = model.forward(x, x1)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step(epoch)\n",
    "\n",
    "        # Free tensors\n",
    "        del x, x1, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return train_loss, time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "946de44a-ec7a-428f-80b0-85736e0067c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VicReg(\n",
       "  (backend): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): ConvNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): ConvNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): ConvNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=True)\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (projector): MLP(\n",
       "    (layer): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (1): BatchNorm1d(5120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "      (4): BatchNorm1d(5120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# Load model\n",
    "model = efficientnet_b0(pretrained=True)\n",
    "embed_dim = model.classifier[1].in_features\n",
    "output_features = embed_dim*4\n",
    "model.classifier[1] = nn.Identity()\n",
    "model = VicReg(input_size=embed_dim, output_size=output_features, backend=model, depth_projector=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b516ff9-e4bd-4466-8474-ffddf7e0e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/378 [00:00<?, ?it/s]/tmp/user/1781838319/ipykernel_1871116/2481369168.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:16<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]  loss: 0.8506856423521799, time: 196.4984793663025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:12<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/100]  loss: 0.7717970983376579, time: 192.84085035324097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:14<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/100]  loss: 0.7427284105744942, time: 194.91769289970398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:17<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/100]  loss: 0.7285782631742891, time: 197.13048815727234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/100]  loss: 0.7185948177304848, time: 198.211341381073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/100]  loss: 0.711681333956895, time: 199.28527522087097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/100]  loss: 0.7075205147266388, time: 199.9650321006775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/100]  loss: 0.7030118675458998, time: 200.446209192276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/100]  loss: 0.6985193910422148, time: 200.17217111587524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/100]  loss: 0.6964212703326392, time: 200.21403455734253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/100]  loss: 0.6934452101036355, time: 199.4116337299347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/100]  loss: 0.6907250300917045, time: 199.29016137123108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/100]  loss: 0.689308600608634, time: 199.58272886276245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/100]  loss: 0.6872269717789201, time: 199.0073902606964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/100]  loss: 0.6846566037841575, time: 198.51671290397644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/100]  loss: 0.6837639685661073, time: 198.88416862487793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17/100]  loss: 0.6821513614326558, time: 199.1288025379181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18/100]  loss: 0.6808242392603052, time: 198.6291708946228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19/100]  loss: 0.6803705993468169, time: 199.32719230651855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/100]  loss: 0.678297890399499, time: 199.63642740249634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21/100]  loss: 0.6776622861781448, time: 199.80562090873718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22/100]  loss: 0.6765590775265264, time: 200.74028182029724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23/100]  loss: 0.6749390127166869, time: 200.31053042411804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24/100]  loss: 0.6745542872835089, time: 201.27559232711792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25/100]  loss: 0.6740481661110328, time: 201.94392919540405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26/100]  loss: 0.6728149210965192, time: 202.17541027069092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27/100]  loss: 0.6724035413492293, time: 202.13495922088623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28/100]  loss: 0.6711558734298383, time: 202.66870379447937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29/100]  loss: 0.670596240216462, time: 202.3392996788025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:23<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30/100]  loss: 0.6699092186317241, time: 203.44974756240845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:23<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31/100]  loss: 0.6688218802686722, time: 203.65258741378784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:23<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32/100]  loss: 0.6683356687820777, time: 203.22749733924866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:24<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33/100]  loss: 0.667772668063956, time: 204.18206405639648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34/100]  loss: 0.6672457223216062, time: 202.97179007530212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35/100]  loss: 0.6667723824422827, time: 202.62185621261597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36/100]  loss: 0.6667826642435064, time: 202.10924530029297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37/100]  loss: 0.6658269622653881, time: 202.3049440383911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38/100]  loss: 0.6654899117177125, time: 202.4152705669403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39/100]  loss: 0.6642668292951331, time: 201.4390037059784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40/100]  loss: 0.6639109298034951, time: 201.1891906261444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41/100]  loss: 0.6637139046002948, time: 200.64772200584412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [42/100]  loss: 0.6633012465068272, time: 200.39906096458435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43/100]  loss: 0.6636190469618197, time: 200.73982167243958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [44/100]  loss: 0.662460389749083, time: 200.76992678642273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [45/100]  loss: 0.662511578627995, time: 200.8782012462616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46/100]  loss: 0.662011427223367, time: 201.3355689048767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47/100]  loss: 0.6619529735158991, time: 201.72804474830627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [48/100]  loss: 0.6611580924382285, time: 202.23471355438232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [49/100]  loss: 0.6608922205589436, time: 202.5001997947693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:22<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50/100]  loss: 0.6601238011052368, time: 202.3122980594635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51/100]  loss: 0.6603769981671893, time: 201.72272181510925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [52/100]  loss: 0.6598373845456138, time: 201.82702040672302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [53/100]  loss: 0.6594041213787422, time: 201.43177914619446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [54/100]  loss: 0.6597755994430925, time: 200.22894549369812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [55/100]  loss: 0.6589938676546491, time: 199.56179523468018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [56/100]  loss: 0.6584222674685181, time: 199.43970370292664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [57/100]  loss: 0.6583894486465152, time: 199.5794439315796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [58/100]  loss: 0.6592158955871743, time: 199.94313883781433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [59/100]  loss: 0.6578419758844628, time: 199.677987575531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [60/100]  loss: 0.6580185644210331, time: 199.98255610466003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [61/100]  loss: 0.6582884936736374, time: 200.35149788856506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [62/100]  loss: 0.6578852092778241, time: 201.01632809638977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [63/100]  loss: 0.6578649328183875, time: 201.50020623207092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [64/100]  loss: 0.6568007849196278, time: 201.46880745887756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [65/100]  loss: 0.6563273471183878, time: 200.58674907684326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [66/100]  loss: 0.6570721904121378, time: 199.88097167015076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [67/100]  loss: 0.6557208406862127, time: 199.5185809135437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [68/100]  loss: 0.6568461533576723, time: 199.25972700119019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69/100]  loss: 0.6549879068104678, time: 199.78171396255493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [70/100]  loss: 0.6560425613292311, time: 199.99852228164673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71/100]  loss: 0.6562193540668992, time: 200.34220385551453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [72/100]  loss: 0.6550492537084711, time: 200.46139550209045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [73/100]  loss: 0.655099386260623, time: 201.0068461894989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [74/100]  loss: 0.6554081711188826, time: 201.16643905639648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [75/100]  loss: 0.6546081723990264, time: 201.22228693962097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [76/100]  loss: 0.6546759301077121, time: 200.77629137039185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [77/100]  loss: 0.6548590973886863, time: 201.03124904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [78/100]  loss: 0.6534861662085094, time: 200.18369340896606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [79/100]  loss: 0.6537712284181484, time: 199.56834435462952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [80/100]  loss: 0.6544949308274284, time: 199.74689388275146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [81/100]  loss: 0.6534125227776785, time: 200.1022663116455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [82/100]  loss: 0.6533783450328484, time: 200.46666622161865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83/100]  loss: 0.6533455094963154, time: 200.33399939537048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [84/100]  loss: 0.6539621182850429, time: 200.53995966911316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [85/100]  loss: 0.6521040204340819, time: 199.9142758846283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [86/100]  loss: 0.6537699401378632, time: 201.0853886604309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [87/100]  loss: 0.6528708821251279, time: 200.48093461990356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [88/100]  loss: 0.6528934544671781, time: 200.84310293197632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [89/100]  loss: 0.6536994002483509, time: 200.89242434501648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:21<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [90/100]  loss: 0.6525887411738199, time: 201.2083125114441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [91/100]  loss: 0.6528764373410947, time: 199.8130841255188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [92/100]  loss: 0.6528252878832439, time: 199.91712045669556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [93/100]  loss: 0.6522210927867385, time: 199.84474730491638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [94/100]  loss: 0.6525729262324237, time: 199.393492937088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [95/100]  loss: 0.651811056666904, time: 199.92905116081238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [96/100]  loss: 0.6517418832690628, time: 199.710125207901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [97/100]  loss: 0.6521596443400812, time: 200.38924479484558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [98/100]  loss: 0.6521531392342199, time: 200.90530848503113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [99/100]  loss: 0.6517015809104556, time: 200.71443724632263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 378/378 [03:20<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]  loss: 0.6512183704704204, time: 200.84397220611572\n",
      "Training time 20049.147972106934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train script\n",
    "start_time = time.time()\n",
    "\n",
    "model = model.to(device)\n",
    "params = model.parameters()\n",
    "optimizer = LARS(params, lr=(base_lr * batch_size) / 256, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, run_time = train_loop(model, epoch, optimizer, train_loader, device)\n",
    "    print(f'Epoch: [{epoch+1}/{epochs}]  loss: {np.mean(train_loss)}, time: {run_time}')\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(f'Training time {train_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c728081f-6c0f-4f81-b64a-15f71af7fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "model_backbone_weights = model.backend\n",
    "# print(model_backbone_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1eaa9f-ca7b-4f50-be0b-de4e439b1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir ./output/VICReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1522def-0fdf-4fe2-b05c-628d8106a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './output/VICReg'\n",
    "torch.save({ 'model_state_dict': model_backbone_weights.state_dict() }, f'{save_model_path}/efficientnet_b0_backbone_weights_v1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6e76-b144-422c-a3ce-017964fb2fd9",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce8b6658-f7cc-4128-9012-cb6f5c1d618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = './datasets/COVIDGR_1.0'\n",
    "positive_path = os.path.join(dataset_path, 'P')\n",
    "negative_path = os.path.join(dataset_path, 'N')\n",
    "\n",
    "# Data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.3, 0.9), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomApply(\n",
    "            [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "            p=0.8\n",
    "        ),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=25, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e7d5c7-fca2-48a0-8795-0cb1cab00a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 852\n",
       "    Root location: ./datasets/COVIDGR_1.0\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(224, 224), scale=(0.3, 0.9), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
       "               RandomApply(\n",
       "               p=0.8\n",
       "               ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.8, 1.2], hue=[-0.1, 0.1])\n",
       "           )\n",
       "               RandomGrayscale(p=0.2)\n",
       "               RandomApply(\n",
       "               p=0.5\n",
       "               GaussianBlur(kernel_size=(25, 25), sigma=(0.1, 2.0))\n",
       "           )\n",
       "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "           )"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Map labels\n",
    "dataset.class_to_idx = {'N': 0, 'P': 1}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8984a78-804b-492d-8f96-7a4292ea8b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766 256\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "batch_size = 256\n",
    "print(train_size, batch_size)\n",
    "\n",
    "# Split off the test set\n",
    "train_val_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for the test set (held out)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f89a992-6939-4c08-8633-9e705c6ba5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./output/VICReg/efficientnet_b0_backbone_weights_v1.ckpt\"\n",
    "best_params = {\"learning_rate\": 0.01, \"weight_decay\": 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "808c7a90-63d0-4127-83a0-a244db80487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_model():\n",
    "    # Load the EfficientNet model\n",
    "    model = efficientnet_b0()\n",
    "    \n",
    "    # Modify the final classification head for your dataset\n",
    "    embed_dim = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Identity()\n",
    "    \n",
    "    # Load the pre-trained weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    state_dict = checkpoint[\"model_state_dict\"]  # Adjust the key if needed\n",
    "    \n",
    "    # Load the weights into the model\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Pretrained weights found at {} and loaded with msg: {}'.format(checkpoint_path, msg))\n",
    "\n",
    "    # Freeze model params\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7933c82e-52a0-4bbd-bbc9-c709efe6fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def train_model(model, classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "\n",
    "        # Validation phase\n",
    "        classifier.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                outputs = classifier(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "\n",
    "        print(f\"[{device}] Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss / len(val_loader):.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return best_val_f1\n",
    "\n",
    "# Evaluate model on the test set\n",
    "def evaluate_model(model, classifier, test_loader):\n",
    "    classifier.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = classifier(outputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def grid_search(train_loader, val_loader, learning_rates, weight_decays, num_epochs):\n",
    "    best_model = None\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decays:\n",
    "            model = get_model()\n",
    "            optimizer = optim.SGD(model.classifier[1].parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
    "            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            print(f\"\\nTraining with lr={lr}, weight_decay={wd}\")\n",
    "            f1_score = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "            if f1_score > best_f1:\n",
    "                best_f1 = f1_score\n",
    "                best_model = model\n",
    "                best_params = {\"learning_rate\": lr, \"weight_decay\": wd}\n",
    "    print(f\"\\nBest Model F1: {best_f1} with params {best_params}\")\n",
    "    return best_model, best_params\n",
    "\n",
    "# Step 9: 5-Fold Cross-Validation\n",
    "def cross_validation(best_params, dataset, test_loader, num_epochs=50, folds=5):\n",
    "    fold_metrics = []\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=100)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\nStarting fold {fold + 1}/{folds}\")\n",
    "\n",
    "        # Split dataset indices for training and validation\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Create DataLoaders for this fold\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model, embed_dim = get_model()\n",
    "        linear_classifier = nn.Linear(embed_dim, 2) # 2 is the number of features\n",
    "        optimizer = optim.SGD(linear_classifier.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'], momentum=0.9)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / 10))\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = train_model(model, linear_classifier, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        accuracy, precision, recall, f1 = evaluate_model(model, linear_classifier, test_loader)\n",
    "        fold_metrics.append((accuracy, precision, recall, f1, end_time - start_time))\n",
    "\n",
    "    return np.array(fold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "776f7512-68b0-4aea-99bd-126eccf7d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7362, Val Loss: 0.7302, F1 Score: 0.4029\n",
      "[cuda] Epoch 2/50, Train Loss: 0.7095, Val Loss: 0.6806, F1 Score: 0.6194\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6457, Val Loss: 0.6133, F1 Score: 0.7362\n",
      "[cuda] Epoch 4/50, Train Loss: 0.5918, Val Loss: 0.5617, F1 Score: 0.7296\n",
      "[cuda] Epoch 5/50, Train Loss: 0.5541, Val Loss: 0.5579, F1 Score: 0.7547\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5576, Val Loss: 0.5635, F1 Score: 0.7342\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5860, Val Loss: 0.5636, F1 Score: 0.7320\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5362, Val Loss: 0.5645, F1 Score: 0.7582\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5302, Val Loss: 0.5386, F1 Score: 0.7532\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5287, Val Loss: 0.5515, F1 Score: 0.7403\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5452, Val Loss: 0.5495, F1 Score: 0.7134\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5329, Val Loss: 0.5774, F1 Score: 0.7089\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5244, Val Loss: 0.5462, F1 Score: 0.7239\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5231, Val Loss: 0.5280, F1 Score: 0.7853\n",
      "[cuda] Epoch 15/50, Train Loss: 0.4893, Val Loss: 0.5129, F1 Score: 0.7730\n",
      "[cuda] Epoch 16/50, Train Loss: 0.5031, Val Loss: 0.5265, F1 Score: 0.7250\n",
      "[cuda] Epoch 17/50, Train Loss: 0.4990, Val Loss: 0.5584, F1 Score: 0.7673\n",
      "[cuda] Epoch 18/50, Train Loss: 0.4864, Val Loss: 0.5047, F1 Score: 0.7578\n",
      "[cuda] Epoch 19/50, Train Loss: 0.4812, Val Loss: 0.5304, F1 Score: 0.7439\n",
      "[cuda] Epoch 20/50, Train Loss: 0.5109, Val Loss: 0.5359, F1 Score: 0.7329\n",
      "[cuda] Epoch 21/50, Train Loss: 0.5075, Val Loss: 0.5434, F1 Score: 0.7239\n",
      "[cuda] Epoch 22/50, Train Loss: 0.4663, Val Loss: 0.5408, F1 Score: 0.7407\n",
      "[cuda] Epoch 23/50, Train Loss: 0.4890, Val Loss: 0.5039, F1 Score: 0.7500\n",
      "[cuda] Epoch 24/50, Train Loss: 0.4974, Val Loss: 0.5386, F1 Score: 0.7134\n",
      "[cuda] Epoch 25/50, Train Loss: 0.4742, Val Loss: 0.5354, F1 Score: 0.7205\n",
      "[cuda] Epoch 26/50, Train Loss: 0.4807, Val Loss: 0.4942, F1 Score: 0.7750\n",
      "[cuda] Epoch 27/50, Train Loss: 0.4772, Val Loss: 0.5108, F1 Score: 0.7500\n",
      "[cuda] Epoch 28/50, Train Loss: 0.4731, Val Loss: 0.4908, F1 Score: 0.7821\n",
      "[cuda] Epoch 29/50, Train Loss: 0.4943, Val Loss: 0.5276, F1 Score: 0.7654\n",
      "[cuda] Epoch 30/50, Train Loss: 0.4745, Val Loss: 0.5333, F1 Score: 0.7453\n",
      "[cuda] Epoch 31/50, Train Loss: 0.4591, Val Loss: 0.5589, F1 Score: 0.7439\n",
      "[cuda] Epoch 32/50, Train Loss: 0.4713, Val Loss: 0.5126, F1 Score: 0.7654\n",
      "[cuda] Epoch 33/50, Train Loss: 0.4685, Val Loss: 0.5183, F1 Score: 0.7602\n",
      "[cuda] Epoch 34/50, Train Loss: 0.4774, Val Loss: 0.5255, F1 Score: 0.7758\n",
      "[cuda] Epoch 35/50, Train Loss: 0.4694, Val Loss: 0.4996, F1 Score: 0.7730\n",
      "[cuda] Epoch 36/50, Train Loss: 0.4906, Val Loss: 0.5216, F1 Score: 0.7561\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4754, Val Loss: 0.5326, F1 Score: 0.7643\n",
      "[cuda] Epoch 38/50, Train Loss: 0.4716, Val Loss: 0.5145, F1 Score: 0.7625\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4727, Val Loss: 0.5162, F1 Score: 0.7500\n",
      "[cuda] Epoch 40/50, Train Loss: 0.4833, Val Loss: 0.5057, F1 Score: 0.7607\n",
      "[cuda] Epoch 41/50, Train Loss: 0.4714, Val Loss: 0.4964, F1 Score: 0.7784\n",
      "[cuda] Epoch 42/50, Train Loss: 0.4890, Val Loss: 0.5124, F1 Score: 0.7362\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4513, Val Loss: 0.4992, F1 Score: 0.7765\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4711, Val Loss: 0.5116, F1 Score: 0.7515\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4654, Val Loss: 0.5364, F1 Score: 0.7273\n",
      "[cuda] Epoch 46/50, Train Loss: 0.4633, Val Loss: 0.5319, F1 Score: 0.7296\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4412, Val Loss: 0.4813, F1 Score: 0.7799\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4637, Val Loss: 0.5214, F1 Score: 0.7673\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4560, Val Loss: 0.5153, F1 Score: 0.7456\n",
      "[cuda] Epoch 50/50, Train Loss: 0.4622, Val Loss: 0.5117, F1 Score: 0.7531\n",
      "\n",
      "Starting fold 2/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6727, Val Loss: 0.6890, F1 Score: 0.5714\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6653, Val Loss: 0.6532, F1 Score: 0.6581\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6238, Val Loss: 0.6255, F1 Score: 0.6536\n",
      "[cuda] Epoch 4/50, Train Loss: 0.5760, Val Loss: 0.5646, F1 Score: 0.7211\n",
      "[cuda] Epoch 5/50, Train Loss: 0.5657, Val Loss: 0.5769, F1 Score: 0.7123\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5434, Val Loss: 0.5472, F1 Score: 0.7606\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5720, Val Loss: 0.5763, F1 Score: 0.7552\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5325, Val Loss: 0.5496, F1 Score: 0.7465\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5491, Val Loss: 0.5696, F1 Score: 0.7465\n",
      "[cuda] Epoch 10/50, Train Loss: 0.4988, Val Loss: 0.5617, F1 Score: 0.7042\n",
      "[cuda] Epoch 11/50, Train Loss: 0.4999, Val Loss: 0.5708, F1 Score: 0.6986\n",
      "[cuda] Epoch 12/50, Train Loss: 0.4899, Val Loss: 0.5868, F1 Score: 0.6667\n",
      "[cuda] Epoch 13/50, Train Loss: 0.4998, Val Loss: 0.5846, F1 Score: 0.7020\n",
      "[cuda] Epoch 14/50, Train Loss: 0.4930, Val Loss: 0.5988, F1 Score: 0.7020\n",
      "[cuda] Epoch 15/50, Train Loss: 0.4909, Val Loss: 0.5739, F1 Score: 0.7020\n",
      "[cuda] Epoch 16/50, Train Loss: 0.4995, Val Loss: 0.5584, F1 Score: 0.7190\n",
      "[cuda] Epoch 17/50, Train Loss: 0.4742, Val Loss: 0.5623, F1 Score: 0.7260\n",
      "[cuda] Epoch 18/50, Train Loss: 0.4883, Val Loss: 0.5508, F1 Score: 0.6933\n",
      "[cuda] Epoch 19/50, Train Loss: 0.4921, Val Loss: 0.5538, F1 Score: 0.7200\n",
      "[cuda] Epoch 20/50, Train Loss: 0.4930, Val Loss: 0.5717, F1 Score: 0.6892\n",
      "[cuda] Epoch 21/50, Train Loss: 0.4984, Val Loss: 0.5637, F1 Score: 0.6883\n",
      "[cuda] Epoch 22/50, Train Loss: 0.4803, Val Loss: 0.5653, F1 Score: 0.7152\n",
      "[cuda] Epoch 23/50, Train Loss: 0.4736, Val Loss: 0.5537, F1 Score: 0.7067\n",
      "[cuda] Epoch 24/50, Train Loss: 0.4644, Val Loss: 0.5833, F1 Score: 0.7226\n",
      "[cuda] Epoch 25/50, Train Loss: 0.4839, Val Loss: 0.5574, F1 Score: 0.7403\n",
      "[cuda] Epoch 26/50, Train Loss: 0.4576, Val Loss: 0.5903, F1 Score: 0.6797\n",
      "[cuda] Epoch 27/50, Train Loss: 0.4702, Val Loss: 0.5612, F1 Score: 0.7027\n",
      "[cuda] Epoch 28/50, Train Loss: 0.4589, Val Loss: 0.5580, F1 Score: 0.7152\n",
      "[cuda] Epoch 29/50, Train Loss: 0.4690, Val Loss: 0.5590, F1 Score: 0.7517\n",
      "[cuda] Epoch 30/50, Train Loss: 0.4696, Val Loss: 0.5466, F1 Score: 0.7260\n",
      "[cuda] Epoch 31/50, Train Loss: 0.4613, Val Loss: 0.5580, F1 Score: 0.7550\n",
      "[cuda] Epoch 32/50, Train Loss: 0.4595, Val Loss: 0.5319, F1 Score: 0.7308\n",
      "[cuda] Epoch 33/50, Train Loss: 0.4505, Val Loss: 0.5794, F1 Score: 0.7067\n",
      "[cuda] Epoch 34/50, Train Loss: 0.4707, Val Loss: 0.5782, F1 Score: 0.7143\n",
      "[cuda] Epoch 35/50, Train Loss: 0.4630, Val Loss: 0.5495, F1 Score: 0.6980\n",
      "[cuda] Epoch 36/50, Train Loss: 0.4694, Val Loss: 0.5731, F1 Score: 0.7237\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4683, Val Loss: 0.5622, F1 Score: 0.7417\n",
      "[cuda] Epoch 38/50, Train Loss: 0.4658, Val Loss: 0.5630, F1 Score: 0.7075\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4635, Val Loss: 0.5695, F1 Score: 0.7190\n",
      "[cuda] Epoch 40/50, Train Loss: 0.4634, Val Loss: 0.5640, F1 Score: 0.7152\n",
      "[cuda] Epoch 41/50, Train Loss: 0.4603, Val Loss: 0.5733, F1 Score: 0.7059\n",
      "[cuda] Epoch 42/50, Train Loss: 0.4630, Val Loss: 0.5820, F1 Score: 0.7285\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4600, Val Loss: 0.5730, F1 Score: 0.6974\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4559, Val Loss: 0.5774, F1 Score: 0.7013\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4302, Val Loss: 0.5654, F1 Score: 0.7059\n",
      "[cuda] Epoch 46/50, Train Loss: 0.4424, Val Loss: 0.5553, F1 Score: 0.7237\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4571, Val Loss: 0.5460, F1 Score: 0.7682\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4406, Val Loss: 0.5500, F1 Score: 0.7417\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4698, Val Loss: 0.5700, F1 Score: 0.7297\n",
      "[cuda] Epoch 50/50, Train Loss: 0.4421, Val Loss: 0.5482, F1 Score: 0.7417\n",
      "\n",
      "Starting fold 3/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7172, Val Loss: 0.7174, F1 Score: 0.5202\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6942, Val Loss: 0.6604, F1 Score: 0.6480\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6465, Val Loss: 0.5928, F1 Score: 0.7317\n",
      "[cuda] Epoch 4/50, Train Loss: 0.5762, Val Loss: 0.5499, F1 Score: 0.7590\n",
      "[cuda] Epoch 5/50, Train Loss: 0.5563, Val Loss: 0.5396, F1 Score: 0.7531\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5688, Val Loss: 0.5811, F1 Score: 0.7421\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5467, Val Loss: 0.5686, F1 Score: 0.7485\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5865, Val Loss: 0.5546, F1 Score: 0.7578\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5470, Val Loss: 0.5715, F1 Score: 0.7097\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5186, Val Loss: 0.5543, F1 Score: 0.7273\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5258, Val Loss: 0.6052, F1 Score: 0.6711\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5220, Val Loss: 0.5680, F1 Score: 0.6624\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5140, Val Loss: 0.5823, F1 Score: 0.7329\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5069, Val Loss: 0.5439, F1 Score: 0.7758\n",
      "[cuda] Epoch 15/50, Train Loss: 0.5024, Val Loss: 0.5597, F1 Score: 0.7134\n",
      "[cuda] Epoch 16/50, Train Loss: 0.4981, Val Loss: 0.5373, F1 Score: 0.7375\n",
      "[cuda] Epoch 17/50, Train Loss: 0.4850, Val Loss: 0.5590, F1 Score: 0.7625\n",
      "[cuda] Epoch 18/50, Train Loss: 0.4985, Val Loss: 0.5608, F1 Score: 0.7683\n",
      "[cuda] Epoch 19/50, Train Loss: 0.4864, Val Loss: 0.5289, F1 Score: 0.7547\n",
      "[cuda] Epoch 20/50, Train Loss: 0.5020, Val Loss: 0.5684, F1 Score: 0.7229\n",
      "[cuda] Epoch 21/50, Train Loss: 0.4961, Val Loss: 0.5140, F1 Score: 0.7975\n",
      "[cuda] Epoch 22/50, Train Loss: 0.4877, Val Loss: 0.5217, F1 Score: 0.7750\n",
      "[cuda] Epoch 23/50, Train Loss: 0.5028, Val Loss: 0.5387, F1 Score: 0.7654\n",
      "[cuda] Epoch 24/50, Train Loss: 0.5000, Val Loss: 0.5457, F1 Score: 0.7654\n",
      "[cuda] Epoch 25/50, Train Loss: 0.4886, Val Loss: 0.5775, F1 Score: 0.7051\n",
      "[cuda] Epoch 26/50, Train Loss: 0.4821, Val Loss: 0.5507, F1 Score: 0.7436\n",
      "[cuda] Epoch 27/50, Train Loss: 0.4913, Val Loss: 0.5368, F1 Score: 0.7516\n",
      "[cuda] Epoch 28/50, Train Loss: 0.4881, Val Loss: 0.5457, F1 Score: 0.7730\n",
      "[cuda] Epoch 29/50, Train Loss: 0.4932, Val Loss: 0.5574, F1 Score: 0.7531\n",
      "[cuda] Epoch 30/50, Train Loss: 0.4612, Val Loss: 0.5405, F1 Score: 0.7375\n",
      "[cuda] Epoch 31/50, Train Loss: 0.4800, Val Loss: 0.5470, F1 Score: 0.7561\n",
      "[cuda] Epoch 32/50, Train Loss: 0.4823, Val Loss: 0.5396, F1 Score: 0.7595\n",
      "[cuda] Epoch 33/50, Train Loss: 0.4842, Val Loss: 0.5563, F1 Score: 0.7578\n",
      "[cuda] Epoch 34/50, Train Loss: 0.4655, Val Loss: 0.5369, F1 Score: 0.7250\n",
      "[cuda] Epoch 35/50, Train Loss: 0.4720, Val Loss: 0.5413, F1 Score: 0.7453\n",
      "[cuda] Epoch 36/50, Train Loss: 0.4682, Val Loss: 0.5361, F1 Score: 0.7451\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4764, Val Loss: 0.5278, F1 Score: 0.7750\n",
      "[cuda] Epoch 38/50, Train Loss: 0.4676, Val Loss: 0.5311, F1 Score: 0.7564\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4688, Val Loss: 0.5362, F1 Score: 0.7673\n",
      "[cuda] Epoch 40/50, Train Loss: 0.4666, Val Loss: 0.5416, F1 Score: 0.7296\n",
      "[cuda] Epoch 41/50, Train Loss: 0.4682, Val Loss: 0.5014, F1 Score: 0.7578\n",
      "[cuda] Epoch 42/50, Train Loss: 0.4607, Val Loss: 0.5360, F1 Score: 0.7673\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4769, Val Loss: 0.5563, F1 Score: 0.7811\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4628, Val Loss: 0.5385, F1 Score: 0.7439\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4422, Val Loss: 0.5524, F1 Score: 0.7239\n",
      "[cuda] Epoch 46/50, Train Loss: 0.4646, Val Loss: 0.5442, F1 Score: 0.7273\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4546, Val Loss: 0.5314, F1 Score: 0.7375\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4594, Val Loss: 0.5549, F1 Score: 0.7636\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4761, Val Loss: 0.5429, F1 Score: 0.7578\n",
      "[cuda] Epoch 50/50, Train Loss: 0.4585, Val Loss: 0.5509, F1 Score: 0.7500\n",
      "\n",
      "Starting fold 4/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.7031, Val Loss: 0.7013, F1 Score: 0.5033\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6795, Val Loss: 0.6868, F1 Score: 0.5600\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6362, Val Loss: 0.6565, F1 Score: 0.6667\n",
      "[cuda] Epoch 4/50, Train Loss: 0.5801, Val Loss: 0.6395, F1 Score: 0.6541\n",
      "[cuda] Epoch 5/50, Train Loss: 0.5287, Val Loss: 0.6600, F1 Score: 0.6625\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5295, Val Loss: 0.6799, F1 Score: 0.6914\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5167, Val Loss: 0.6965, F1 Score: 0.6667\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5299, Val Loss: 0.7153, F1 Score: 0.6497\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5666, Val Loss: 0.6987, F1 Score: 0.6581\n",
      "[cuda] Epoch 10/50, Train Loss: 0.4889, Val Loss: 0.6720, F1 Score: 0.6839\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5053, Val Loss: 0.6401, F1 Score: 0.6581\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5000, Val Loss: 0.6193, F1 Score: 0.6667\n",
      "[cuda] Epoch 13/50, Train Loss: 0.4815, Val Loss: 0.6192, F1 Score: 0.6829\n",
      "[cuda] Epoch 14/50, Train Loss: 0.4838, Val Loss: 0.6012, F1 Score: 0.6957\n",
      "[cuda] Epoch 15/50, Train Loss: 0.4978, Val Loss: 0.5946, F1 Score: 0.6914\n",
      "[cuda] Epoch 16/50, Train Loss: 0.4679, Val Loss: 0.6098, F1 Score: 0.7030\n",
      "[cuda] Epoch 17/50, Train Loss: 0.4688, Val Loss: 0.6402, F1 Score: 0.6707\n",
      "[cuda] Epoch 18/50, Train Loss: 0.4842, Val Loss: 0.6268, F1 Score: 0.6790\n",
      "[cuda] Epoch 19/50, Train Loss: 0.4596, Val Loss: 0.6292, F1 Score: 0.6994\n",
      "[cuda] Epoch 20/50, Train Loss: 0.4517, Val Loss: 0.6552, F1 Score: 0.6626\n",
      "[cuda] Epoch 21/50, Train Loss: 0.4933, Val Loss: 0.6115, F1 Score: 0.6792\n",
      "[cuda] Epoch 22/50, Train Loss: 0.4760, Val Loss: 0.6254, F1 Score: 0.6875\n",
      "[cuda] Epoch 23/50, Train Loss: 0.4869, Val Loss: 0.6209, F1 Score: 0.6923\n",
      "[cuda] Epoch 24/50, Train Loss: 0.4600, Val Loss: 0.6319, F1 Score: 0.6750\n",
      "[cuda] Epoch 25/50, Train Loss: 0.4584, Val Loss: 0.6135, F1 Score: 0.6951\n",
      "[cuda] Epoch 26/50, Train Loss: 0.4563, Val Loss: 0.6100, F1 Score: 0.6871\n",
      "[cuda] Epoch 27/50, Train Loss: 0.4523, Val Loss: 0.6455, F1 Score: 0.6832\n",
      "[cuda] Epoch 28/50, Train Loss: 0.4742, Val Loss: 0.6355, F1 Score: 0.6667\n",
      "[cuda] Epoch 29/50, Train Loss: 0.4378, Val Loss: 0.6268, F1 Score: 0.6709\n",
      "[cuda] Epoch 30/50, Train Loss: 0.4663, Val Loss: 0.6078, F1 Score: 0.7044\n",
      "[cuda] Epoch 31/50, Train Loss: 0.4451, Val Loss: 0.6550, F1 Score: 0.6541\n",
      "[cuda] Epoch 32/50, Train Loss: 0.4761, Val Loss: 0.5941, F1 Score: 0.7037\n",
      "[cuda] Epoch 33/50, Train Loss: 0.4567, Val Loss: 0.6125, F1 Score: 0.6795\n",
      "[cuda] Epoch 34/50, Train Loss: 0.4545, Val Loss: 0.5745, F1 Score: 0.7152\n",
      "[cuda] Epoch 35/50, Train Loss: 0.4693, Val Loss: 0.6142, F1 Score: 0.6750\n",
      "[cuda] Epoch 36/50, Train Loss: 0.4807, Val Loss: 0.6262, F1 Score: 0.7073\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4585, Val Loss: 0.6367, F1 Score: 0.6750\n",
      "[cuda] Epoch 38/50, Train Loss: 0.4657, Val Loss: 0.5968, F1 Score: 0.7089\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4601, Val Loss: 0.5866, F1 Score: 0.6957\n",
      "[cuda] Epoch 40/50, Train Loss: 0.4599, Val Loss: 0.6232, F1 Score: 0.6832\n",
      "[cuda] Epoch 41/50, Train Loss: 0.4804, Val Loss: 0.6177, F1 Score: 0.6835\n",
      "[cuda] Epoch 42/50, Train Loss: 0.4483, Val Loss: 0.5870, F1 Score: 0.7250\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4321, Val Loss: 0.6128, F1 Score: 0.6918\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4417, Val Loss: 0.6003, F1 Score: 0.6875\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4357, Val Loss: 0.5767, F1 Score: 0.6968\n",
      "[cuda] Epoch 46/50, Train Loss: 0.4453, Val Loss: 0.6273, F1 Score: 0.6795\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4452, Val Loss: 0.6390, F1 Score: 0.6829\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4259, Val Loss: 0.6243, F1 Score: 0.7051\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4293, Val Loss: 0.5926, F1 Score: 0.6832\n",
      "[cuda] Epoch 50/50, Train Loss: 0.4506, Val Loss: 0.6160, F1 Score: 0.7097\n",
      "\n",
      "Starting fold 5/5\n",
      "Pretrained weights found at ./output/VICReg/efficientnet_b0_backbone_weights_v1.ckpt and loaded with msg: <All keys matched successfully>\n",
      "[cuda] Epoch 1/50, Train Loss: 0.6860, Val Loss: 0.6930, F1 Score: 0.5868\n",
      "[cuda] Epoch 2/50, Train Loss: 0.6701, Val Loss: 0.6544, F1 Score: 0.6667\n",
      "[cuda] Epoch 3/50, Train Loss: 0.6219, Val Loss: 0.6029, F1 Score: 0.7037\n",
      "[cuda] Epoch 4/50, Train Loss: 0.5857, Val Loss: 0.5357, F1 Score: 0.7375\n",
      "[cuda] Epoch 5/50, Train Loss: 0.5472, Val Loss: 0.5291, F1 Score: 0.7500\n",
      "[cuda] Epoch 6/50, Train Loss: 0.5590, Val Loss: 0.5160, F1 Score: 0.7632\n",
      "[cuda] Epoch 7/50, Train Loss: 0.5524, Val Loss: 0.5144, F1 Score: 0.7733\n",
      "[cuda] Epoch 8/50, Train Loss: 0.5431, Val Loss: 0.5178, F1 Score: 0.7448\n",
      "[cuda] Epoch 9/50, Train Loss: 0.5380, Val Loss: 0.5063, F1 Score: 0.7483\n",
      "[cuda] Epoch 10/50, Train Loss: 0.5412, Val Loss: 0.5152, F1 Score: 0.7808\n",
      "[cuda] Epoch 11/50, Train Loss: 0.5239, Val Loss: 0.4991, F1 Score: 0.7600\n",
      "[cuda] Epoch 12/50, Train Loss: 0.5061, Val Loss: 0.5311, F1 Score: 0.7200\n",
      "[cuda] Epoch 13/50, Train Loss: 0.5185, Val Loss: 0.4940, F1 Score: 0.7867\n",
      "[cuda] Epoch 14/50, Train Loss: 0.5124, Val Loss: 0.5308, F1 Score: 0.7632\n",
      "[cuda] Epoch 15/50, Train Loss: 0.4997, Val Loss: 0.5174, F1 Score: 0.7671\n",
      "[cuda] Epoch 16/50, Train Loss: 0.4838, Val Loss: 0.5301, F1 Score: 0.7582\n",
      "[cuda] Epoch 17/50, Train Loss: 0.5137, Val Loss: 0.4988, F1 Score: 0.7871\n",
      "[cuda] Epoch 18/50, Train Loss: 0.4972, Val Loss: 0.5377, F1 Score: 0.7682\n",
      "[cuda] Epoch 19/50, Train Loss: 0.5169, Val Loss: 0.5464, F1 Score: 0.7467\n",
      "[cuda] Epoch 20/50, Train Loss: 0.4753, Val Loss: 0.5371, F1 Score: 0.7517\n",
      "[cuda] Epoch 21/50, Train Loss: 0.4831, Val Loss: 0.5429, F1 Score: 0.7484\n",
      "[cuda] Epoch 22/50, Train Loss: 0.4798, Val Loss: 0.5522, F1 Score: 0.7333\n",
      "[cuda] Epoch 23/50, Train Loss: 0.4913, Val Loss: 0.5573, F1 Score: 0.7239\n",
      "[cuda] Epoch 24/50, Train Loss: 0.4861, Val Loss: 0.5446, F1 Score: 0.7771\n",
      "[cuda] Epoch 25/50, Train Loss: 0.4712, Val Loss: 0.5258, F1 Score: 0.7484\n",
      "[cuda] Epoch 26/50, Train Loss: 0.4740, Val Loss: 0.5173, F1 Score: 0.7692\n",
      "[cuda] Epoch 27/50, Train Loss: 0.4713, Val Loss: 0.5205, F1 Score: 0.7651\n",
      "[cuda] Epoch 28/50, Train Loss: 0.4846, Val Loss: 0.5328, F1 Score: 0.7432\n",
      "[cuda] Epoch 29/50, Train Loss: 0.4798, Val Loss: 0.5343, F1 Score: 0.7500\n",
      "[cuda] Epoch 30/50, Train Loss: 0.4716, Val Loss: 0.5269, F1 Score: 0.7733\n",
      "[cuda] Epoch 31/50, Train Loss: 0.4764, Val Loss: 0.5307, F1 Score: 0.7550\n",
      "[cuda] Epoch 32/50, Train Loss: 0.4657, Val Loss: 0.5183, F1 Score: 0.7838\n",
      "[cuda] Epoch 33/50, Train Loss: 0.4648, Val Loss: 0.5155, F1 Score: 0.7692\n",
      "[cuda] Epoch 34/50, Train Loss: 0.4793, Val Loss: 0.5316, F1 Score: 0.7613\n",
      "[cuda] Epoch 35/50, Train Loss: 0.4975, Val Loss: 0.4915, F1 Score: 0.7484\n",
      "[cuda] Epoch 36/50, Train Loss: 0.4621, Val Loss: 0.5177, F1 Score: 0.7417\n",
      "[cuda] Epoch 37/50, Train Loss: 0.4522, Val Loss: 0.5467, F1 Score: 0.7712\n",
      "[cuda] Epoch 38/50, Train Loss: 0.4681, Val Loss: 0.5475, F1 Score: 0.7421\n",
      "[cuda] Epoch 39/50, Train Loss: 0.4602, Val Loss: 0.5211, F1 Score: 0.7468\n",
      "[cuda] Epoch 40/50, Train Loss: 0.4676, Val Loss: 0.5413, F1 Score: 0.7632\n",
      "[cuda] Epoch 41/50, Train Loss: 0.4472, Val Loss: 0.5412, F1 Score: 0.7355\n",
      "[cuda] Epoch 42/50, Train Loss: 0.4556, Val Loss: 0.5494, F1 Score: 0.7763\n",
      "[cuda] Epoch 43/50, Train Loss: 0.4675, Val Loss: 0.5067, F1 Score: 0.7550\n",
      "[cuda] Epoch 44/50, Train Loss: 0.4622, Val Loss: 0.5176, F1 Score: 0.7273\n",
      "[cuda] Epoch 45/50, Train Loss: 0.4482, Val Loss: 0.5304, F1 Score: 0.7600\n",
      "[cuda] Epoch 46/50, Train Loss: 0.4332, Val Loss: 0.5071, F1 Score: 0.7843\n",
      "[cuda] Epoch 47/50, Train Loss: 0.4601, Val Loss: 0.5605, F1 Score: 0.7403\n",
      "[cuda] Epoch 48/50, Train Loss: 0.4567, Val Loss: 0.5429, F1 Score: 0.7662\n",
      "[cuda] Epoch 49/50, Train Loss: 0.4473, Val Loss: 0.5617, F1 Score: 0.7389\n",
      "[cuda] Epoch 50/50, Train Loss: 0.4621, Val Loss: 0.5187, F1 Score: 0.7632\n",
      "\n",
      "Average metrics over 5 folds in test set:\n",
      "Accuracy: 0.6744 ± 0.0221\n",
      "Precision: 0.5473 ± 0.0224\n",
      "Recall: 0.7250 ± 0.0306\n",
      "F1 Score: 0.6237 ± 0.0248\n",
      "Training Time per Fold: 1884.10 ± 19.57 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold cross-validation\n",
    "metrics = cross_validation(best_params, train_val_dataset, test_loader, num_epochs=50, folds=5)\n",
    "\n",
    "# Calculate average and standard deviation of metrics across folds\n",
    "avg_metrics = metrics.mean(axis=0)\n",
    "std_metrics = metrics.std(axis=0)\n",
    "\n",
    "print(f\"\\nAverage metrics over 5 folds in test set:\\n\"\n",
    "      f\"Accuracy: {avg_metrics[0]:.4f} ± {std_metrics[0]:.4f}\\n\"\n",
    "      f\"Precision: {avg_metrics[1]:.4f} ± {std_metrics[1]:.4f}\\n\"\n",
    "      f\"Recall: {avg_metrics[2]:.4f} ± {std_metrics[2]:.4f}\\n\"\n",
    "      f\"F1 Score: {avg_metrics[3]:.4f} ± {std_metrics[3]:.4f}\\n\"\n",
    "      f\"Training Time per Fold: {avg_metrics[4]:.2f} ± {std_metrics[4]:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1033d199-b12f-4a7b-9c1f-ccfab7161163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.51162791e-01, 5.23809524e-01, 6.87500000e-01, 5.94594595e-01,\n",
       "        1.90210057e+03],\n",
       "       [7.09302326e-01, 5.81395349e-01, 7.81250000e-01, 6.66666667e-01,\n",
       "        1.87772851e+03],\n",
       "       [6.51162791e-01, 5.22727273e-01, 7.18750000e-01, 6.05263158e-01,\n",
       "        1.89479169e+03],\n",
       "       [6.74418605e-01, 5.47619048e-01, 7.18750000e-01, 6.21621622e-01,\n",
       "        1.84858736e+03],\n",
       "       [6.86046512e-01, 5.60975610e-01, 7.18750000e-01, 6.30136986e-01,\n",
       "        1.89730236e+03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "558b478a-966b-4322-8d58-e90c25c4baaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>training time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>1902.100572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1877.728509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1894.791689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>1848.587361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>1897.302361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision   recall        f1  training time\n",
       "0  0.651163   0.523810  0.68750  0.594595    1902.100572\n",
       "1  0.709302   0.581395  0.78125  0.666667    1877.728509\n",
       "2  0.651163   0.522727  0.71875  0.605263    1894.791689\n",
       "3  0.674419   0.547619  0.71875  0.621622    1848.587361\n",
       "4  0.686047   0.560976  0.71875  0.630137    1897.302361"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(metrics, columns=['accuracy', 'precision', 'recall', 'f1', 'training time'])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e26de73f-0593-405e-851f-84a732843424",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/vicreg_fine_tuned_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a898a-b511-44f4-981b-a3a7b81b8388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
